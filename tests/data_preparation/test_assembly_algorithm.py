#!/usr/bin/env python3
#-*- coding: utf-8 -*-
#
# Personality Matching Experiment Framework
# Copyright (C) 2025 [Your Name/Institution]
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
#
# Filename: tests/data_preparation/test_assembly_algorithm.py

"""
Verifies the integrity of the personality description assembly algorithm.

This is a high-level integration test that validates the deterministic correctness
of the final data generation step. It uses the pre-selected "assembly logic"
set of subjects and compares the output of our script against a ground truth file
generated by the source expert system (Solar Fire).

The test performs a three-stage process in the existing assembly logic sandbox:
1.  Bypasses the LLM to create a "passthrough" delineation library.
2.  Runs the main `generate_personalities_db.py` script.
3.  Asserts that the generated output is bit-for-bit identical to the
    pre-made ground truth file.
"""

import csv
import hashlib
import io
import shutil
import subprocess
from pathlib import Path

import pandas as pd
import pytest
from pandas.testing import assert_frame_equal


def debug_check_directory(stage_name: str, sandbox_path: Path):
    """Prints the state of the neutralized_delineations dir."""
    print(f"\n--- DEBUG CHECK: {stage_name} ---")
    target_dir = sandbox_path / "data/foundational_assets/neutralized_delineations"
    if target_dir.exists():
        contents = list(target_dir.iterdir())
        print(f"Directory '{target_dir}' exists.")
        if contents:
            print("Contents:")
            for item in contents:
                print(f"  - {item.name}")
        else:
            print("Directory is EMPTY.")
    else:
        print(f"!!! DIRECTORY NOT FOUND: '{target_dir}' !!!")


@pytest.fixture
def assembly_test_environment() -> Path:
    """
    Identifies the existing assembly logic sandbox, copies all necessary source
    files into it, and ensures it's clean for the test run.
    """
    project_root = Path(__file__).resolve().parents[2]
    sandbox_path = project_root / "temp_assembly_logic_validation"

    # Ensure the primary sandbox directory exists.
    assert sandbox_path.is_dir(), (
        f"The assembly logic sandbox was not found at '{sandbox_path}'. "
        "Please run the prerequisite scripts first."
    )

    # --- Setup ---
    # Define source and destination directories
    assets_source_dir = project_root / "data/foundational_assets"
    assembly_logic_source_dir = assets_source_dir / "assembly_logic"
    assets_dest_dir = sandbox_path / "data/foundational_assets"
    processed_dest_dir = sandbox_path / "data/processed"

    # Create destination directories
    assets_dest_dir.mkdir(exist_ok=True)
    processed_dest_dir.mkdir(exist_ok=True)

    # List of files to copy: (source_path, destination_path)
    files_to_copy = [
        (assets_source_dir / "sf_delineations_library.txt", assets_dest_dir / "sf_delineations_library.txt"),
        (assets_source_dir / "point_weights.csv", assets_dest_dir / "point_weights.csv"),
        (assets_source_dir / "balance_thresholds.csv", assets_dest_dir / "balance_thresholds.csv"),
        (assembly_logic_source_dir / "subject_db.assembly_logic.csv", processed_dest_dir / "subject_db.csv"),
    ]

    # Copy all required input files into the sandbox
    for src, dest in files_to_copy:
        shutil.copy(src, dest)

    # Clean up the main output file from any previous test run.
    output_to_delete = sandbox_path / "personalities_db.txt"
    if output_to_delete.exists():
        output_to_delete.unlink()

    yield sandbox_path  # Provide the correct, prepared sandbox path to the test.

    # --- Teardown ---
    # Clean up only the artifacts generated BY THE TEST ITSELF to avoid
    # interfering with other files.
    if output_to_delete.exists():
        output_to_delete.unlink()


def load_all_delineations(sandbox_path: Path) -> dict:
    """Loads all delineation files from the sandbox into a dictionary."""
    delineations = {}
    delineations_dir = sandbox_path / "data/foundational_assets/neutralized_delineations"
    for f in delineations_dir.glob("*.csv"):
        with open(f, "r", encoding="utf-8") as infile:
            reader = csv.reader(infile)
            for row in reader:
                if len(row) == 2:
                    delineations[row[0]] = row[1]
    return delineations


def run_script(script_path: str, sandbox_path: Path, *args):
    """
    Helper function to run a script via PDM in a subprocess, ensuring it
    runs from the project root so all relative paths are correct.
    This helper will print the script's output for debugging purposes.
    """
    project_root = Path(__file__).resolve().parents[2]
    command = [
        "pdm",
        "run",
        "python",
        script_path,
        "--sandbox-path",
        str(sandbox_path),
        *args,
    ]
    result = subprocess.run(
        command,
        capture_output=True,
        text=True,
        check=False,
        cwd=project_root,
    )

    # Always print the output for debugging, even on success
    if result.stdout:
        print(f"\n--- STDOUT from {Path(script_path).name} ---")
        print(result.stdout)
    if result.stderr:
        print(f"\n--- STDERR from {Path(script_path).name} ---")
        print(result.stderr)

    assert result.returncode == 0, f"Script {script_path} failed."


def test_assembly_algorithm_matches_ground_truth(assembly_test_environment, test_record_number):
    """
    Validates that `generate_personalities_db.py` produces an output
    identical to the pre-computed ground truth file.

    Can be limited to a single record by using the --test-record-number option.
    Example: pdm run pytest tests/test_assembly_algorithm.py --test-record-number=15
    """
    sandbox_path = assembly_test_environment
    project_root = Path(__file__).resolve().parents[2]

    # --- Step 1: Generate the "passthrough" delineations in the correct sandbox ---
    neutralize_script = str(project_root / "src/neutralize_delineations.py")
    run_script(neutralize_script, sandbox_path, "--bypass-llm", "--force")

    # --- Step 2: Generate the personalities database in the correct sandbox ---
    generate_db_script = str(project_root / "src/generate_personalities_db.py")
    # If a record number is specified for the test, pass it to the script
    # to ensure it only processes and prints debug info for that one record.
    if test_record_number is not None:
        run_script(generate_db_script, sandbox_path, "--force", f"--test-record-number={test_record_number}")
    else:
        run_script(generate_db_script, sandbox_path, "--force")

    # --- Step 3: Compare the output against the ground truth ---
    generated_path = sandbox_path / "personalities_db.txt"
    ground_truth_path = (
        project_root
        / "data/foundational_assets/assembly_logic/personalities_db.assembly_logic.txt"
    )

    assert generated_path.exists(), "The personalities_db.txt file was not created."

    # Read both files with robust settings to ensure correct comparison
    generated_df = pd.read_csv(generated_path, sep="\t", dtype=str).fillna("")
    ground_truth_df = pd.read_csv(ground_truth_path, sep="\t", dtype=str).fillna("")

    # Sort to ensure order doesn't affect comparison
    generated_df["Index"] = pd.to_numeric(generated_df["Index"])
    ground_truth_df["Index"] = pd.to_numeric(ground_truth_df["Index"])

    generated_df = generated_df.sort_values(by="Index").reset_index(drop=True)
    ground_truth_df = ground_truth_df.sort_values(by="Index").reset_index(drop=True)

    # If a specific record number (1-18) is provided, map it to the actual
    # 'Index' value for a focused test.
    if test_record_number is not None:
        total_records = len(ground_truth_df)
        assert 1 <= test_record_number <= total_records, (
            f"Invalid record number. Please choose a number between 1 and {total_records}."
        )
        
        # Map the 1-based record number to the 0-based iloc index
        row_to_test_iloc = test_record_number - 1
        # Get the actual 'Index' value from that row
        actual_index = ground_truth_df.iloc[row_to_test_iloc]["Index"]

        print(f"\n--- FOCUSED TEST: Record #{test_record_number} (maps to Index {actual_index}) ---")
        
        # Filter both dataframes by the actual 'Index' value
        generated_df = generated_df[generated_df["Index"] == actual_index].reset_index(drop=True)
        ground_truth_df = ground_truth_df[ground_truth_df["Index"] == actual_index].reset_index(drop=True)
        
        assert not generated_df.empty, f"Index {actual_index} not found in generated data."
        assert not ground_truth_df.empty, f"Index {actual_index} not found in ground truth data."

    try:
        # Assert that the dataframes are identical
        assert_frame_equal(generated_df, ground_truth_df)
    except AssertionError:
        # --- Advanced Debugging on Failure ---
        print("\n--- DEBUG: COMPARISON FAILED ---")

        # Get the text blocks for hashing and printing
        generated_text = generated_df.iloc[0]["DescriptionText"]
        ground_truth_text = ground_truth_df.iloc[0]["DescriptionText"]

        # Calculate and print SHA-256 hashes for a definitive comparison
        generated_hash = hashlib.sha256(generated_text.encode('utf-8')).hexdigest()
        ground_truth_hash = hashlib.sha256(ground_truth_text.encode('utf-8')).hexdigest()
        print("\n--- DEFINITIVE CHECKSUM COMPARISON (SHA-256) ---")
        print(f" [GENERATED HASH]: {generated_hash}")
        print(f"[GROUND TRUTH HASH]: {ground_truth_hash}")
        print("--------------------------------------------------")

        # Load all possible text snippets for reverse-engineering
        delineations = load_all_delineations(sandbox_path)
        sorted_snippets = sorted(delineations.items(), key=lambda item: len(item[1]), reverse=True)

        # Reverse-engineer the ground truth assembly order
        ground_truth_assembly = []
        remaining_text = ground_truth_text
        while remaining_text:
            found_match = False
            for key, snippet in sorted_snippets:
                if remaining_text.startswith(snippet):
                    ground_truth_assembly.append((key, snippet))
                    remaining_text = remaining_text[len(snippet):].lstrip()
                    found_match = True
                    break
            if not found_match:
                ground_truth_assembly.append(("UNMATCHED", remaining_text))
                break

        # Print the ground truth assembly order
        print(f"\n--- DEBUG: Processing Subject: {ground_truth_df.iloc[0]['Name']} ---")
        print("--- GROUND TRUTH ASSEMBLY ORDER (REVERSE-ENGINEERED) ---")
        for i, (key, part) in enumerate(ground_truth_assembly):
            snippet = (part[:70] + '..') if len(part) > 70 else part
            print(f"  {i+1}. Key: {repr(key):<28} -> Snippet: '{snippet}'")
        print("--------------------------------------------------------")

        # Print the full text blocks and the full failing rows for final comparison
        print(f"\n[GENERATED DESCRIPTION]:\n{generated_text}")
        print(f"\n[GROUND TRUTH DESCRIPTION]:\n{ground_truth_text}")
        
        print("\n--- DETAILED ROW COMPARISON ---")
        print("GENERATED ROW:")
        print(generated_df.iloc[0])
        print("\nGROUND TRUTH ROW:")
        print(ground_truth_df.iloc[0])
        print("-------------------------------")

        # --- Definitive Dtype Comparison ---
        buffer = io.StringIO()
        generated_df.info(buf=buffer)
        generated_info = buffer.getvalue()
        
        buffer = io.StringIO() # Reset buffer
        ground_truth_df.info(buf=buffer)
        ground_truth_info = buffer.getvalue()

        print("\n--- GENERATED DTYPES ---")
        print(generated_info)
        print("\n--- GROUND TRUTH DTYPES ---")
        print(ground_truth_info)
        print("--------------------------")

        # Re-raise the original exception to make the test fail
        raise

# === End of tests/data_preparation/test_assembly_algorithm.py ===
