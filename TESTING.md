# Testing Strategy for the LLM Personality Matching Project

This document outlines the testing philosophy, procedures, and coverage strategy for the framework. It serves as a guide for developers and a record of the project's quality assurance standards.

## How to Run Automated Tests

The project uses `pytest` for Python unit tests. All automated tests are managed via PDM.

-   **Run all Python unit tests:**
    ```bash
    pdm run test
    ```
-   **Run unit tests with a console coverage report:**
    ```bash
    pdm run cov
    ```
-   **Run coverage for a specific file by its base name:**
    ```bash
    pdm run cov-file validate_wikipedia_pages
    ```

## A Guide to Manual & Integration Testing

### Core Algorithm Validation (Profile Generation)

This is a standalone, push-button `pytest` script that provides a permanent, high-level validation of the core personality assembly algorithm within the "Profile Generation" stage. It verifies that our Python script (`generate_personalities_db.py`) produces output that is bit-for-bit identical to a pre-computed ground truth file generated by the source expert system (Solar Fire).

-   **Run the full assembly logic test:**
    This test runs automatically as part of the main `pdm run test` suite. To run it in isolation:
    ```bash
    pdm run test-assembly
    ```
-   **Run the test for a single, specific record:**
    The test harness includes a flexible command-line option to target a single record for focused debugging.
    ```bash
    # Test the 3rd record in the ground-truth set
    pdm run test-assembly -- --test-record-number=3
    ```

> **Note:** The integration test procedures below (Layers 3 and higher) create a temporary `temp_test_environment` directory at the project root to run in a safe, isolated sandbox. These tests are non-destructive and will not modify your main project files.

This section provides a unified, step-by-step guide to the project's validation process, from developing a single script to performing a full end-to-end integration test of the data preparation pipeline.

### The Seven Layers of Validation

The framework is validated using a multi-layered strategy to ensure correctness at all levels:

1.  **Layer 1: Unit Testing:** Validating a single Python script in isolation.
2.  **Layer 2: Data Pipeline Orchestration Testing:** Validating the `prepare_data.ps1` orchestrator's logic using mock scripts.
3.  **Layer 3: Data Pipeline Integration Testing:** Validating the full data preparation pipeline (`Data Sourcing` -> `Candidate Qualification` -> `LLM-based Candidate Selection` -> `Profile Generation`) with a controlled seed dataset.
4.  **Layer 4: Main Workflow Integration Testing:** Validating the core `new -> audit -> fix` experiment lifecycle for a single experiment.
5.  **Layer 5: Migration Workflow Integration Testing:** Validating the `migrate_experiment.ps1` workflow for a single experiment.
6.  **Layer 6: Post-Hoc Study Evaluation:** Validating the `compile_study.ps1` workflow for creating a study from pre-existing, independent experiments.
7.  **Layer 7: New Study Generation and Lifecycle:** Validating the entire `new_study.ps1` lifecycle (`create -> audit -> break -> fix`) for a study generated from scratch.

---

### Layer 1: Unit & Integration Testing (A Single Python Script)

This is the iterative, three-stage workflow for developing or modifying any individual Python script. It follows a robust `Modify -> Unit Test -> Integration Test` pattern to ensure quality at every step.

#### Stage 1: Modify
Make the necessary code changes to the target Python script (e.g., `src/data_preparation/my_script.py`).

#### Stage 2: Unit Test
After modifying the code, run the script's dedicated unit test suite to verify its internal logic and catch any regressions.
```powershell
# Run the specific test file with code coverage
pdm run cov-file <script_name>
```
If the unit tests fail, fix the script before proceeding to the next stage.

#### Stage 3: Integration Test
Once unit tests pass, perform an integration test to validate that the script functions correctly within the live pipeline. This is done using the Layer 3 test harness, which provides a controlled environment.

1.  **Add a Checkpoint:** Temporarily modify the main pipeline orchestrator, `prepare_data.ps1`. Add an `exit 0` command immediately after the line that calls the script you just modified. This will halt the pipeline at the desired point.
2.  **Run the Test:** Execute the default Layer 3 test profile from the project root.
    ```powershell
    pdm run test-l3-default
    ```
3.  **Verify:** The test should run successfully up to your checkpoint and exit gracefully. The test harness will automatically create a backup of the sandbox (`temp_test_environment/layer3_sandbox/`) in `data/backup/`. Inspect the artifacts in this sandbox to confirm your script produced the correct output.
4.  **Clean Up:** Once verified, simply remove the temporary `exit 0` from `prepare_data.ps1`. The test harness handles all other cleanup automatically.

---

### Layer 2: Data Pipeline Orchestration Testing (`prepare_data.ps1` with Mocks)

This procedure validates the state machine logic of the `prepare_data.ps1` orchestrator in isolation, using fast, lightweight mock scripts. It verifies that the script correctly sequences automated steps, pauses for manual steps, and resumes properly. The test is fully automated, self-contained, and runs in milliseconds.

The test script is "self-healing": it programmatically parses the real `prepare_data.ps1` to determine the correct sequence of steps and required output files, ensuring the test never becomes outdated as the pipeline evolves.

**To run the test:**
```powershell
pdm run test-l2
```
The script handles all setup, execution of the test scenarios (including simulating manual steps), and cleanup automatically.

---

### Layer 3: Data Pipeline Integration Testing (`prepare_data.ps1`)

This procedure validates the real data preparation pipeline using a robust, profile-driven test harness. The harness uses a controlled seed dataset from `tests/assets/` to ensure complete test isolation. It tests the main `prepare_data.ps1` orchestrator as the System Under Test, covering all four pipeline stages from Data Sourcing to Profile Generation.

The test is managed by a single master script, `run_layer3_test.ps1`, which can be executed with different profiles to simulate various scenarios. All test runs are fully automated, including setup and cleanup. Upon completion, the test sandbox is automatically archived as a timestamped `.zip` file in `data/backup/` for post-mortem analysis.

#### How to Run Layer 3 Tests
All Layer 3 tests are run via PDM scripts from the project root.

*   **Default Profile (`default`):**
    This is the standard test case. It runs the full pipeline with LLM-based candidate selection active and injects controlled validation failures to test the script's resilience.
    ```powershell
    pdm run test-l3-default
    ```

*   **Bypass Profile (`bypass`):**
    This profile tests the pipeline with the `bypass_candidate_selection` flag enabled, ensuring the LLM-scoring stages are correctly skipped.
    ```powershell
    pdm run test-l3-bypass
    ```

*   **Interactive Mode (Guided Tour):**
    This profile provides a step-by-step guided tour of the pipeline. The script will pause before executing each Python script, explain what it is about to do, and wait for you to press Enter. This is an excellent way for new contributors to learn how the data pipeline works.
    ```powershell
    pdm run test-l3-interactive
    ```

**Prerequisites:** A configured `.env` file in the project root with a valid API key (for the eminence and OCEAN scoring steps in the `default` and `interactive` profiles).

### Layer 4: Main Workflow Integration Testing

This procedure validates the core `new -> audit -> break -> fix` experiment lifecycle in a safe, isolated sandbox. It follows a clean `Setup -> Test -> Cleanup` pattern.

#### Step 1: Automated Setup
Run this script to create the test environment, including a sandboxed `config.ini` and a minimal dataset.
```powershell
.\tests\testing_harness\layer4_step1_setup.ps1
```

#### Step 2: Execute the Test Workflow
This script fully automates the `new -> audit -> break -> fix` lifecycle and verifies the final output.
```powershell
.\tests\testing_harness\layer4_step2_test_workflow.ps1
```

#### Step 3: Automated Cleanup
After inspecting the artifacts, run this script to delete the test sandbox and all generated experiment files.
```powershell
.\tests\testing_harness\layer4_step3_cleanup.ps1
```

### Layer 5: Migration Workflow Integration Testing

This procedure validates the `migrate_experiment.ps1` workflow in a safe, isolated sandbox. It automatically creates a valid experiment, corrupts it, runs the migration, and validates the final, repaired output.

#### Step 1: Automated Setup
Run this script to create a sandboxed test environment and a deliberately corrupted experiment.
```powershell
.\tests\testing_harness\layer5_step1_setup.ps1
```

#### Step 2: Execute the Test Workflow
This script fully automates the `audit -> migrate -> validate` lifecycle and verifies the final repaired experiment.
```powershell
.\tests\testing_harness\layer5_step2_test_workflow.ps1
```

#### Step 3: Automated Cleanup
After inspecting the artifacts, run this script to delete the test sandbox and all generated experiment files.
```powershell
.\tests\testing_harness\layer5_step3_cleanup.ps1
```

### Layer 6: Post-Hoc Study Evaluation (Planned)

> **Note:** This is a planned testing layer. The harness scripts will be created as part of the study-level workflow development.

This procedure will validate the workflow for creating a formal study from a collection of pre-existing, independent experiments using the `compile_study.ps1` script.

#### Step 1: Automated Setup
The setup script will create a study directory and populate it with two small, independently generated, valid experiments.

#### Step 2: Execute the Test Workflow
This script will run `compile_study.ps1` on the prepared study directory and verify that the final analysis artifacts (`STUDY_results.csv`, `anova/` directory) are created successfully.

#### Step 3: Automated Cleanup
The cleanup script will delete the entire test study directory and restore the project's base files.

### Layer 7: New Study Generation and Lifecycle (Planned)

> **Note:** This is a planned testing layer. The harness scripts will be created after the `new_study.ps1` script is developed.

This procedure will validate the entire lifecycle for a study generated from scratch using the `new_study.ps1` orchestrator.

#### Step 1: Automated Setup
The setup script will create a test-specific `config.ini` that includes the `[StudyFactors]` section, configured to generate a small study of two experiments.

#### Step 2: Execute the Test Workflow
This script will validate the full `create -> audit -> break -> fix` lifecycle for a study:
1.  **Create:** Run `new_study.ps1` to generate the test study.
2.  **Audit & Verify:** Run `audit_study.ps1` to confirm the new study is `VALIDATED`.
3.  **Break:** Deliberately break one of the experiments within the study.
4.  **Audit & Fix:** Run `audit_study.ps1` (to confirm the broken state) and then `fix_study.ps1` to repair it.
5.  **Final Verification:** Run `audit_study.ps1` a final time to confirm the study is `VALIDATED` again.

#### Step 3: Automated Cleanup
The cleanup script will delete the test study directory and restore the project.

## Testing Status

### Data Preparation Pipeline

**Milestone Complete:** All four layers of testing for the data preparation pipeline (Core Algorithm, Unit, Orchestration, and Integration) are complete and passing. The status of individual components is detailed below.

--------------------------------------------------------------------------------------------------------------------
Module                              Cov. (%)        Status & Justification
----------------------------------- --------------- -----------------------------------------------------------------
**Stage 1: Data Sourcing**

`src/fetch_adb_data.py`             `33%`           COMPLETE. Unit tests cover critical offline logic. Live network
                                                    code is validated via integration testing.

**Stage 2: Candidate Qualification**

`src/find_wikipedia_links.py`       `38%`           COMPLETE. Unit tests cover key logic, including HTML parsing
                                                    and mocked API calls. Orchestration is validated via integration.

`src/validate_wikipedia_pages.py`   `39%`           COMPLETE. Unit tests cover all critical validation logic.

`src/select_eligible_candidates.py` `72%`           COMPLETE. Unit tests cover all core filtering and resumability
                                                    logic.

**Stage 3: LLM-based Candidate Selection**

`src/generate_eminence_scores.py`   `54%`           COMPLETE. Unit tests cover critical offline logic. Live LLM
                                                    calls are validated via integration testing.

`src/generate_ocean_scores.py`      `16%`           COMPLETE. Unit tests cover critical offline logic. Live LLM
                                                    calls are validated via integration testing.

`src/select_final_candidates.py`    `66%`           COMPLETE. Unit tests cover the entire data transformation
                                                    workflow for both default and bypass modes.

**Stage 4: Profile Generation**

`src/prepare_sf_import.py`          `62%`           COMPLETE. Unit tests cover the core data transformation logic.

`src/create_subject_db.py`          `57%`           COMPLETE. Unit tests cover the core data integration logic.

`src/neutralize_delineations.py`    `26%`           COMPLETE. Unit tests cover critical offline logic. Live LLM
                                                    calls are validated via integration testing.

`src/generate_personalities_db.py`  `70%`           COMPLETE. Unit test suite is complete. A separate `pytest`
                                                    provides bit-for-bit validation of the assembly algorithm.

`prepare_data.ps1`                  `N/A`           COMPLETE. As the primary orchestrator, this script is the
                                                    System Under Test for the Layer 3 integration test. Its state
                                                    machine, resumability, and interactive logic are fully validated
                                                    by the new profile-driven test harness.
--------------------------------------------------------------------------------------------------------------------

### Main Experiment & Analysis Pipeline

The following table details the testing status for each script in the main experimental and analysis workflows.

-----------------------------------------------------------------------------------------------------------------------------------------
Module                                  Cov. (%)        Status & Justification
--------------------------------------- --------------- ----------------------------------------------------------------------------------
**EXPERIMENT LIFECYCLE MANAGEMENT**
**Primary Orchestrators**

`src/experiment_manager.py`             `56%`           COMPLETE. Unit tests are complete, and the core `new`/`audit`/`fix`
                                                        workflows have been successfully validated via the scripted
                                                        end-to-end integration test.

`src/experiment_auditor.py`             `71%`           COMPLETE. The unit test suite validates the auditor's
                                                        ability to correctly identify all major experiment states
                                                        (New, Complete, Aggregation Needed, Reprocess Needed, Repair
                                                        Needed, and Migration Needed) by using a mocked file system
                                                        to simulate various data completeness scenarios.

**Finalization Scripts**

`src/manage_experiment_log.py`          `79%`           COMPLETE. The unit test suite validates all core commands
                                                        (`rebuild`, `finalize`, `start`) and their file I/O
                                                        operations. It confirms correct CSV parsing, generation, and
                                                        the idempotency of the `finalize` command.

`src/compile_experiment_results.py`     `74%`           COMPLETE. Unit tests cover the main aggregation workflow and
                                                        robustly handle edge cases like empty or missing replication
                                                        files.

**SINGLE REPLICATION PIPELINE**
**Primary Orchestrator**

`src/replication_manager.py`            `77%`           COMPLETE. Unit tests cover the core control flow for both
                                                        "new run" and "reprocess" modes, including failure handling.

**Pipeline Stages**

`src/build_llm_queries.py`              `68%`           COMPLETE. Unit tests cover the core orchestration logic,
                                                        including new runs, continued runs, and key failure modes.

`src/query_generator.py`                `74%`           COMPLETE. Unit tests cover both 'correct' and 'random'
                                                        mapping strategies, edge cases (e.g., k=max), and key
                                                        failure modes like missing or insufficient input data.

`src/llm_prompter.py`                   `53%`           COMPLETE. Unit tests cover the core logic for successful API
                                                        calls, error conditions (HTTP, timeout), and file I/O failures.
`src/process_llm_responses.py`          `67%`           COMPLETE. Unit tests cover the core parsing logic, including
                                                        markdown, fallback, flexible spacing, reordered columns, and
                                                        key failure modes.

`src/analyze_llm_performance.py`        `63%`           COMPLETE. Unit tests cover the main orchestrator, all core
                                                        statistical calculations (including edge cases), and the robust
                                                        parsing of complex file formats (e.g., Markdown).

`src/run_bias_analysis.py`              `86%`           COMPLETE. Unit tests cover the main orchestrator workflow,
                                                        core bias calculations, and robust handling of empty or
                                                        malformed data files.

`src/generate_replication_report.py`    `90%`           COMPLETE. Unit tests cover the main workflow, including
                                                        robust error handling for missing/corrupted files and correct
                                                        fallback for optional data sources.

`src/compile_replication_results.py`    `78%`           COMPLETE. Unit tests cover the main workflow, data merging
                                                        logic, and robust error handling for missing or invalid input
                                                        files.

**Study-Level & Analysis**

`src/compile_study_results.py`          `76%`           COMPLETE. Unit tests cover the recursive aggregation
                                                        workflow and robustly handle edge cases like empty or missing
                                                        experiment files.

`src/analyze_study_results.py`          `66%`           COMPLETE. The unit test suite fully validates the script's
                                                        core logic, including data filtering, control flow for different
                                                        analysis scenarios (e.g., zero variance), and graceful
                                                        shutdowns. Key statistical and plotting functions are mocked
                                                        to ensure isolated validation.

**Utility & Other Scripts**

`src/upgrade_legacy_experiment.py`      `75%`           COMPLETE. The unit test suite validates the script's core
                                                        batch-processing logic, ensuring it correctly finds all
                                                        target directories and halts immediately if its worker
                                                        script reports an error.

`src/restore_experiment_config.py`      `83%`           COMPLETE. The unit test suite validates the script's ability
                                                        to parse legacy report files and correctly generate a new,
                                                        valid `config.ini.archived` file. It also confirms that the
                                                        script exits gracefully if the target directory or report
                                                        files are missing.

`src/config_loader.py`                  `51%`           COMPLETE. Unit tests cover the core `get_config_value`
                                                        helper, including successful parsing, type conversions,
                                                        fallbacks, and robust error handling for missing sections or
                                                        keys.

**PowerShell Wrappers (Experiments)**

`new_experiment.ps1`                    `N/A`           COMPLETE. Validated via end-to-end integration testing.

`audit_experiment.ps1`                  `N/A`           COMPLETE. Validated via end-to-end integration testing.

`fix_experiment.ps1`                    `N/A`           COMPLETE. Validated via end-to-end integration testing.

`migrate_experiment.ps1`                `N/A`           COMPLETE. Validated via the scripted end-to-end integration
                                                        test (Layer 5), which confirms the script correctly handles a
                                                        severely corrupted experiment and produces a valid, repaired output.

`compile_study.ps1`                     `N/A`           PENDING. Manual validation is pending.

**PowerShell Wrappers (Studies)**

`new_study.ps1`                         `N/A`           PENDING. Manual validation is pending.

`audit_study.ps1`                       `N/A`           PENDING. Manual validation is pending.

`fix_study.ps1`                         `N/A`           PENDING. Manual validation is pending.

`migrate_study.ps1`                     `N/A`           PENDING. Manual validation is pending.
-----------------------------------------------------------------------------------------------------------------------------------------