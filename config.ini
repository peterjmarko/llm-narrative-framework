[Study]
# High-level parameters defining the entire experimental design.
# NUmber of replications to execute (r)
num_replications = 30
# Number of trils for each replication (m)
num_trials = 100
# Number of subjects in each group (k)
group_size = 10
# The ground truth mapping to use. Options: 'correct', 'random'.
mapping_strategy = random

[LLM]
# Parameters specific to the Language Model's behavior.
# Other inexpensive models:
#   deepseek/deepseek-chat-v3-0324 (very slow: ~28h)
#   google/gemini-2.0-flash-001
#   google/gemini-2.5-flash-preview-05-20 (fast: 4.5h)
#   google/gemma-3-27b-it
#   meta-llama/llama-3.3-70b-instruct (slow: 14.3h)
#   meta-llama/llama-4-maverick
#   mistralai/mistral-medium-3
#   mistralai/mistral-small-3.1-24b-instruct
#   openai/gpt-4o-mini (slow: 12.1h)
#   qwen/qwen3-235b-a22b (extremely slow: 60h)
#   x-ai/grok-3-mini-beta (slow: ~12h)
model_name = openai/gpt-4o-mini
temperature = 0.2
max_tokens = 8192

[API]
# Global settings for the API provider.
api_endpoint = https://openrouter.ai/api/v1/chat/completions
referer_header = http://localhost:3000
api_timeout_seconds = 120

[General]
# Housekeeping settings for file and directory names.
# Base directory for all pipeline outputs
base_output_dir = output
# Subdirectory for query files generated by build_queries.py
queries_subdir = session_queries
# Subdirectory for LLM responses generated by conduct_llm_sessions.py
responses_subdir = session_responses
# Subdirectory for processed scores and mappings ready for meta-analysis
analysis_inputs_subdir = analysis_inputs
# Subdirectory under 'output' for run reports
reports_subdir = reports
# Top-level directory for run archives
archives_dir = archives
# Global verbosity level (INFO, DEBUG, WARNING, ERROR, CRITICAL) for logging
# Scripts can have their own --verbose flags to override this for a single run
default_log_level = INFO

[Filenames]
# Source files (relative to the script needing them, or resolved to be alongside scripts)
personalities_src = personalities_db_1-5000.txt
base_query_src = base_query.txt
successful_indices_log = successful_query_indices.txt

# Tracking and aggregated files (names within their respective subdirectories)
used_indices_log = used_personality_indices.txt
aggregated_mappings_in_queries_dir = mappings.txt 

# Final files for meta-analysis script
all_scores_for_analysis = all_scores.txt
all_mappings_for_analysis = all_mappings.txt

# Temporary file used by build_queries.py and query_generator.py
temp_subset_personalities = temp_personalities_subset.txt
# Prefix for temporary files is created by query_generator.py
# qgen_temp_prefix

[MetaAnalysis]
# Default Top-K for accuracy in run_meta_analysis.py
default_top_k_accuracy = 3
# Delimiter expected by run_meta_analysis.py for its input files.
# 'None' means whitespace, or specify like ',' or '\t'.
# If process_responses.py outputs tab-delimited, this should be '\t'.
analysis_input_delimiter = \t 

[Schema]
# Defines the standard column names used across the analysis pipeline.

# Columns that define the experimental conditions (independent variables).
factors = model, mapping_strategy, temperature, k, m

# Columns that contain the performance results to be analyzed (dependent variables).
metrics = mean_mrr, mean_top_1_acc, mean_top_3_acc, mean_effect_size_r, mwu_stouffer_z, mwu_fisher_chi2

# The full, ordered list of columns for the final CSV output.
# This ensures every generated summary file has a consistent layout.
csv_header_order = run_directory,replication,model,mapping_strategy,temperature,k,m,db,mwu_stouffer_z,mwu_stouffer_p,mwu_fisher_chi2,mwu_fisher_p,mean_effect_size_r,effect_size_r_p,mean_mrr,mrr_p,mean_top_1_acc,top_1_acc_p,mean_top_3_acc,top_3_acc_p