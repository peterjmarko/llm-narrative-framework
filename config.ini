# ==============================================================================
# Main Configuration for the Personality Matching Experiment
# ==============================================================================
# This file controls all parameters for the experimental pipeline, from
# data paths to model settings and analysis configurations.
# 
[Study]
# High-level parameters defining the entire experimental design.
# Number of replications to execute (r)
num_replications = 30
# Number of trials for each replication (m)
num_trials = 100
# Number of subjects in each group (k)
group_size = 4
# The ground truth mapping to use. Options: 'correct', 'random'.
mapping_strategy = correct

[LLM]
# Parameters specific to the Language Model's behavior.
model_name = google/gemini-flash-1.5
temperature = 0.2
max_tokens = 8192
# Reliable models:
#   google/gemini-flash-1.5 (fast: 5.6h)
#   meta-llama/llama-4-maverick (fast: 5.9h)
#   mistralai/mistral-medium-3 (slow: 10.2h)
# Inexpensive models:
#   deepseek/deepseek-chat-v3-0324 (very slow: 23.9h)
#   google/gemini-flash-1.5 (fast: 5.6h)
#   google/gemini-2.0-flash-001 (fast: 4.4h)
#   google/gemini-2.5-flash-preview-05-20 (fast: 4.5h)
#   google/gemma-3-27b-it (very slow: 20.6h)
#   meta-llama/llama-3.3-70b-instruct (slow: 14.3h)
#   meta-llama/llama-4-maverick (fast: 5.9h)
#   mistralai/mistral-medium-3 (slow: 10.2h)
#   mistralai/mistral-small-3.1-24b-instruct (fast: 5.2h)
#   nvidia/llama-3.1-nemotron-ultra-253b-v1 (extremely slow: 36.6h)
#   openai/gpt-4.1-mini (slow: 12.0h)
#   openai/gpt-4o-mini (slow: 12.1h)
#   qwen/qwen3-235b-a22b (extremely slow: ~60h)
#   x-ai/grok-3-mini-beta (slow: 11.7h)

[API]
# Global settings for the API provider.
api_endpoint = https://openrouter.ai/api/v1/chat/completions
referer_header = http://localhost:3000
api_timeout_seconds = 120

[General]
# Housekeeping settings for file and directory names.
# Base directory for all pipeline outputs
base_output_dir = output
# Subdirectory for query files generated by build_queries.py
queries_subdir = session_queries
# Subdirectory for LLM responses generated by conduct_llm_sessions.py
responses_subdir = session_responses
# Subdirectory for processed scores and mappings ready for meta-analysis
analysis_inputs_subdir = analysis_inputs
# Subdirectory under 'output' for run reports
reports_subdir = reports
# Subdirectory within 'base_output_dir' for new, timestamped experiments
new_experiments_subdir = new_experiments
# Prefix for timestamped experiment directories
experiment_dir_prefix = experiment_
# Top-level directory for run archives
archives_dir = archives
# Global verbosity level (INFO, DEBUG, WARNING, ERROR, CRITICAL) for logging
# Scripts can have their own --verbose flags to override this for a single run
default_log_level = INFO

[Filenames]
# Source files (relative to the script needing them, or resolved to be alongside scripts)
personalities_src = personalities_db_1-5000.txt
base_query_src = base_query.txt
successful_indices_log = successful_query_indices.txt

# Tracking and aggregated files (names within their respective subdirectories)
used_indices_log = used_personality_indices.txt
aggregated_mappings_in_queries_dir = mappings.txt 

# Final files for meta-analysis script
all_scores_for_analysis = all_scores.txt
all_mappings_for_analysis = all_mappings.txt

# Temporary file used by build_queries.py and query_generator.py
temp_subset_personalities = temp_personalities_subset.txt
# Prefix for temporary files is created by query_generator.py
# qgen_temp_prefix

[MetaAnalysis]
# Default Top-K for accuracy in run_meta_analysis.py
default_top_k_accuracy = 3
# Delimiter expected by run_meta_analysis.py for its input files.
# 'None' means whitespace, or specify like ',' or '\t'.
# If process_responses.py outputs tab-delimited, this should be '\t'.
analysis_input_delimiter = \t 

[Schema]
# Defines the standard column names used across the analysis pipeline.

# Columns that define the experimental conditions (independent variables).
factors = model, mapping_strategy, temperature, k, m

# Columns that contain the performance results to be analyzed (dependent variables).
# ADDED: top1_pred_bias_std, true_false_score_diff
# ADDED: bias_slope, bias_p_value for analysis
metrics = mean_mrr,mean_top_1_acc,mean_top_3_acc,mean_effect_size_r,mwu_stouffer_z,mwu_fisher_chi2,mean_rank_of_correct_id,n_valid_responses,top1_pred_bias_std,true_false_score_diff,bias_slope,bias_p_value

# The full, ordered list of columns for the final CSV output.
# This ensures every generated summary file has a consistent layout.
# ADDED: The new bias metrics to the end of the header.
csv_header_order = run_directory,replication,n_valid_responses,model,mapping_strategy,temperature,k,m,db,mwu_stouffer_z,mwu_stouffer_p,mwu_fisher_chi2,mwu_fisher_p,mean_effect_size_r,effect_size_r_p,mean_mrr,mrr_p,mean_top_1_acc,top_1_acc_p,mean_top_3_acc,top_3_acc_p,mean_rank_of_correct_id,rank_of_correct_id_p,top1_pred_bias_std,true_false_score_diff,bias_slope,bias_intercept,bias_r_value,bias_p_value,bias_std_err

[ModelNormalization]
# Maps raw keywords found in run directories to a single, canonical internal name.
# This version systematically includes both hyphenated and underscored variants for maximum robustness.
# format: canonical_name = comma,separated,keyword(s)
deepseek-v3                      = deepseek-chat-v3, deepseek_chat_v3
google-gemini-2-0-flash          = gemini-2.0-flash, gemini_2_0_flash
google-gemini-2-5-flash          = gemini-2.5-flash, gemini_2_5_flash
google-gemini-flash-1-5          = gemini-flash-1.5, gemini_flash_1_5
google-gemma-3-27b               = gemma-3-27b, gemma_3_27b
meta-llama-3-3-70b               = llama-3.3-70b, llama_3_3_70b
meta-llama-4-maverick            = llama-4-maverick, llama_4_maverick
mistralai-mistral-medium-3       = mistral-medium-3, mistral_medium_3
mistralai-mistral-small-3-1      = mistral-small-3.1, mistral_small_3_1
nvidia-llama-3-1-nemotron-ultra  = nemotron-ultra
openai-gpt-4-1-mini              = gpt-4.1-mini, gpt_4.1_mini
openai-gpt-4o-mini               = gpt-4o-mini, gpt_4o_mini
qwen-qwen3-235b                  = qwen3-235b, qwen_235b
xai-grok-3-mini-beta             = grok-3-mini-beta, grok_3_mini_beta

[ModelDisplayNames]
# Maps the canonical internal names to human-readable names for plots and reports.
# The keys here MUST exactly match the keys in the [ModelNormalization] section.
# format: canonical_name = Friendly Display Name
deepseek-v3                      = DeepSeek V3
google-gemini-2-0-flash          = Gemini 2.0 Flash
google-gemini-2-5-flash          = Gemini 2.5 Flash
google-gemini-flash-1-5          = Gemini 1.5 Flash
google-gemma-3-27b               = Gemma 3 27B
meta-llama-3-3-70b               = Llama 3.3 70B
meta-llama-4-maverick            = Llama 4 Maverick
mistralai-mistral-medium-3       = Mistral Medium 3
mistralai-mistral-small-3-1      = Mistral Small 3.1
nvidia-llama-3-1-nemotron-ultra  = Llama 3.1 Nemotron Ultra 253B
openai-gpt-4-1-mini              = GPT 4.1 mini
openai-gpt-4o-mini               = GPT 4o mini
qwen-qwen3-235b                  = Qwen3 235B
xai-grok-3-mini-beta             = Grok 3 Mini

[ConfigCompatibility]
# This section provides a fallback map for legacy configuration files.
# The format is: canonical_name = new_section:new_key, old_section:old_key
# Scripts will try these locations in order when reading archived configs.

model_name = Model:model_name, LLM:model
num_trials = Study:num_trials, Study:num_iterations
num_subjects = Study:num_subjects, Study:k_per_query
mapping_strategy = Study:mapping_strategy
personalities_db_path = General:personalities_db_path, Filenames:personalities_src

[FactorDisplayNames]
# Maps internal factor names to human-readable names for plots and reports.
model = Model
mapping_strategy = Mapping Strategy
temperature = Temperature
k = Group Size (k)
m = Number of Trials (m)

[MetricDisplayNames]
# Maps internal metric names to human-readable names for plots and reports.
mean_mrr = Mean Reciprocal Rank (MRR)
mean_top_1_acc = Top-1 Accuracy
mean_top_3_acc = Top-3 Accuracy
mean_effect_size_r = Effect Size (r)
mwu_stouffer_z = Stouffer's Z-score
mwu_fisher_chi2 = Fisher's Chi-Squared
mean_rank_of_correct_id = Mean Rank of Correct ID
n_valid_responses = Valid Responses
top1_pred_bias_std = Top-1 Prediction Bias (Std Dev)
true_false_score_diff = True vs. False Score Difference
bias_slope = Bias Slope
bias_p_value = Bias P-value

[Analysis]
# The minimum average number of valid responses per replication for a model to be
# included in the final statistical analysis. Models below this are excluded.
# Set to 0 to disable filtering. A value of 25 is a reasonable default to
# exclude models that failed to produce responses for most trials.
min_valid_response_threshold = 25
