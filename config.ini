# ==============================================================================
# Main Configuration for the Personality Matching Experiment
# ==============================================================================
# This file controls all parameters for the experimental pipeline, from
# data paths to model settings and analysis configurations.
# 
[Study]
# High-level parameters defining the entire experimental design.
# Number of replications to execute (r)
num_replications = 2
# Number of trials for each replication (m)
num_trials = 3
# Number of subjects in each group (k)
group_size = 4
# The ground truth mapping to use. Options: 'correct', 'random'.
mapping_strategy = random

[LLM]
# Parameters specific to the Language Model's behavior.
model_name = google/gemini-flash-1.5
temperature = 0.2
max_tokens = 8192
# The maximum number of concurrent API calls to make during a replication run.
# A higher number can significantly speed up the experiment but may hit API rate limits.
# Recommended value: 10
max_parallel_sessions = 10
# Processing times below assume max_parallel_sessions = 1 (sequential processing)
# Reliable models:
#   google/gemini-flash-1.5 (fast: 5.6h)
#   meta-llama/llama-4-maverick (fast: 5.9h)
#   mistralai/mistral-medium-3 (slow: 10.2h)
# Inexpensive models (under $15 for 3,000 queries):
#   deepseek/deepseek-chat-v3-0324 (very slow: 23.9h)
#   google/gemini-flash-1.5 (fast: 5.6h)
#   google/gemini-2.0-flash-001 (fast: 4.4h)
#   google/gemini-2.5-flash-preview-05-20 (fast: 4.5h)
#   google/gemma-3-27b-it (very slow: 20.6h)
#   meta-llama/llama-3.3-70b-instruct (slow: 14.3h)
#   meta-llama/llama-4-maverick (fast: 5.9h)
#   mistralai/mistral-medium-3 (slow: 10.2h)
#   mistralai/mistral-small-3.1-24b-instruct (fast: 5.2h)
#   nvidia/llama-3.1-nemotron-ultra-253b-v1 (extremely slow: 36.6h)
#   openai/gpt-4.1-mini (slow: 12.0h)
#   openai/gpt-4o-mini (slow: 12.1h)
#   qwen/qwen3-235b-a22b (extremely slow: ~60h)
#   x-ai/grok-3-mini-beta (slow: 11.7h)
# Thinking-only models:
#   deepseek/deepseek-r1-0528 ($13)
#   google/gemini-2.5-pro ($38)

[API]
# Global settings for the API provider.
api_endpoint = https://openrouter.ai/api/v1/chat/completions
referer_header = http://localhost:3000
api_timeout_seconds = 120

[General]
# Housekeeping settings for file and directory names.
# Base directory for all pipeline outputs
base_output_dir = output
# Subdirectory for query files generated by build_queries.py
queries_subdir = session_queries
# Subdirectory for LLM responses generated by conduct_llm_sessions.py
responses_subdir = session_responses
# Subdirectory for processed scores and mappings ready for meta-analysis
analysis_inputs_subdir = analysis_inputs
# Subdirectory under 'output' for run reports
reports_subdir = reports
# Subdirectory within 'base_output_dir' for new, timestamped experiments
new_experiments_subdir = new_experiments
# Prefix for timestamped experiment directories
experiment_dir_prefix = experiment_
# Top-level directory for run archives
archives_dir = archives
# Global verbosity level (INFO, DEBUG, WARNING, ERROR, CRITICAL) for logging
# Scripts can have their own --verbose flags to override this for a single run
default_log_level = INFO

[Filenames]
# Source files (relative to the script needing them, or resolved to be alongside scripts)
personalities_src = personalities_db.txt
base_query_src = base_query.txt
successful_indices_log = successful_query_indices.txt

# Tracking and aggregated files (names within their respective subdirectories)
used_indices_log = used_personality_indices.txt
aggregated_mappings_in_queries_dir = mappings.txt 

# Final files for meta-analysis script
all_scores_for_analysis = all_scores.txt
all_mappings_for_analysis = all_mappings.txt

# Temporary file used by build_queries.py and query_generator.py
temp_subset_personalities = temp_personalities_subset.txt
# Prefix for temporary files is created by query_generator.py
# qgen_temp_prefix

[MetaAnalysis]
# Default Top-K for accuracy in run_meta_analysis.py
default_top_k_accuracy = 3
# Delimiter expected by run_meta_analysis.py for its input files.
# 'None' means whitespace, or specify like ',' or '\t'.
# If process_responses.py outputs tab-delimited, this should be '\t'.
analysis_input_delimiter = \t 

[Schema]
# Defines the standard column names used across the analysis pipeline.

# Columns that define the experimental conditions (independent variables).
factors = model, mapping_strategy, temperature, k, m

# Columns that contain the performance results to be analyzed (dependent variables).
# ADDED: top1_pred_bias_std, true_false_score_diff
# ADDED: bias_slope, bias_p_value for analysis
metrics = mean_mrr,mean_top_1_acc,mean_top_3_acc,mean_mrr_lift,mean_top_1_acc_lift,mean_top_3_acc_lift,mean_effect_size_r,mwu_stouffer_z,mwu_fisher_chi2,mean_rank_of_correct_id,n_valid_responses,top1_pred_bias_std,true_false_score_diff,bias_slope,bias_p_value

# The full, ordered list of columns for the final CSV output.
# This ensures every generated summary file has a consistent layout.
# ADDED: The new bias metrics to the end of the header.
csv_header_order = run_directory,replication,n_valid_responses,model,mapping_strategy,temperature,k,m,db,mwu_stouffer_z,mwu_stouffer_p,mwu_fisher_chi2,mwu_fisher_p,mean_effect_size_r,effect_size_r_p,mean_mrr,mrr_p,mean_top_1_acc,top_1_acc_p,mean_top_3_acc,top_3_acc_p,mean_mrr_lift,mean_top_1_acc_lift,mean_top_3_acc_lift,mean_rank_of_correct_id,rank_of_correct_id_p,top1_pred_bias_std,true_false_score_diff,bias_slope,bias_intercept,bias_r_value,bias_p_value,bias_std_err

[ModelNormalization]
# Maps raw keywords found in run directories to a single, canonical internal name.
# This version systematically includes both hyphenated and underscored variants for maximum robustness.
# format: canonical_name = comma,separated,keyword(s)
deepseek-v3                      = deepseek-chat-v3, deepseek_chat_v3
google-gemini-2-0-flash          = gemini-2.0-flash, gemini_2_0_flash
google-gemini-2-5-flash          = gemini-2.5-flash, gemini_2_5_flash
google-gemini-flash-1-5          = gemini-flash-1.5, gemini_flash_1_5
google-gemma-3-27b               = gemma-3-27b, gemma_3_27b
meta-llama-3-3-70b               = llama-3.3-70b, llama_3_3_70b
meta-llama-4-maverick            = llama-4-maverick, llama_4_maverick
mistralai-mistral-medium-3       = mistral-medium-3, mistral_medium_3
mistralai-mistral-small-3-1      = mistral-small-3.1, mistral_small_3_1
nvidia-llama-3-1-nemotron-ultra  = nemotron-ultra
openai-gpt-4-1-mini              = gpt-4.1-mini, gpt_4.1_mini
openai-gpt-4o-mini               = gpt-4o-mini, gpt_4o_mini
qwen-qwen3-235b                  = qwen3-235b, qwen_235b
xai-grok-3-mini-beta             = grok-3-mini-beta, grok_3_mini_beta

[ModelDisplayNames]
# Maps the canonical internal names to human-readable names for plots and reports.
# The keys here MUST exactly match the keys in the [ModelNormalization] section.
# format: canonical_name = Friendly Display Name
deepseek-v3                      = DeepSeek V3
google-gemini-2-0-flash          = Gemini 2.0 Flash
google-gemini-2-5-flash          = Gemini 2.5 Flash
google-gemini-flash-1-5          = Gemini 1.5 Flash
google-gemma-3-27b               = Gemma 3 27B
meta-llama-3-3-70b               = Llama 3.3 70B
meta-llama-4-maverick            = Llama 4 Maverick
mistralai-mistral-medium-3       = Mistral Medium 3
mistralai-mistral-small-3-1      = Mistral Small 3.1
nvidia-llama-3-1-nemotron-ultra  = Llama 3.1 Nemotron Ultra 253B
openai-gpt-4-1-mini              = GPT 4.1 mini
openai-gpt-4o-mini               = GPT 4o mini
qwen-qwen3-235b                  = Qwen3 235B
xai-grok-3-mini-beta             = Grok 3 Mini

[ConfigCompatibility]
# This section provides a fallback map for legacy configuration files.
# The format is: canonical_name = new_section:new_key, old_section:old_key
# Scripts will try these locations in order, with the most modern format listed first.

model_name = LLM:model_name, Model:model_name, LLM:model
num_trials = Study:num_trials, Study:num_iterations
num_subjects = Study:group_size, Study:num_subjects, Study:k_per_query
mapping_strategy = Study:mapping_strategy
personalities_db_path = Filenames:personalities_src, General:personalities_db_path

[FactorDisplayNames]
# Maps internal factor names to human-readable names for plots and reports.
model = Model
mapping_strategy = Mapping Strategy
temperature = Temperature
k = Group Size (k)
m = Number of Trials (m)

[MetricDisplayNames]
# Maps internal metric names to human-readable names for plots and reports.
mean_mrr = Mean Reciprocal Rank (MRR)
mean_top_1_acc = Top-1 Accuracy
mean_top_3_acc = Top-3 Accuracy
mean_mrr_lift = MRR Lift (vs. Chance)
mean_top_1_acc_lift = Top-1 Accuracy Lift (vs. Chance)
mean_top_3_acc_lift = Top-3 Accuracy Lift (vs. Chance)
mean_effect_size_r = Effect Size (r)
mwu_stouffer_z = Stouffer's Z-score
mwu_fisher_chi2 = Fisher's Chi-Squared
mean_rank_of_correct_id = Mean Rank of Correct ID
n_valid_responses = Valid Responses
top1_pred_bias_std = Top-1 Prediction Bias (Std Dev)
true_false_score_diff = True vs. False Score Difference
bias_slope = Bias Slope
bias_p_value = Bias P-value

[Analysis]
# The minimum average number of valid responses per replication for a model to be
# included in the final statistical analysis. Models below this are excluded.
# Set to 0 to disable filtering. A value of 25 is a reasonable default to
# exclude models that failed to produce responses for most trials.
min_valid_response_threshold = 25

[DataGeneration]
# Settings for scripts that generate foundational data assets.

# Methodological Controls
# ---
# Setting this to 'true' will cause the data pipeline to bypass the LLM-based
# eminence and OCEAN scoring steps. The `select_final_candidates.py` script
# will instead use the full list of eligible candidates as the final subject pool.
# This provides a methodological control for researchers who wish to validate
# the findings without the potential confound of LLM-based subject selection.
bypass_llm_scoring = false

# Eminence Score Generation Settings
# ---
# The LLM to use for eminence scoring.
eminence_model = openai/gpt-5-chat
# Number of subjects per API call for eminence scoring.
eminence_batch_size = 100
# Path for the final eminence scores CSV file.
eminence_scores_output = data/foundational_assets/eminence_scores.csv
# Path to the raw, tab-delimited file exported from ADB.
raw_adb_export_input = data/sources/adb_raw_export.txt

# OCEAN Score Generation Settings
# ---
# Path for the final OCEAN scores CSV file.
ocean_scores_output = data/foundational_assets/ocean_scores.csv
# Name of the LLM to use for OCEAN scoring.
ocean_model = anthropic/claude-sonnet-4
# Number of subjects per API call for OCEAN scoring.
ocean_batch_size = 50
# Variance-based cutoff: stop if avg variance of the analysis window is < this % of overall variance.
variance_cutoff_percentage = 0.40
# The number of recent analysis windows to consider for the cutoff decision (N).
variance_check_window = 5
# The number of windows within the check window that must be below the threshold to stop (M).
variance_trigger_count = 4
# The fixed number of new entries to use for each variance analysis check.
variance_analysis_window = 100
# The number of top-ranked subjects to use for establishing the benchmark variance.
benchmark_population_size = 500
# The minimum number of subjects to process before the variance cutoff logic is activated.
cutoff_start_point = 600

# Delineation Neutralization Settings
# ---
# Name of the LLM to use for neutralizing delineations.
neutralization_model = google/gemini-2.5-pro
# Comma-separated list of points to include in the 'points_in_signs' output.
points_for_neutralization = Sun, Moon, Mercury, Venus, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto, Ascendant, Midheaven