#!/usr/bin/env python3
#-*- coding: utf-8 -*-
#
# A Framework for Testing Complex Narrative Systems
# Copyright (C) 2025 Peter J. Marko
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
#
# Filename: src/run_bias_analysis.py

"""
Stage 4 (Part 2): Diagnostic Bias Analyzer.

This script is a secondary analysis module that completes the unified
"Comprehensive Analysis" stage. Called by the orchestrator immediately after
the core performance analysis, its purpose is to answer diagnostic questions
about *how* the LLM performed the task, not just how well.

It reads the raw `all_scores.txt` and `all_mappings.txt` files to calculate
positional bias metrics. It then reads the `replication_metrics.json` file
(generated by `analyze_llm_performance.py`), injects its new metrics, and
overwrites the file. This augmented JSON becomes the final, authoritative
source of all metrics for the replication run.

Key metrics calculated:
- top1_pred_bias_std: The standard deviation of top-1 predictions across all
  possible positions, indicating choice concentration.
- true_false_score_diff: The mean difference between scores for true matches
  and scores for false matches.
"""

import argparse
import os
import pandas as pd
import numpy as np
import logging
import json
import glob
from io import StringIO

logging.basicConfig(level=logging.INFO, format='%(levelname)s (run_bias_analysis): %(message)s')

def build_long_format_df(replication_dir, k_value):
    """Builds the detailed DataFrame for a single replication, validating against k."""
    analysis_dir = os.path.join(replication_dir, "analysis_inputs")
    scores_file = os.path.join(analysis_dir, "all_scores.txt")
    mappings_file = os.path.join(analysis_dir, "all_mappings.txt")

    if not all(os.path.exists(p) for p in [scores_file, mappings_file]):
        logging.warning(f"Missing scores or mappings file in {analysis_dir}.")
        return None

    try:
        # Use a context manager for reading the file
        with open(scores_file, 'r') as f:
            content = f.read().strip()
        # Handle empty file case
        if not content:
            score_matrices = []
        else:
            # Manually split by the double newline, then process each block.
            # This is more robust than relying on loadtxt's newline handling.
            blocks = content.strip().split('\n\n')
            score_matrices = [np.loadtxt(StringIO(block)) for block in blocks if block.strip()]
        
        with open(mappings_file, 'r') as f:
            mappings_list = [list(map(int, line.strip().split())) for line in f if line.strip() and line.strip()[0].isdigit()]
    except Exception as e:
        logging.error(f"Could not read score/mapping files in {analysis_dir}: {e}")
        return None

    all_points = []
    
    # Ensure we don't process more matrices than we have mappings for
    num_trials = min(len(score_matrices), len(mappings_list))
    if len(score_matrices) != len(mappings_list):
        logging.warning(f"Mismatch between number of score matrices ({len(score_matrices)}) and mappings ({len(mappings_list)}). Processing the minimum ({num_trials}).")

    for i in range(num_trials):
        matrix = score_matrices[i]
        true_map = mappings_list[i]

        # Validate that the matrix is 2D and has the expected shape
        if matrix.ndim != 2 or matrix.shape != (k_value, k_value):
            logging.warning(f"Matrix {i} in {scores_file} has shape {matrix.shape}, expected ({k_value}, {k_value}). Skipping.")
            continue
        for row_idx in range(matrix.shape[0]):
            true_col_for_row = true_map[row_idx]
            max_score = np.max(matrix[row_idx, :])
            for col_idx in range(matrix.shape[1]):
                all_points.append({
                    'person_row': row_idx + 1,
                    'desc_col': col_idx + 1,
                    'score': matrix[row_idx, col_idx],
                    'is_true_match': (col_idx + 1 == true_col_for_row),
                    'is_top_1': (matrix[row_idx, col_idx] == max_score)
                })
    return pd.DataFrame(all_points)

def calculate_bias_metrics(df, k_value):
    """Calculates numerical summary metrics for bias."""
    if df is None or df.empty: return {}
    
    num_trials = len(df) / (k_value * k_value)
    if num_trials == 0: return {}

    # Metric 1: Std Dev of top-1 prediction proportions across columns
    top1_props = df[df['is_top_1']].groupby('desc_col').size() / num_trials
    top1_props = top1_props.reindex(range(1, k_value + 1), fill_value=0)
    top1_pred_bias_std = top1_props.std()

    # Metric 2: Difference between mean score of true matches and false matches
    true_scores = df[df['is_true_match']]['score']
    false_scores = df[~df['is_true_match']]['score']
    true_false_score_diff = true_scores.mean() - false_scores.mean() if not true_scores.empty and not false_scores.empty else 0

    return {
        "top1_pred_bias_std": top1_pred_bias_std,
        "true_false_score_diff": true_false_score_diff,
    }

def main():
    parser = argparse.ArgumentParser(description="Calculate bias metrics and update a replication report.")
    parser.add_argument("replication_dir", help="Path to the replication run directory.")
    parser.add_argument("--k_value", type=int, required=True, help="The 'k' dimension (num_subjects).")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose (INFO level) logging.")
    args = parser.parse_args()

    # If --verbose is NOT specified, suppress INFO logs.
    if not args.verbose:
        logging.getLogger().setLevel(logging.WARNING)

    metrics_filepath = os.path.join(args.replication_dir, "analysis_inputs", "replication_metrics.json")
    if not os.path.exists(metrics_filepath):
        logging.error(f"Metrics file not found at {metrics_filepath}. Aborting bias analysis.")
        # Create a dummy report to prevent the orchestrator from thinking this stage needs to be re-run.
        # This indicates a failure upstream that should be fixed there.
        report_files = glob.glob(os.path.join(args.replication_dir, "replication_report_*.txt"))
        if not report_files:
             # If no report exists at all, create one noting the failure.
            fail_report_path = os.path.join(args.replication_dir, f"replication_report_{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}_FAILED.txt")
            with open(fail_report_path, 'w') as f:
                f.write("Bias analysis failed: metrics JSON file not found.")
        return

    df_long = build_long_format_df(args.replication_dir, args.k_value)

    if df_long is None or df_long.empty:
        logging.warning("DataFrame is empty or could not be built. Writing null bias metrics.")
        bias_metrics = {
            "top1_pred_bias_std": None,
            "true_false_score_diff": None,
        }
    else:
        bias_metrics = calculate_bias_metrics(df_long, args.k_value)

    try:
        with open(metrics_filepath, 'r', encoding='utf-8') as f:
            report_data = json.load(f)

        # Inject the new metrics
        report_data["positional_bias_metrics"] = bias_metrics

        with open(metrics_filepath, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=4)
        logging.info(f"Successfully updated metrics with bias analysis in {metrics_filepath}")

    except Exception as e:
        logging.error(f"Failed to read/write metrics file {metrics_filepath}: {e}", exc_info=True)

if __name__ == "__main__":
    main()

# === End of src/run_bias_analysis.py ===
