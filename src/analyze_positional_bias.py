#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Filename: src/analyze_positional_bias.py

"""
This script performs a retrospective analysis on the outputs of an LLM personality
matching experiment to detect potential biases related to presentation order
and other factors that might lead to performance below random chance.

It leverages the structured data generated by the experiment pipeline's
`process_llm_responses.py` and `build_queries.py` scripts, specifically
the LLM's score matrices, ground truth mappings, and the original query/manifest
files that preserve input presentation order.

Key Features:
- **Data Augmentation**: Combines LLM assigned scores with detailed positional
  information (row and column in the prompt) and ground truth for each score.
- **Positional Bias in Assigned Scores**: Analyzes and visualizes if the LLM
  assigns systematically higher or lower scores based on the position of persons
  (List A rows) or descriptions (List B columns) in the input prompt.
  - Generates bar plots for average scores by row and column position.
  - Creates a heatmap showing average scores across the 10x10 prompt grid.
- **Positional Bias in Top-1 Predictions**: Investigates if the LLM systematically
  favors or avoids certain description column positions (ID X) when making its
  top-1 prediction, which can explain anti-correlation with random performance.
  - Generates a bar plot showing the proportion of top-1 predictions for each
    column position, compared to random chance.
- **Score Distribution for True vs. False Matches**: Compares the distribution
  of scores assigned to actual correct pairs versus incorrect pairs. This helps
  understand if the LLM genuinely differentiates between them or if it assigns
  higher scores to incorrect matches.
  - Generates a Kernel Density Estimate (KDE) plot for score distributions.
  - Prints summary statistics (mean, std) for true vs. false match scores.
- **Automated Plot Generation**: Saves all generated plots as PNG files within
  a dedicated `bias_analysis_plots` subdirectory inside the specified run's output.

Input Files (read from `<run_output_dir>`):
- `<run_output_dir>/analysis_inputs/all_scores.txt`: Aggregated LLM score matrices.
- `<run_output_dir>/analysis_inputs/all_mappings.txt`: Aggregated ground truth mappings.
- `<run_output_dir>/analysis_inputs/successful_query_indices.txt`: List of successfully processed query indices.
- `<run_output_dir>/session_queries/llm_query_XXX.txt`: Original LLM query files (for input order).
- `<run_output_dir>/session_queries/llm_query_XXX_manifest.txt`: Manifest files (linking presented items to true IDs).

Output Files (written to `<run_output_dir>/bias_analysis_plots/`):
- `avg_score_by_row_position.png`: Bar plot of average scores by List A row position.
- `avg_score_by_col_position.png`: Bar plot of average scores by List B column position.
- `avg_score_heatmap.png`: Heatmap of average scores by prompt row and column.
- `top1_prediction_frequency_by_col.png`: Bar plot of top-1 prediction frequency by List B column.
- `score_distribution_true_vs_false.png`: KDE plot of score distributions for true vs. false matches.

Usage:
To run the analysis, provide the absolute path to a specific experiment run's
output directory. This directory should contain the `analysis_inputs` and
`session_queries` subdirectories with the necessary data files.

Example:
    python analyze_retrospective_bias.py --run_output_dir "output/reports/exp_mistral_random_map"
"""

import argparse
import os
import re
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import logging
from io import StringIO
import glob # Added for finding replication directories

# --- Configuration and Logging (Mimic project's structure) ---
# Assuming config_loader.py is in the same 'src' directory or accessible via sys.path
try:
    from config_loader import APP_CONFIG, get_config_value, PROJECT_ROOT
except ImportError:
    # Fallback for standalone execution if config_loader isn't directly available
    class DummyConfig:
        def has_option(self, section, key): return False
        def get(self, section, key, fallback=None): return fallback
    APP_CONFIG = DummyConfig()
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    def get_config_value(cfg, section, key, fallback=None, value_type=str): return fallback

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

# --- Helper Functions (adapted from project scripts or new) ---

def read_successful_indices(filepath):
    """Reads the list of original query indices that were successfully processed."""
    if not os.path.exists(filepath):
        logging.error(f"Error: Successful indices file not found at '{filepath}'.")
        return None
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            indices = [int(line.strip()) for line in f if line.strip()]
        return indices
    except (ValueError, IOError) as e:
        logging.error(f"Error reading or parsing successful indices file '{filepath}': {e}")
        return None

def read_score_matrices(filepath, expected_k, delimiter_char=None):
    """
    Reads score matrices from a file, robustly handling various formats.
    Adapted from analyze_performance.py.
    """
    if expected_k is None or expected_k <= 0:
        logging.error(f"Invalid expected_k ({expected_k}) for reading score matrices.")
        return None

    matrices = []
    current_matrix_str_rows = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line_content in enumerate(f, 1):
                line = line_content.strip()
                if not line:
                    if current_matrix_str_rows:
                        if len(current_matrix_str_rows) == expected_k:
                            try:
                                matrix_data_float = []
                                for row_str_list_of_str in current_matrix_str_rows:
                                    matrix_data_float.append([float(x.strip()) for x in row_str_list_of_str])
                                matrix = np.array(matrix_data_float)
                                if matrix.shape == (expected_k, expected_k):
                                    matrices.append(matrix)
                                else:
                                    logging.warning(f"Matrix shape mismatch (file: {filepath}, end ~L{line_num}): Shape {matrix.shape}, expected ({expected_k},{expected_k}). Skipping.")
                            except ValueError:
                                logging.warning(f"Non-float data in matrix (file: {filepath}, end ~L{line_num}). Skipping.")
                        else:
                            logging.warning(f"Incomplete matrix block (file: {filepath}, end ~L{line_num}): {len(current_matrix_str_rows)} lines, expected {expected_k}. Skipping.")
                        current_matrix_str_rows = []
                else:
                    row_items_str_cleaned = None
                    if line.startswith('|'): # Markdown table format
                        if '---' in line: continue
                        parts = [p.strip() for p in line.strip('|').split('|')]
                        if len(parts) > 1:
                            try:
                                float(parts[1]) # Heuristic: if second element is number, it's data
                                row_items_str_cleaned = parts[1:]
                            except (ValueError, IndexError):
                                continue # Likely header row
                        else: continue # Not a valid data line
                    else: # Standard delimited formats
                        parts = line.split(delimiter_char) if delimiter_char else line.split()
                        try:
                            [float(p.strip()) for p in parts]
                            row_items_str_cleaned = [p.strip() for p in parts]
                        except ValueError:
                            if len(parts) > 1:
                                try:
                                    [float(p.strip()) for p in parts[1:]]
                                    row_items_str_cleaned = [p.strip() for p in parts[1:]]
                                except (ValueError, IndexError):
                                    continue # Still fails, malformed
                            else: continue # Not enough columns

                    if row_items_str_cleaned is None: continue

                    if len(row_items_str_cleaned) != expected_k:
                        logging.warning(f"Column count mismatch (file: {filepath}, L{line_num}): {len(row_items_str_cleaned)} cols, expected {expected_k}. Skipping block.")
                        current_matrix_str_rows = []
                        while line.strip(): # Consume rest of malformed block
                            try: line = next(f,'').strip()
                            except StopIteration: break
                        continue
                    current_matrix_str_rows.append(row_items_str_cleaned)

            if current_matrix_str_rows: # Process last block if file ends without blank line
                if len(current_matrix_str_rows) == expected_k:
                    try:
                        matrix_data_float = [[float(x.strip()) for x in row_str_list] for row_str_list in current_matrix_str_rows]
                        matrix = np.array(matrix_data_float)
                        if matrix.shape == (expected_k, expected_k):
                             matrices.append(matrix)
                        else:
                             logging.warning(f"Last matrix block shape mismatch (file: {filepath}, EOF): Shape {matrix.shape}, expected ({expected_k},{expected_k}). Skipping.")
                    except ValueError:
                        logging.warning(f"Non-float data in last matrix block (file: {filepath}, EOF). Skipping.")
                else:
                    logging.warning(f"Incomplete last matrix block (file: {filepath}, EOF): {len(current_matrix_str_rows)} lines, expected {expected_k}. Skipping.")

    except FileNotFoundError:
        logging.error(f"Score matrices file not found at {filepath}")
        return None
    if not matrices:
        logging.warning(f"No valid matrices loaded from: {filepath}, k={expected_k}.")
    return matrices

def read_mappings_and_deduce_k(filepath, k_override=None, specified_delimiter_keyword=None):
    """
    Reads mappings from a file and deduces k if not provided.
    Adapted from analyze_performance.py.
    """
    mappings_list = []
    actual_delimiter_char_used = None

    delimiter_char_to_try_first = None
    if specified_delimiter_keyword:
        if specified_delimiter_keyword == ',': delimiter_char_to_try_first = ','
        elif specified_delimiter_keyword and specified_delimiter_keyword.lower() == 'tab': delimiter_char_to_try_first = '\t'
        elif specified_delimiter_keyword and specified_delimiter_keyword.lower() == 'space': delimiter_char_to_try_first = ' '

    actual_delimiter_to_parse_with = delimiter_char_to_try_first

    lines_for_detection_and_first_data = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for _ in range(15):
                line = f.readline()
                if not line: break
                stripped_line = line.strip()
                if stripped_line: lines_for_detection_and_first_data.append(stripped_line)
            if not lines_for_detection_and_first_data:
                logging.error(f"Mappings file {filepath} is empty.")
                return None, None, None
    except FileNotFoundError:
        logging.error(f"Mappings file not found at {filepath}")
        return None, None, None

    # Auto-detect delimiter if not specified
    if actual_delimiter_to_parse_with is None:
        first_line_sample = lines_for_detection_and_first_data[0]
        is_header = False
        # Heuristic to check if first line is a header (contains non-numeric parts or specific keywords)
        if re.search(r'[a-zA-Z_]', first_line_sample) or (first_line_sample.count('\t') > 0 and 'Map_idx' in first_line_sample):
            is_header = True

        if is_header:
            if '\t' in first_line_sample: actual_delimiter_to_parse_with = '\t'
            elif ',' in first_line_sample: actual_delimiter_to_parse_with = ','
        else: # Try to deduce from data lines
            for delim_char_candidate in ['\t', ',']: # Prefer tab, then comma
                temp_k_for_this_delim = None
                consistent_k_count = 0
                parsed_lines_with_this_delim = 0
                for line_sample in lines_for_detection_and_first_data:
                    try:
                        parts = line_sample.split(delim_char_candidate)
                        items = [int(p.strip()) for p in parts if p.strip()]
                        if items:
                            parsed_lines_with_this_delim += 1
                            if temp_k_for_this_delim is None: temp_k_for_this_delim = len(items)
                            if temp_k_for_this_delim == len(items): consistent_k_count += 1
                    except ValueError:
                        pass # Not purely numeric with this delimiter

                if temp_k_for_this_delim and parsed_lines_with_this_delim > 0 and \
                   (consistent_k_count / parsed_lines_with_this_delim) >= 0.8:
                    actual_delimiter_to_parse_with = delim_char_candidate
                    break
        if actual_delimiter_to_parse_with is None:
            logging.info(f"Could not auto-detect comma or tab delimiter for {filepath}. Assuming whitespace.")

    final_k_to_use = k_override
    if final_k_to_use is None:
        start_idx_k_deduce = 1 if is_header else 0 # Skip header if detected
        if start_idx_k_deduce < len(lines_for_detection_and_first_data):
            for line_sample in lines_for_detection_and_first_data[start_idx_k_deduce:]:
                try:
                    parts = line_sample.split(actual_delimiter_to_parse_with) if actual_delimiter_to_parse_with else line_sample.split()
                    items = [int(p.strip()) for p in parts if p.strip()]
                    if items:
                        final_k_to_use = len(items)
                        break
                except ValueError:
                    pass
        if final_k_to_use is None or final_k_to_use <= 0:
            logging.error(f"Could not deduce a valid k > 0 from {filepath}.")
            return None, None, actual_delimiter_to_parse_with
        logging.info(f"Deduced k as {final_k_to_use} for {filepath}.")
    else:
        logging.info(f"Using overridden k={final_k_to_use} for {filepath}.")


    # Re-read entire file for parsing all data lines
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            if is_header: f.readline() # Skip header if detected
            
            for line_num, line_content in enumerate(f, 1):
                line = line_content.strip()
                if line:
                    try:
                        parts = line.split(actual_delimiter_to_parse_with) if actual_delimiter_to_parse_with else line.split()
                        parsed_indices = [int(p.strip()) for p in parts if p.strip()]
                        if parsed_indices and len(parsed_indices) == final_k_to_use:
                            mappings_list.append(parsed_indices)
                        else:
                            logging.warning(f"Line {line_num} in {filepath} has {len(parsed_indices)} elements, expected {final_k_to_use}. Skipping: '{line}'")
                    except ValueError:
                        logging.warning(f"Could not parse line {line_num} in {filepath} as data. Skipping: '{line}'")
    except FileNotFoundError:
        logging.error(f"Mappings file not found at {filepath} (during full read).")
        return None, None, None

    if not mappings_list:
        logging.error(f"No valid mapping data lines found in {filepath} for k={final_k_to_use}.")
        return None, final_k_to_use, actual_delimiter_to_parse_with

    return mappings_list, final_k_to_use, actual_delimiter_to_parse_with


def parse_llm_query_file(query_filepath):
    """
    Parses llm_query_XXX.txt to get ordered List A names and List B ID-description pairs.
    """
    list_a_names = []
    list_b_descriptions = []
    in_list_a = False
    in_list_b = False

    try:
        with open(query_filepath, 'r', encoding='utf-8') as f:
            for line in f:
                stripped_line = line.strip()
                if not stripped_line:
                    continue

                if stripped_line.lower() == "list a":
                    in_list_a = True
                    in_list_b = False
                    continue
                if stripped_line.lower() == "list b":
                    in_list_a = False
                    in_list_b = True
                    continue

                if in_list_a:
                    list_a_names.append(stripped_line)
                elif in_list_b:
                    match = re.match(r"ID (\d+):\s*(.*)", stripped_line)
                    if match:
                        list_b_descriptions.append({
                            'id_in_prompt': int(match.group(1)),
                            'description_text': match.group(2)
                        })
        return list_a_names, list_b_descriptions
    except FileNotFoundError:
        logging.error(f"Query file not found: {query_filepath}")
        return None, None
    except Exception as e:
        logging.error(f"Error parsing query file {query_filepath}: {e}")
        return None, None

def parse_manifest_file(manifest_filepath):
    """
    Parses llm_query_XXX_manifest.txt into a DataFrame.
    Header: Name_in_Query Name_Ref_ID Shuffled_Desc_Index Desc_Ref_ID Desc_in_Query
    """
    try:
        manifest_df = pd.read_csv(manifest_filepath, sep='\t', encoding='utf-8')
        return manifest_df
    except FileNotFoundError:
        logging.error(f"Manifest file not found: {manifest_filepath}")
        return None
    except Exception as e:
        logging.error(f"Error parsing manifest file {manifest_filepath}: {e}")
        return None

# --- Main Analysis Function ---
def analyze_positional_bias(run_output_dir):
    logging.info(f"Starting positional bias analysis for run: {run_output_dir}")

    # --- Path Resolution ---
    analysis_inputs_subdir_cfg = get_config_value(APP_CONFIG, 'General', 'analysis_inputs_subdir', fallback="analysis_inputs")
    queries_subdir_cfg = get_config_value(APP_CONFIG, 'General', 'queries_subdir', fallback="session_queries")

    output_plots_dir = os.path.join(run_output_dir, "bias_analysis_plots")
    os.makedirs(output_plots_dir, exist_ok=True)

    # List to store processed data for each individual trial (query)
    # This will hold (trial_id, score_matrix, true_mapping_for_trial, query_filepath, manifest_filepath)
    processed_trials_data = []
    k_value = None # k_value will be deduced from the first valid replication

    # Find all replication subdirectories (those starting with 'run_')
    replication_dirs_pattern = os.path.join(run_output_dir, "run_*")
    replication_paths = sorted(glob.glob(replication_dirs_pattern)) # Sort for consistent processing order

    if not replication_paths:
        logging.error(f"No replication directories found matching '{replication_dirs_pattern}'. Exiting.")
        return

    logging.info(f"Found {len(replication_paths)} replication directories in {run_output_dir}.")

    for rep_path in replication_paths:
        rep_name = os.path.basename(rep_path) # Get just the directory name (e.g., run_20250619...)
        current_analysis_inputs_dir = os.path.join(rep_path, analysis_inputs_subdir_cfg)
        current_queries_dir = os.path.join(rep_path, queries_subdir_cfg)

        # Construct full paths to files within this specific replication's analysis_inputs
        current_scores_filepath = os.path.join(current_analysis_inputs_dir, get_config_value(APP_CONFIG, 'Filenames', 'all_scores_for_analysis', fallback="all_scores.txt"))
        current_mappings_filepath = os.path.join(current_analysis_inputs_dir, get_config_value(APP_CONFIG, 'Filenames', 'all_mappings_for_analysis', fallback="all_mappings.txt"))
        current_successful_indices_filepath = os.path.join(current_analysis_inputs_dir, get_config_value(APP_CONFIG, 'Filenames', 'successful_indices_log', fallback="successful_query_indices.txt"))

        # Check if all required files exist for the current replication
        missing_files = []
        if not os.path.exists(current_successful_indices_filepath):
            missing_files.append(current_successful_indices_filepath)
        if not os.path.exists(current_scores_filepath):
            missing_files.append(current_scores_filepath)
        if not os.path.exists(current_mappings_filepath):
            missing_files.append(current_mappings_filepath)

        if missing_files:
            logging.warning(f"Skipping replication '{rep_name}' due to missing files: {', '.join(missing_files)}")
            continue

        logging.info(f"Loading data from replication: {rep_name}")

        try:
            # Load successful query indices for this replication
            current_successful_indices = read_successful_indices(current_successful_indices_filepath)
            if not current_successful_indices:
                logging.warning(f"No successful queries found in '{current_successful_indices_filepath}'. Skipping replication.")
                continue

            # Deduce k_value if not already determined (from the first valid replication)
            if k_value is None:
                if current_successful_indices:
                    first_manifest_path_in_rep = os.path.join(current_queries_dir, f"llm_query_{current_successful_indices[0]:03d}_manifest.txt")
                    first_query_path_in_rep = os.path.join(current_queries_dir, f"llm_query_{current_successful_indices[0]:03d}.txt")
                    if os.path.exists(first_manifest_path_in_rep):
                        manifest_df_temp = parse_manifest_file(first_manifest_path_in_rep)
                        if manifest_df_temp is not None:
                            k_value = len(manifest_df_temp)
                    elif os.path.exists(first_query_path_in_rep):
                        list_a_names_temp, _ = parse_llm_query_file(first_query_path_in_rep)
                        if list_a_names_temp:
                            k_value = len(list_a_names_temp)
                if k_value is None:
                    logging.warning(f"Could not determine k_value from replication '{rep_name}'. Skipping.")
                    continue # Cannot proceed without k_value
                else:
                    logging.info(f"Deduced k_value as {k_value} from replication '{rep_name}'.")


            # Load mappings and score matrices for this replication
            current_mappings_list, _, _ = read_mappings_and_deduce_k(current_mappings_filepath, k_override=k_value)
            current_score_matrices = read_score_matrices(current_scores_filepath, expected_k=k_value)

            if not current_mappings_list or not current_score_matrices or len(current_mappings_list) != len(current_score_matrices) or len(current_successful_indices) != len(current_score_matrices):
                logging.warning(f"Data count mismatch (indices, scores, mappings) for replication '{rep_name}'. Expected {len(current_successful_indices)} trials, got {len(current_score_matrices)} scores and {len(current_mappings_list)} mappings. Skipping.")
                continue

            # Store data for each trial in this replication
            for i, trial_id in enumerate(current_successful_indices):
                processed_trials_data.append({
                    'trial_id': trial_id,
                    'score_matrix': current_score_matrices[i],
                    'true_mapping': current_mappings_list[i],
                    'query_filepath': os.path.join(current_queries_dir, f"llm_query_{trial_id:03d}.txt"),
                    'manifest_filepath': os.path.join(current_queries_dir, f"llm_query_{trial_id:03d}_manifest.txt")
                })

        except Exception as e:
            logging.error(f"Error processing replication '{rep_name}': {e}")
            continue # Continue to the next replication if an error occurs

    if not processed_trials_data:
        logging.error("No valid trial data collected across any replication. Exiting.")
        return

    logging.info(f"Aggregated {len(processed_trials_data)} successful queries from all processed replications with k={k_value}.")

    # --- Build Long Format DataFrame ---
    all_data_for_analysis = []
    logging.info("Starting to build long-format DataFrame from aggregated trial data...")

    for trial_data in processed_trials_data:
        trial_id = trial_data['trial_id']
        score_matrix = trial_data['score_matrix']
        true_mapping_for_trial = trial_data['true_mapping']
        query_filepath = trial_data['query_filepath']
        manifest_filepath = trial_data['manifest_filepath']

        list_a_names, list_b_descriptions = parse_llm_query_file(query_filepath)
        manifest_df = parse_manifest_file(manifest_filepath)

        if list_a_names is None or list_b_descriptions is None or manifest_df is None:
            logging.warning(f"Skipping trial {trial_id} due to missing or unparsable query/manifest files in its replication folder.")
            continue

        # Create lookup for original ref IDs based on prompt positions
        # Name_in_Query (from manifest) is in order of List A (row position)
        # Shuffled_Desc_Index (from manifest) is the ID X (column position)
        person_ref_id_lookup = {
            row_idx + 1: manifest_df.loc[row_idx, 'Name_Ref_ID']
            for row_idx in range(k_value) # k_value is now determined from the first valid replication
        }
        desc_ref_id_lookup = {
            row['id_in_prompt']: manifest_df[manifest_df['Shuffled_Desc_Index'] == row['id_in_prompt']]['Desc_Ref_ID'].iloc[0]
            for row in list_b_descriptions
        }

        for row_idx in range(k_value): # 0-indexed for numpy
            person_prompt_row_position = row_idx + 1
            person_original_ref_id = person_ref_id_lookup.get(person_prompt_row_position)
            
            # Find the true column position for this person_prompt_row_position
            # true_mapping_for_trial is 1-based column index for each row
            true_col_position_for_this_row = true_mapping_for_trial[row_idx]

            # Determine LLM's top 1 prediction for this row
            max_score_in_row = np.max(score_matrix[row_idx, :])
            
            for col_idx in range(k_value): # 0-indexed for numpy
                description_prompt_col_position = col_idx + 1
                description_original_ref_id = desc_ref_id_lookup.get(description_prompt_col_position)
                
                llm_assigned_score = score_matrix[row_idx, col_idx]

                is_true_match = (description_prompt_col_position == true_col_position_for_this_row)
                is_llm_top_1_prediction_for_row = (llm_assigned_score == max_score_in_row) # Handles ties by marking all as top-1

                all_data_for_analysis.append({
                    'trial_id': trial_id,
                    'person_prompt_row_position': person_prompt_row_position,
                    'person_original_ref_id': person_original_ref_id,
                    'description_prompt_col_position': description_prompt_col_position,
                    'description_original_ref_id': description_original_ref_id,
                    'llm_assigned_score': llm_assigned_score,
                    'is_true_match': is_true_match,
                    'is_llm_top_1_prediction_for_row': is_llm_top_1_prediction_for_row,
                })

    if not all_data_for_analysis:
        logging.error("No data collected for analysis. Exiting.")
        return

    logging.info(f"Collected {len(all_data_for_analysis)} raw data points. Converting to DataFrame...")
    df_analysis = pd.DataFrame(all_data_for_analysis)
    logging.info(f"DataFrame built with {len(df_analysis)} rows for analysis.")

    # --- Cast positional columns to integer type to avoid seaborn/matplotlib warnings ---
    # The columns 'person_prompt_row_position' and 'description_prompt_col_position'
    # are intended to be numerical for plotting, but pandas might infer them as
    # generic objects or strings, causing warnings in plotting libraries.
    # Explicitly casting them to 'int' resolves this.
    df_analysis['person_prompt_row_position'] = df_analysis['person_prompt_row_position'].astype(int)
    df_analysis['description_prompt_col_position'] = df_analysis['description_prompt_col_position'].astype(int)
    logging.info("Casted positional columns to integer type for plotting.")
    # --- End of casting ---

    logging.info("Starting analysis and plot generation...")

    # --- Perform Analyses and Plot ---

    # 1. Positional Bias in Assigned Scores
    logging.info("Analyzing positional bias in assigned scores...")

    # Average Score by Row Position
    avg_score_by_row = df_analysis.groupby('person_prompt_row_position')['llm_assigned_score'].mean().reset_index()
    plt.figure(figsize=(10, 6))
    sns.barplot(x='person_prompt_row_position', y='llm_assigned_score', data=avg_score_by_row)
    plt.title('Average LLM Assigned Score by Person\'s Row Position in Prompt (List A)')
    plt.xlabel('Person\'s Row Position (1-indexed)')
    plt.ylabel('Average Assigned Score')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig(os.path.join(output_plots_dir, 'avg_score_by_row_position.png'))
    plt.close()
    logging.info(f"Saved avg_score_by_row_position.png to {output_plots_dir}")

    # Average Score by Column Position
    avg_score_by_col = df_analysis.groupby('description_prompt_col_position')['llm_assigned_score'].mean().reset_index()
    plt.figure(figsize=(10, 6))
    sns.barplot(x='description_prompt_col_position', y='llm_assigned_score', data=avg_score_by_col)
    plt.title('Average LLM Assigned Score by Description\'s Column Position (ID X) in Prompt (List B)')
    plt.xlabel('Description\'s Column Position (ID X, 1-indexed)')
    plt.ylabel('Average Assigned Score')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig(os.path.join(output_plots_dir, 'avg_score_by_col_position.png'))
    plt.close()
    logging.info(f"Saved avg_score_by_col_position.png to {output_plots_dir}")

    # Heatmap of Average Scores
    avg_score_heatmap_data = df_analysis.pivot_table(
        values='llm_assigned_score',
        index='person_prompt_row_position',
        columns='description_prompt_col_position',
        aggfunc='mean'
    )
    plt.figure(figsize=(10, 8))
    sns.heatmap(avg_score_heatmap_data, annot=True, fmt=".3f", cmap="viridis", linewidths=.5)
    plt.title('Average LLM Assigned Score by Prompt Row vs. Column Position')
    plt.xlabel('Description Column Position (ID X)')
    plt.ylabel('Person Row Position')
    plt.savefig(os.path.join(output_plots_dir, 'avg_score_heatmap.png'))
    plt.close()
    logging.info(f"Saved avg_score_heatmap.png to {output_plots_dir}")

    # 2. Positional Bias in Top-1 Predictions (Explaining Anti-Correlation)
    logging.info("Analyzing positional bias in Top-1 predictions...")

    # Frequency of Top-1 Prediction by Column Position
    top1_freq_by_col = df_analysis[df_analysis['is_llm_top_1_prediction_for_row']].groupby('description_prompt_col_position').size().reset_index(name='count')
    # Normalize by total number of top-1 predictions (or total rows if each row has exactly one top-1)
    # If ties are possible, sum of counts > total trials * k.
    # A more robust normalization is by the number of times each column *could* have been a top-1.
    # For simplicity, let's normalize by total number of predictions.
    total_top1_predictions = df_analysis['is_llm_top_1_prediction_for_row'].sum()
    top1_freq_by_col['proportion'] = top1_freq_by_col['count'] / total_top1_predictions

    plt.figure(figsize=(10, 6))
    sns.barplot(x='description_prompt_col_position', y='proportion', data=top1_freq_by_col)
    plt.axhline(1/k_value, color='r', linestyle='--', label=f'Expected Random ({1/k_value:.2f})')
    plt.title('Proportion of Top-1 Predictions by Description\'s Column Position (ID X)')
    plt.xlabel('Description\'s Column Position (ID X, 1-indexed)')
    plt.ylabel('Proportion of Top-1 Predictions')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.legend()
    plt.savefig(os.path.join(output_plots_dir, 'top1_prediction_frequency_by_col.png'))
    plt.close()
    logging.info(f"Saved top1_prediction_frequency_by_col.png to {output_plots_dir}")

    # 3. Score Distribution of True vs. False Matches
    logging.info("Analyzing score distribution of true vs. false matches...")

    plt.figure(figsize=(10, 6))
    sns.kdeplot(df_analysis[df_analysis['is_true_match']]['llm_assigned_score'], label='True Matches', fill=True)
    sns.kdeplot(df_analysis[~df_analysis['is_true_match']]['llm_assigned_score'], label='False Matches', fill=True)
    plt.title('Distribution of LLM Assigned Scores for True vs. False Matches')
    plt.xlabel('LLM Assigned Score')
    plt.ylabel('Density')
    plt.legend()
    plt.savefig(os.path.join(output_plots_dir, 'score_distribution_true_vs_false.png'))
    plt.close()
    logging.info(f"Saved score_distribution_true_vs_false.png to {output_plots_dir}")

    # Print summary statistics for true vs false matches
    true_match_scores = df_analysis[df_analysis['is_true_match']]['llm_assigned_score']
    false_match_scores = df_analysis[~df_analysis['is_true_match']]['llm_assigned_score']

    logging.info("\nSummary Statistics for LLM Assigned Scores:")
    logging.info(f"  True Matches (N={len(true_match_scores)}): Mean={true_match_scores.mean():.4f}, Std={true_match_scores.std():.4f}")
    logging.info(f"  False Matches (N={len(false_match_scores)}): Mean={false_match_scores.mean():.4f}, Std={false_match_scores.std():.4f}")

    if true_match_scores.mean() < false_match_scores.mean():
        logging.warning("  WARNING: Average score for TRUE matches is LOWER than for FALSE matches. This indicates anti-correlation.")

    # 4. Performance Trend Over Trials (Linear Regression)
    logging.info("Analyzing performance trend over trials...")
    
    # Calculate Mean Reciprocal Rank for each trial
    mrr_per_trial = df_analysis.groupby('trial_id').apply(
        lambda trial_df: 1.0 / trial_df.loc[trial_df['is_true_match'], 'llm_assigned_score'].rank(ascending=False).iloc[0] if trial_df['is_true_match'].any() else 0
    ).sort_index()
    
    # Ensure we have a list of MRRs in the order of trials
    performance_over_time = mrr_per_trial.values
    
    # Perform linear regression
    positional_bias_metrics = calculate_positional_bias(performance_over_time)
    
    logging.info("\nSummary of Performance Trend (Positional Bias over time):")
    logging.info(f"  Linear Regression on MRR across {len(performance_over_time)} trials:")
    logging.info(f"    Slope: {positional_bias_metrics['bias_slope']:.6f}")
    logging.info(f"    Intercept: {positional_bias_metrics['bias_intercept']:.4f}")
    logging.info(f"    p-value: {positional_bias_metrics['bias_p_value']:.4g}")
    logging.info(f"    R-squared: {positional_bias_metrics['bias_r_value']**2:.4f}")

    logging.info("\nAnalysis complete. Check the 'bias_analysis_plots' directory for visualizations.")

# --- Command Line Interface ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Performs retrospective analysis to detect presentation order bias and other biases in LLM responses.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("run_output_dir",
                        help="The absolute or relative path to the self-contained output directory for a specific run (e.g., '\output\reports\6_Study_4\Gemini_2.0_Flash_map=correct').")

    args = parser.parse_args()

    analyze_positional_bias(args.run_output_dir)

# === End of src/analyze_positional_bias.py ===