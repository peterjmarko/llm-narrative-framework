#!/usr/bin/env python3
#-*- coding: utf-8 -*-
#
# Personality Matching Experiment Framework
# Copyright (C) 2025 Peter J. Marko
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
#
# Filename: src/analyze_study_results.py

"""
Final Study Analyzer.

This script performs the final statistical analysis for an entire study. It
takes the master `STUDY_results.csv` file (generated by
`compile_study_results.py`) as input and performs a Two-Way ANOVA and
post-hoc tests for each specified performance metric.

Its outputs are:
1.  A detailed `STUDY_analysis_log.txt` in the study's `anova/` directory.
2.  A set of publication-quality boxplots and diagnostic plots.

It is the final step in the `process_study.ps1` workflow.

Key Statistical Features:
-   **Data Sanitization**: Applies canonical and display names from `config.ini`.
-   **Reliability Filtering**: Excludes models that fail to meet a valid response threshold.
-   **Factorial ANOVA**: Conducts ANOVA with effect sizes (eta-squared).
-   **Assumption Checking**: Generates Q-Q plots of residuals.
-   **Intelligent Post-Hoc Testing**: Uses Tukey HSD with a fallback to Games-Howell.
-   **Advanced Performance Grouping**: Uses a clique-finding algorithm to identify performance tiers.

Usage:
    python src/study_analyzer.py /path/to/study_directory
"""

import argparse
import importlib
import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import os
import sys
import shutil
import logging
import warnings
import numpy as np
from textwrap import dedent

try:
    # Assumes this script is run from the project root or src is in the path
    from config_loader import APP_CONFIG, get_config_list, get_config_section_as_dict
except ImportError:
    # Fallback for running the script directly from src/
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    if current_script_dir not in sys.path:
        sys.path.insert(0, current_script_dir)
    from config_loader import APP_CONFIG, get_config_list, get_config_section_as_dict

try:
    # Use a non-interactive backend for saving plots to file
    import matplotlib
    matplotlib.use('Agg')
    import seaborn as sns
    import matplotlib.pyplot as plt
    import networkx as nx
    import pingouin as pg
    from pingouin import multicomp
except ImportError:
    logging.error("ERROR: Plotting libraries not found. Run: pip install seaborn matplotlib networkx pingouin")
    sys.exit(1)

import re

# Suppress the FutureWarning from seaborn/pandas
warnings.simplefilter(action='ignore', category=FutureWarning)


class ColorStrippingFormatter(logging.Formatter):
    """A logging formatter that strips ANSI color codes for file output."""
    ANSI_ESCAPE_CODE_RE = re.compile(r'\x1b\[[0-9;]*m')

    def format(self, record):
        formatted_message = super().format(record)
        return self.ANSI_ESCAPE_CODE_RE.sub('', formatted_message)

class ColorFormatter(logging.Formatter):
    """A logging formatter that adds ANSI color codes for console output."""
    GREY = "\x1b[38;20m"
    GREEN = "\x1b[32;20m"
    YELLOW = "\x1b[33;20m"
    RED = "\x1b[31;20m"
    BOLD_RED = "\x1b[31;1m"
    RESET = "\x1b[0m"

    def format(self, record):
        log_message = super().format(record)
        
        # Specific keyword coloring
        if "Conclusion: Significant effect found" in log_message:
            return f"{self.GREEN}{log_message}{self.RESET}"

        # Level-based coloring
        if record.levelno == logging.WARNING:
            return f"{self.YELLOW}{log_message}{self.RESET}"
        if record.levelno == logging.ERROR:
            return f"{self.RED}{log_message}{self.RESET}"
            
        return log_message

def find_master_csv(search_dir):
    """Finds the most relevant summary CSV in a directory."""
    possible_files = [
        'STUDY_results.csv',          # New standard study-level file
        'final_summary_results.csv',  # Old study-level for backward compatibility
        'EXPERIMENT_results.csv'      # Experiment-level if no study file found
    ]
    for filename in possible_files:
        search_path = os.path.join(search_dir, filename)
        if os.path.exists(search_path):
            logging.info(f"Found summary file: {filename}")
            return search_path

    logging.error(f"ERROR: No summary CSV file found in {search_dir}.")
    logging.error(f"Looked for: {', '.join(possible_files)}")
    return None

def format_p_value(p_value):
    """Formats a p-value for display on plots."""
    if pd.isna(p_value): return "p = N/A"
    return "p < 0.001" if p_value < 0.001 else f"p = {p_value:.3f}"

def interpret_bf(bf10):
    """Provides a qualitative interpretation of a Bayes Factor (BF10)."""
    bf10 = float(bf10)
    if bf10 >= 100: return "Extreme evidence for H1"
    if bf10 >= 30: return "Very Strong evidence for H1"
    if bf10 >= 10: return "Strong evidence for H1"
    if bf10 >= 3: return "Moderate evidence for H1"
    if bf10 > 1: return "Anecdotal evidence for H1"
    if bf10 == 1: return "No evidence"
    if bf10 > 1/3: return "Anecdotal evidence for H0"
    if bf10 > 1/10: return "Moderate evidence for H0"
    if bf10 > 1/30: return "Strong evidence for H0"
    if bf10 > 1/100: return "Very Strong evidence for H0"
    return "Extreme evidence for H0"

def generate_performance_tiers(df, metric, posthoc_df, sanitized_to_display_map):
    """
    Generates performance groups from a post-hoc results DataFrame using a
    clique-finding algorithm. This avoids the issue of chaining non-significant
    results across disparate groups. A "clique" is a group where every model is
    not statistically different from every other model within that same group.
    """
    logging.info("\n--- Performance Groups (Models in the same group are not statistically different from each other) ---")
    G = nx.Graph()
    # Nodes are the friendly display names
    models_display = list(sanitized_to_display_map.values())
    G.add_nodes_from(models_display)
    
    for _, row in posthoc_df.iterrows():
        # Get the display names from the sanitized names in the post-hoc results
        group1_display = sanitized_to_display_map.get(row['group1'], row['group1'])
        group2_display = sanitized_to_display_map.get(row['group2'], row['group2'])

        # Only add edges for models that are actually in our current analysis set
        if group1_display in models_display and group2_display in models_display and not row['reject']:
            G.add_edge(group1_display, group2_display)
            
    # Find all maximal cliques. A clique is a fully connected subgraph.
    # This is a much better representation of performance tiers.
    cliques = list(nx.find_cliques(G))
    
    # Sort cliques by their median performance
    clique_medians = sorted(
        [(df[df['model_display'].isin(clique)][metric].median(), clique) for clique in cliques if clique], 
        key=lambda x: x[0], 
        reverse=True
    )
    
    if not clique_medians:
        logging.info("No distinct performance groups found (all models are significantly different from each other).")
        return

    group_data = [[f"Group {i+1}", f"{median_val:.4f}", ", ".join(sorted(list(clique_models)))] for i, (median_val, clique_models) in enumerate(clique_medians)]
    group_df = pd.DataFrame(group_data, columns=["Performance Group", "Median Score", "Models"])
    logging.info(f"\n{group_df.to_string(index=False)}")

def create_diagnostic_plot(model, display_metric_name, output_dir, metric_key):
    """Generates and saves a Q-Q plot into the 'diagnostics' subdirectory."""
    if hasattr(model, 'resid') and not model.resid.empty:
        fig = plt.figure(figsize=(8, 6))
        ax = fig.add_subplot(111)
        sm.qqplot(model.resid, line='s', ax=ax)
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.set_title(f"Q-Q Plot of Residuals for '{display_metric_name}'")

        diagnostic_subdir = os.path.join(output_dir, 'diagnostics')
        plot_filename = f"qqplot_{metric_key}.png"
        full_plot_path = os.path.join(diagnostic_subdir, plot_filename)
        
        plt.savefig(full_plot_path)
        logging.info(f"-> Diagnostic plot saved successfully to: {full_plot_path}")
        plt.close(fig)
    else:
        logging.warning(f"-> Could not generate Q-Q plot for '{display_metric_name}': No residuals found.")


def create_and_save_plot(df, metric_key, display_metric_name, factor, p_value, output_dir, factor_display_map, project_root):
    """Creates and saves a boxplot, and copies it to the docs/images/boxplots directory."""
    # Suppress the specific PendingDeprecationWarning from seaborn's internal call to matplotlib
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=PendingDeprecationWarning)
        fig = plt.figure(figsize=(12, 8))
        ax = plt.gca()
        
        # Use the friendly display names for the y-axis if plotting by model
        plot_factor = 'model_display' if factor == 'model' else factor
        
        order = df.groupby(plot_factor)[metric_key].median().sort_values(ascending=False).index
        sns.boxplot(ax=ax, y=plot_factor, x=metric_key, data=df, order=order, orientation='horizontal', palette="coolwarm", legend=False)

        p_value_str = format_p_value(p_value)
        display_factor_name = factor_display_map.get(factor, factor.capitalize())
        title = f'Performance Comparison for: {display_metric_name}\n(Grouped by {display_factor_name}, ANOVA {p_value_str})'
        
        ax.set_title(title, fontsize=16)
        ax.set_xlabel(f'Metric: {display_metric_name}', fontsize=12)
        ax.set_ylabel(display_factor_name, fontsize=12)
        ax.grid(axis='x', linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        
        # Standardize plot filename format: boxplot_{factor}_{metric_key}.png
        plot_filename = f"boxplot_{factor}_{metric_key}.png"
        
        # 1. Save to the anova results directory
        boxplot_subdir = os.path.join(output_dir, 'boxplots', factor)
        full_plot_path = os.path.join(boxplot_subdir, plot_filename)
        plt.savefig(full_plot_path)
        logging.info(f"-> Plot saved successfully to: {full_plot_path}")

        # 2. Copy the plot to the docs/images/boxplots directory for easy access
        docs_images_dir = os.path.join(project_root, 'docs', 'images', 'boxplots')
        os.makedirs(docs_images_dir, exist_ok=True)
        dest_plot_path = os.path.join(docs_images_dir, plot_filename)
        shutil.copy2(full_plot_path, dest_plot_path)
        logging.info(f"-> Copied plot to: {dest_plot_path}")

        plt.close(fig)

def create_and_save_interaction_plot(df, metric_key, display_metric_name, factors, p_value, output_dir, factor_display_map, project_root):
    """Creates and saves a line plot to visualize an interaction effect."""
    fig = plt.figure(figsize=(12, 8))
    ax = plt.gca()
    
    hue_factor, x_factor = factors[0], factors[1]
    
    sns.pointplot(data=df, x=x_factor, y=metric_key, hue=hue_factor, ax=ax, errorbar='se', capsize=0.1)
    
    p_value_str = format_p_value(p_value)
    hue_display = factor_display_map.get(hue_factor, hue_factor.capitalize())
    x_display = factor_display_map.get(x_factor, x_factor.capitalize())
    
    title = f'Interaction Effect on: {display_metric_name}\n({hue_display} * {x_display}, ANOVA {p_value_str})'
    
    ax.set_title(title, fontsize=16)
    ax.set_xlabel(f'Factor: {x_display}', fontsize=12)
    ax.set_ylabel(f'Mean {display_metric_name}', fontsize=12)
    ax.grid(axis='y', linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    
    plot_filename = f"interaction_plot_{factors[0]}_x_{factors[1]}_{metric_key}.png"
    
    # 1. Save to the anova results directory
    boxplot_subdir = os.path.join(output_dir, 'boxplots') # Save in the main boxplots dir
    full_plot_path = os.path.join(boxplot_subdir, plot_filename)
    plt.savefig(full_plot_path)
    logging.info(f"-> Interaction plot saved successfully to: {full_plot_path}")
    
    # 2. Copy the plot to the docs/images/boxplots directory
    docs_images_dir = os.path.join(project_root, 'docs', 'images', 'boxplots')
    os.makedirs(docs_images_dir, exist_ok=True)
    dest_plot_path = os.path.join(docs_images_dir, plot_filename)
    shutil.copy2(full_plot_path, dest_plot_path)
    logging.info(f"-> Copied plot to: {dest_plot_path}")

    plt.close(fig)

def perform_analysis(df, metric_key, all_possible_factors, output_dir, sanitized_to_display_map, metric_display_map, factor_display_map, project_root):
    """Performs a full statistical analysis for a single metric."""
    display_metric_name = metric_display_map.get(metric_key, metric_key)
    
    logging.info("\n" + "="*80)
    logging.info(f" ANALYSIS FOR METRIC: '{display_metric_name}'")
    logging.info("="*80)

    if df[metric_key].var() == 0:
        logging.warning(f"WARNING: Metric '{display_metric_name}' has zero variance. Skipping all analysis.")
        return

    active_factors = [f for f in all_possible_factors if df[f].nunique() > 1]
    if not active_factors:
        logging.info("INFO: Only one experimental group found. No formal analysis is possible.")
        # Even with one group, generate a plot if there's data to visualize.
        # Use the first factor listed in the config as the grouping variable for the plot.
        if not df.empty and all_possible_factors:
            grouping_factor = all_possible_factors[0]
            logging.info(f"--- Generating Performance Plot (grouped by '{grouping_factor}') ---")
            create_and_save_plot(df, metric_key, display_metric_name, grouping_factor, float('nan'), output_dir, factor_display_map, project_root)
        return
    
    logging.info(f"Detected {len(active_factors)} active factor(s) with variation: {', '.join(active_factors)}")

    logging.info(f"\n--- Descriptive Statistics by {', '.join(active_factors)} ---")
    if 'n_valid_responses' in df.columns:
        desc_stats = df.groupby(active_factors).agg(
            Replications=('model', 'size'),
            N=('n_valid_responses', 'sum'),
            Mean=(metric_key, 'mean'),
            StdDev=(metric_key, 'std')
        ).rename(columns={'N': 'Total Trials (N)', 'StdDev': 'Std. Dev.'})
    else:
        desc_stats = df.groupby(active_factors)[metric_key].agg(['count', 'mean', 'std']).rename(columns={'count': 'Replications (N)'})
    logging.info(f"\n{desc_stats.to_string(float_format='%.4f')}")
    
    # If there are multiple factors, include their interaction effects.
    # A * B in formula syntax expands to A + B + A:B (main effects + interaction)
    formula_joiner = ' * ' if len(active_factors) > 1 else ' + '
    formula = f"Q('{metric_key}') ~ {formula_joiner.join([f'C({f})' for f in active_factors])}"

    try:
        model = ols(formula, data=df).fit()
        create_diagnostic_plot(model, display_metric_name, output_dir, metric_key)
        anova_table = sm.stats.anova_lm(model, typ=2)

        # Add Eta-squared (η²) for effect size
        ss_total = anova_table['sum_sq'].sum()
        anova_table['eta_sq'] = anova_table['sum_sq'] / ss_total
        
        # Correct for multiple comparisons using Benjamini-Hochberg FDR
        p_values = anova_table['PR(>F)'].dropna()
        if not p_values.empty:
            rejected, p_corrected = multicomp(p_values.tolist(), method='fdr_bh')
            p_corr_series = pd.Series(p_corrected, index=p_values.index)
            anova_table['p-corr'] = p_corr_series

        logging.info(f"\n--- ANOVA Summary for {display_metric_name} ---")
        logging.info("NOTE: 'p-corr' column shows p-values corrected for multiple comparisons using the Benjamini-Hochberg FDR method.")
        logging.info(f"\n{anova_table.to_string(float_format='%.6f')}")

        # --- Bayesian Analysis for 'mapping_strategy' ---
        if 'mapping_strategy' in active_factors and df['mapping_strategy'].nunique() == 2:
            try:
                logging.info("\n--- Bayesian Analysis (for 'mapping_strategy' factor) ---")
                levels = df['mapping_strategy'].unique()
                group1 = df[df['mapping_strategy'] == levels[0]][metric_key].dropna()
                group2 = df[df['mapping_strategy'] == levels[1]][metric_key].dropna()

                if group1.empty or group2.empty:
                    raise ValueError("One or both groups are empty after dropping NaNs.")
                if np.var(group1) == 0 or np.var(group2) == 0:
                    raise ValueError("One or both groups have zero variance.")

                # The high-level pingouin.ttest function performs a standard T-test and
                # automatically includes the Bayes Factor (BF10) in its output.
                # This is the correct function for performing a Bayesian t-test on raw data.
                bf_result = pg.ttest(group1, group2, paired=False)

                if not isinstance(bf_result, pd.DataFrame) or bf_result.empty or 'BF10' not in bf_result.columns:
                    raise ValueError("T-test calculation did not return a valid DataFrame with a Bayes Factor.")

                # Extract the BF10 value and immediately cast it to a float.
                bf10 = float(bf_result['BF10'].iloc[0])

                logging.info(f"Comparing '{levels[0]}' vs '{levels[1]}'")
                logging.info("The Bayes Factor (BF₁₀) quantifies how many times more likely the data are")
                logging.info("under the alternative hypothesis (H1: a difference exists) than the null (H0).")
                logging.info(f" -> Bayes Factor (BF₁₀): {bf10:.3f}")
                logging.info(f" -> Interpretation: {interpret_bf(bf10)}")
            except Exception as e:
                logging.warning(f"\nWARNING: Could not perform Bayesian analysis for 'mapping_strategy'. Reason: {e}")
        # --- End of Bayesian Analysis ---
        
        significant_factors = [f.replace('C(', '').replace(')', '') for f in anova_table.index if anova_table.loc[f, 'PR(>F)'] < 0.05 and 'Residual' not in f]

        if significant_factors:
            logging.info(f"\nConclusion: Significant effect found for factor(s): {', '.join(significant_factors)}")
            logging.info("\n--- Post-Hoc Analysis ---")
            for factor in significant_factors:
                if df[factor].nunique() <= 2:
                    p_val_str = f"{anova_table.loc[f'C({factor})', 'PR(>F)']:.4f}"
                    logging.info(f"\nFactor '{factor}' has only two levels and is significant (ANOVA p={p_val_str}). No pairwise table needed.")
                    continue

                logging.info(f"\nComparing levels for factor: '{factor}'")
                posthoc_df = None
                
                try:
                    # Attempt Tukey HSD first, which assumes equal variance
                    logging.info("\nAttempting Tukey HSD...")
                    tukey_result = pairwise_tukeyhsd(endog=df[metric_key], groups=df[factor], alpha=0.05)
                    if pd.DataFrame(tukey_result._results_table.data).isnull().values.any():
                        raise ValueError("Tukey HSD result contains NaN values.")
                    
                    logging.info(f"\n{tukey_result}")
                    posthoc_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])

                except (ValueError, ZeroDivisionError) as tukey_err:
                    logging.warning(f"  - Tukey HSD failed: {tukey_err}")
                    logging.info("  - Falling back to Games-Howell test (does not assume equal variance).")
                    
                    gh_result = pg.pairwise_gameshowell(data=df, dv=metric_key, between=factor)
                    logging.info(f"\n{gh_result.to_string()}")
                    
                    # Standardize Games-Howell output to match the format needed for tier generation
                    posthoc_df = gh_result[['A', 'B', 'pval']].copy()
                    posthoc_df.rename(columns={'A': 'group1', 'B': 'group2'}, inplace=True)
                    posthoc_df['reject'] = posthoc_df['pval'] < 0.05

                if factor == 'model' and posthoc_df is not None:
                    generate_performance_tiers(df, metric_key, posthoc_df, sanitized_to_display_map)

        else:
            logging.info("\nConclusion: No factors had a statistically significant effect on this metric.")
            
        logging.info("\n--- Generating Performance Plots ---")
        logging.info("\n--- Generating Performance Plots ---")
        # Plot main effects
        for factor in active_factors:
            key = f"C({factor})"
            p_val = anova_table.loc[key, 'PR(>F)'] if key in anova_table.index else float('nan')
            create_and_save_plot(df, metric_key, display_metric_name, factor, p_val, output_dir, factor_display_map, project_root)
            
        # Plot significant interaction effects (for 2-way ANOVA)
        if len(active_factors) == 2:
            interaction_term = f"C({active_factors[0]}):C({active_factors[1]})"
            if interaction_term in anova_table.index:
                p_val = anova_table.loc[interaction_term, 'PR(>F)']
                if p_val < 0.05:
                    logging.info(f"\n--- Generating Interaction Plot for '{interaction_term}' ---")
                    # Assuming the first factor is the hue and the second is the x-axis
                    create_and_save_interaction_plot(df, metric_key, display_metric_name, active_factors, p_val, output_dir, factor_display_map, project_root)

    except Exception as e:
        logging.error(f"\nERROR: Could not perform analysis for metric '{display_metric_name}'. Reason: {e}")

def main():
    """Main entry point for the ANOVA analysis script."""
    global APP_CONFIG
    parser = argparse.ArgumentParser(
        description="Perform ANOVA on experiment results.",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog=dedent("""
        Example Usage:
        --------------
        # To analyze the results of a full study
        python src/study_analysis.py path/to/your/study_folder/
        
        # To analyze the results of a single experiment
        python src/study_analysis.py path/to/your/experiment_folder/
        
        The script automatically finds the most relevant summary file in the
        specified directory and saves all output to an 'anova/' subfolder.
        """)
    )
    parser.add_argument("study_directory", help="Path to the top-level study directory containing the master CSV.")
    parser.add_argument('--config-path', type=str, default=None, help=argparse.SUPPRESS) # For testing
    args = parser.parse_args()

    if args.config_path:
        os.environ['PROJECT_CONFIG_OVERRIDE'] = os.path.abspath(args.config_path)
        if 'config_loader' in sys.modules:
            importlib.reload(sys.modules['config_loader'])
        # Re-import and re-assign the global APP_CONFIG
        from config_loader import APP_CONFIG as RELOADED_APP_CONFIG
        APP_CONFIG = RELOADED_APP_CONFIG

    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    base_dir = os.path.abspath(args.study_directory)
    output_dir = os.path.join(base_dir, 'anova')
    os.makedirs(output_dir, exist_ok=True)

    # Set up logging configuration early to capture all messages
    log_filename = 'STUDY_analysis_log.txt'
    log_filepath = os.path.join(output_dir, log_filename)
    
    # Configure logging with both file and console handlers from the start
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # Clear any existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # --- Single-level Backup of Previous Run ---
    try:
        import shutil
        archive_dir = os.path.join(output_dir, 'archive')
        # List items in the anova/ directory, excluding the 'archive' subdir and the new log file
        items_to_archive = [item for item in os.listdir(output_dir) if item not in ['archive', log_filename]]
        
        if items_to_archive:
            logging.info(f"Archiving {len(items_to_archive)} file(s) from previous analysis run...")
            os.makedirs(archive_dir, exist_ok=True)
            for item_name in items_to_archive:
                source_path = os.path.join(output_dir, item_name)
                destination_path = os.path.join(archive_dir, item_name)
                
                # Overwrite existing item in archive to ensure a clean, single-level backup
                if os.path.exists(destination_path):
                    if os.path.isdir(destination_path):
                        shutil.rmtree(destination_path)
                    else:
                        os.remove(destination_path)
                
                shutil.move(source_path, destination_path)
            logging.info(f"Previous results moved to: {archive_dir}\n")
    except Exception as e:
        logging.warning(f"Could not archive previous results. Reason: {e}")
    # --- End of Backup ---

    # --- Create Output Subdirectories ---
    factors_from_config = get_config_list(APP_CONFIG, 'Schema', 'factors')
    if factors_from_config:
        boxplot_base_dir = os.path.join(output_dir, 'boxplots')
        os.makedirs(boxplot_base_dir, exist_ok=True)
        for factor in factors_from_config:
            os.makedirs(os.path.join(boxplot_base_dir, factor), exist_ok=True)
    
    os.makedirs(os.path.join(output_dir, 'diagnostics'), exist_ok=True)
    # --- End of Directory Creation ---

    # Set up the final logging configuration with two distinct handlers.
    # 1. Add a handler for the log file that uses the custom formatter to strip color codes.
    file_handler = logging.FileHandler(log_filepath, mode='w', encoding='utf-8')
    file_handler.setFormatter(ColorStrippingFormatter('%(message)s'))
    root_logger.addHandler(file_handler)

    # 2. Add a handler for the console that uses the custom ColorFormatter.
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(ColorFormatter('%(message)s'))
    root_logger.addHandler(console_handler)

    # --- Write the comprehensive header to the log file ---
    logging.info("\n================================================================================")
    logging.info("       LLM Personality Matching Study: Comprehensive Statistical Analysis")
    logging.info("================================================================================")
    logging.info("\nIntroduction")
    logging.info("------------")
    logging.info("This document details the comprehensive statistical analysis for the entire study, automatically")
    logging.info("generated by the analyze_study_results.py script. The analysis is performed on the master `STUDY_results.csv`")
    logging.info("dataset, which aggregates results from all experimental replications.")
    logging.info("\nThe primary goal is to determine whether the experimental factors—specifically the LLM `model` and")
    logging.info("the `mapping_strategy`—have a statistically significant effect on key performance metrics.")
    logging.info("\nMethodology:")
    logging.info("1.  **Data Filtering:** To ensure the reliability of the analysis, models with an average")
    logging.info("    number of valid responses below the configured threshold (e.g., <25) are excluded.")
    logging.info("2.  **Primary Analysis:** A Two-Way ANOVA is conducted for each performance metric. The ANOVA")
    logging.info("    table shows the main effects of each factor, with a p-value (PR(>F)) indicating")
    logging.info("    statistical significance. Effect size (eta_sq) is also reported.")
    logging.info("3.  **Post-Hoc Testing:** When a factor has a significant effect (p < 0.05), a post-hoc")
    logging.info("    test (Tukey HSD or Games-Howell) is performed to identify which specific groups")
    logging.info("    (e.g., which models) are statistically different from one another.")
    logging.info("4.  **Multiple Comparisons Correction:** A Benjamini-Hochberg (FDR) correction is applied to")
    logging.info("    the ANOVA p-values to control the false discovery rate across all factors tested.")
    logging.info("5.  **Performance Grouping:** For the 'model' factor, post-hoc results are summarized into")
    logging.info("    distinct performance tiers, where models in the same group are not statistically")
    logging.info("    different from each other.")
    logging.info("\nThe report is structured by metric. Each section includes descriptive statistics, the ANOVA")
    logging.info("summary, post-hoc results (if applicable), and references to the generated boxplots which")
    logging.info("visualize these findings.")
    logging.info("\n" + "="*80) # Separator after intro
    logging.info(f"Full analysis log is being saved to: {log_filepath}") # Re-add this existing log line
    logging.info("="*80 + "\n") # Another separator

    try:
        master_csv_path = find_master_csv(base_dir)
        if not master_csv_path:
            # sys.exit(1) would skip the finally block, so we use return.
            return
        
        df = pd.read_csv(master_csv_path)
        logging.info(f"Successfully loaded {len(df)} rows from {master_csv_path}\n") # Add newline for spacing
    except Exception as e:
        logging.error(f"FATAL: Could not load master CSV file. Error: {e}")
        return

    # --- Data Cleaning and Preparation ---
    if 'db' in df.columns:
        original_db_values = df['db'].unique()
        df['db'] = df['db'].replace('personalities_db_1-5000.jsonl', 'personalities_db_1-5000.txt')
        cleaned_db_values = df['db'].unique()
        # Log if the set of unique values has changed, regardless of length reduction
        # Use np.array_equal for robust comparison of numpy arrays
        if not np.array_equal(original_db_values, cleaned_db_values):
            logging.info("Performed data cleaning on 'db' column to unify values.")
            logging.info(f" -> Original unique values: {original_db_values}")
            logging.info(f" -> Cleaned unique values:  {cleaned_db_values}\n")

    # --- Pre-analysis Filtering based on Valid Response Rate ---
    min_valid_responses = APP_CONFIG.getint('Analysis', 'min_valid_response_threshold', fallback=0)
    if 'n_valid_responses' in df.columns and min_valid_responses > 0:
        logging.info(f"Applying filter: Models with an average valid response rate < {min_valid_responses} will be excluded.")
        model_avg_responses = df.groupby('model')['n_valid_responses'].mean()
        models_to_exclude = model_avg_responses[model_avg_responses < min_valid_responses].index.tolist()
        
        if models_to_exclude:
            logging.info(f"Excluding {len(models_to_exclude)} model(s) due to low valid response rates: {', '.join(models_to_exclude)}")
            df = df[~df['model'].isin(models_to_exclude)]
            if not df.empty:
                logging.info(f"Analysis will proceed with {len(df)} rows from {df['model'].nunique()} models.\n")
        else:
            logging.info("All models meet the minimum valid response rate.\n")
    
    # Gracefully exit if the filtering step removed all data.
    if df.empty:
        logging.warning("WARNING: All models were filtered out. No further analysis is possible.")
        # We must return here to avoid processing an empty dataframe.
        return

    try:
        normalization_map = get_config_section_as_dict(APP_CONFIG, 'ModelNormalization')
        display_name_map = get_config_section_as_dict(APP_CONFIG, 'ModelDisplayNames')
        metric_display_map = get_config_section_as_dict(APP_CONFIG, 'MetricDisplayNames')
        factor_display_map = get_config_section_as_dict(APP_CONFIG, 'FactorDisplayNames')
        factors = get_config_list(APP_CONFIG, 'Schema', 'factors')
        metrics = get_config_list(APP_CONFIG, 'Schema', 'metrics')

        if not all([normalization_map, display_name_map, factors, metrics, metric_display_map, factor_display_map]):
            logging.error("FATAL: Could not load required sections ('ModelNormalization', 'ModelDisplayNames', 'MetricDisplayNames', 'FactorDisplayNames', 'Schema') from config.ini.")
            return

        keyword_to_canonical_map = {kw.strip(): can for can, kws in normalization_map.items() for kw in kws.split(',')}

        if 'model' in df.columns:
            logging.info("Normalizing model names for consistency based on config...")
            df['model_canonical'] = df['model'].apply(lambda name: next((can for kw, can in keyword_to_canonical_map.items() if kw in str(name)), str(name)))
            df['model_display'] = df['model_canonical'].map(display_name_map).fillna(df['model_canonical'])
            df['model'] = df['model_canonical'].str.replace(r'[/.-]', '_', regex=True) # Sanitize for formula
            logging.info("Model name normalization and sanitization complete.\n")

            sanitized_to_display = df[['model', 'model_display']].drop_duplicates().set_index('model')['model_display'].to_dict()

        for factor in factors:
            if factor in df.columns:
                df[factor] = df[factor].astype(str)

        # Perform analysis for each metric
        for metric_key in metrics:
            if metric_key in df.columns:
                perform_analysis(df, metric_key, factors, output_dir, sanitized_to_display, metric_display_map, factor_display_map, project_root)
            else:
                logging.warning(f"\nWarning: Metric column '{metric_key}' not found. Skipping analysis.")
    finally:
        # This block ensures that logging is always shut down, releasing file
        # handles even if the function returns early or an error occurs.
        logging.shutdown()

if __name__ == "__main__":
    main()

# === End of src/analyze_study_results.py ===
