# Assembly Logic Validation Workflow

This directory contains a five-step, ordered workflow to create and validate the "ground truth" dataset used for testing the core personality profile assembly algorithm (`generate_personalities_db.py`).

The goal of this workflow is to produce a `personalities_db.assembly_logic.txt` file where the descriptions are generated by the source expert system (Solar Fire) rather than our Python script. This file serves as the ground truth against which our script's output is compared in a bit-for-bit `pytest` verification (`test_assembly_logic.py`). A successful match provides definitive proof that our implementation of the personality assembly algorithm is correct.

## The Workflow

The scripts must be run in the numbered order to ensure data integrity. The entire process is conducted within a temporary sandbox directory (`temp_assembly_logic_validation/`) to avoid interfering with the main project data.

### Step 1: Generate Coverage Map (`1_generate_coverage_map.py`)

-   **Purpose**: To pre-compute which delineation keys (e.g., "Sun in Aries", "Element Strong Fire") are triggered by each of the ~5,000 subjects in the main database.
-   **Input**: `data/processed/subject_db.csv`
-   **Output**: `data/reports/delineation_coverage_map.csv`

### Step 2: Select Subjects (`2_select_assembly_logic_subjects.py`)

-   **Purpose**: To select the smallest possible set of subjects that provides maximum coverage of all possible delineation keys. It uses a greedy algorithm, iteratively picking the subject who adds the most new, uncovered keys.
-   **Input**: `data/reports/delineation_coverage_map.csv`
-   **Output**:
    -   `temp_assembly_logic_validation/data/processed/subject_db.assembly_logic.csv`: The final, minimal set of subjects.
    -   `temp_assembly_logic_validation/data/reports/assembly_logic_coverage_report.txt`: A report detailing the final coverage percentage.

### Step 3: Prepare Solar Fire Import (`3_prepare_assembly_logic_import.py`)

-   **Purpose**: To format the selected subject list into the specific `.CQD` file format required for manual import into the Solar Fire software. This script also encodes each subject's unique ID into the `ZoneAbbr` field for a lossless data round-trip.
-   **Input**: `temp_assembly_logic_validation/data/processed/subject_db.assembly_logic.csv`
-   **Output**: `temp_assembly_logic_validation/data/intermediate/sf_data_import.assembly_logic.txt`

> **Manual Step**: At this point, the developer must import the generated file into Solar Fire, run the "Interpretation Reports" for all subjects, and save the raw text files into the `temp_assembly_logic_validation/data/intermediate/assembly_logic_raw_reports/` directory.

### Step 4: Extract Ground Truth Text (`4_extract_assembly_logic_text.py`)

-   **Purpose**: To parse the raw, multi-section text reports generated by Solar Fire and extract only the specific delineation components used by our algorithm. It assembles these snippets into a clean, final text file.
-   **Input**: `temp_assembly_logic_validation/data/intermediate/assembly_logic_raw_reports/*.txt`
-   **Output**: `temp_assembly_logic_validation/personalities_db.assembly_logic.txt` (This is the final ground truth file).

### Step 5: Validate Data Round-Trip (`5_validate_assembly_logic_subjects.py`)

-   **Purpose**: An optional but critical integrity check. This script takes the chart data exported from Solar Fire after the manual step, decodes the subject IDs, and reconstructs the `subject_db`. It then compares this file to the original from Step 2. A perfect match confirms that the manual process was lossless.
-   **Input**: `temp_assembly_logic_validation/data/foundational_assets/sf_chart_export.assembly_logic.csv`
-   **Output**: A success or failure message to the console.