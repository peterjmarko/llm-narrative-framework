[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"

[project]
name = "llm-narrative-framework"
version = "12.4.0"
description = "A reproducible pipeline for LLM personality matching experiments."
authors = [
    {name = "Peter J. Marko", email = "peter.j.marko@gmail.com"},
]
requires-python = ">=3.11"
readme = "README.md"
license = {text = "MIT"}

dependencies = [
    "numpy",
    "pandas",
    "scipy",
    "statsmodels",
    "networkx",
    "requests>=2.32.4",
    "python-dotenv",
    "tqdm>=4.67.1",
    "seaborn",
    "matplotlib",
    "pingouin>=0.5.5",
    "thefuzz[speedup]>=0.22.1",
    "beautifulsoup4>=4.13.4",
    "certifi>=2025.8.3",
]

[tool.pdm]
# This tells PDM to use a virtual environment located in the project's .venv folder.
venv.in-project = true

[tool.pytest.ini_options]
pythonpath = ["src"]

[tool.pdm.scripts]
# ==============================================================================
# === MAINTENANCE, LINTING & REPORTING ===
# ==============================================================================
clean = {cmd = "python scripts/maintenance/clean_project.py", help = "Clean up temporary and generated files."}
scope-report = "python scripts/maintenance/generate_scope_report.py"
list-files = "python scripts/maintenance/list_project_files.py"
sync-project = "python scripts/maintenance/sync_project_assets.py"
txt-copy = "python scripts/maintenance/convert_py_to_txt.py"
lint = {composite = ["check-headers", "lint-docstrings"], help="Run all read-only linting checks."}
lint-fix = {composite = ["lint-headers-fix", "lint-docstrings"], help="Run all linters and apply automatic fixes."}
check-headers = {cmd = "python scripts/lint/lint_file_headers.py", help = "Check headers and footers on all project scripts."}
lint-headers-fix = {cmd = "python scripts/lint/lint_file_headers.py --fix", help = "Fix headers and footers on all project scripts."}
lint-docstrings = {cmd = "python scripts/lint/lint_docstrings.py {args}", help = "Check all scripts for docstring compliance (--deep for full scan)."}

# ==============================================================================
# === BUILD & RELEASE WORKFLOW ===
# ==============================================================================
build-docs = "python scripts/build/build_docs.py"
commit = {cmd = "cz commit", help = "Create a new commit using the interactive Commitizen prompt."}
release = {cmd = "python scripts/build/finalize_release.py", help = "Finalize a new release (bump, changelog, tag)."}

# ==============================================================================
# === CORE PROJECT WORKFLOWS ===
# ==============================================================================
# Data preparation workflow shortcuts
prep-data = "pwsh -ExecutionPolicy Bypass -File ./prepare_data.ps1"
# Main workflow shortcuts
new-exp = "pwsh -ExecutionPolicy Bypass -File ./new_experiment.ps1"
aud-exp = "pwsh -ExecutionPolicy Bypass -File ./audit_experiment.ps1"
fix-exp = "pwsh -ExecutionPolicy Bypass -File ./fix_experiment.ps1"
com-stu = "pwsh -ExecutionPolicy Bypass -File ./compile_study.ps1"
aud-stu = "pwsh -ExecutionPolicy Bypass -File ./audit_study.ps1"

# ==============================================================================
# === TESTING WORKFLOW (Organized by Typical Testing Sequence) ===
# ==============================================================================
# For the complete testing workflow and sequence details, see:
# docs/TESTING_GUIDE.md - "Typical Testing Sequence" section

# --- Stage 1: Unit Tests - Data Preparation Pipeline ---
test-data-prep = {cmd = "pytest tests/data_preparation/", help = "Run all data preparation unit tests."}

# --- Stage 2: Data Pipeline Integration (Layer 2-3) ---
test-l2 = "pwsh -File ./tests/testing_harness/data_preparation/layer2/run_layer2_test.ps1"
test-l3 = "pwsh ./tests/testing_harness/data_preparation/layer3/run_layer3_test.ps1 -Profile default"
test-l3-bypass = "pwsh ./tests/testing_harness/data_preparation/layer3/run_layer3_test.ps1 -Profile bypass"
test-l3-interactive = "pwsh ./tests/testing_harness/data_preparation/layer3/run_layer3_test.ps1 -Profile default -Interactive"

# --- Stage 3: Algorithm Validation ---
# Prerequisite steps for test-assembly (Personality Assembly Algorithm Validation)
test-assembly-step1 = {cmd = "python scripts/workflows/assembly_logic/1_generate_coverage_map.py", help = "Step 1: Generate coverage map for assembly logic validation."}
test-assembly-step2 = {cmd = "python scripts/workflows/assembly_logic/2_select_assembly_logic_subjects.py", help = "Step 2: Select optimal subject set for assembly logic validation."}
test-assembly-step3 = {cmd = "python scripts/workflows/assembly_logic/3_prepare_assembly_logic_import.py", help = "Step 3: Prepare import file for Solar Fire."}
test-assembly-step4 = {cmd = "python scripts/workflows/assembly_logic/4_extract_assembly_logic_text.py", help = "Step 5: Extract and assemble the ground truth database."}
test-assembly-step5 = {cmd = "python scripts/workflows/assembly_logic/5_validate_assembly_logic_subjects.py", help = "Step 6: Validate the assembled ground truth database."}
test-assembly-setup = {cmd = "python scripts/workflows/assembly_logic/test_assembly_setup.py", help = "Run all steps with interactive pause for manual Solar Fire processing."}
test-assembly = {cmd = "pytest tests/algorithm_validation/test_profile_generation_algorithm.py", help = "Run the personality assembly algorithm validation."}
test-query-gen = "pwsh -File ./tests/testing_harness/validate_query_generation.ps1"
test-l3-selection = "pwsh ./tests/algorithm_validation/validate_selection_algorithms.ps1"

# --- Stage 4: Unit Tests - Experiment Lifecycle ---
test-exp-lc = {cmd = "pytest tests/experiment_lifecycle/", help = "Run all experiment lifecycle unit tests."}

# --- Stage 5: Experiment & Study Integration (Layer 4-5) ---
test-l4 = "pwsh -File ./tests/testing_harness/experiment_lifecycle/layer4/run_layer4_test.ps1"
test-l4-interactive = "pwsh -File ./tests/testing_harness/experiment_lifecycle/layer4/run_layer4_test.ps1 -Interactive"
test-l5 = "pwsh -File ./tests/testing_harness/experiment_lifecycle/layer5/run_layer5_test.ps1"

# --- Stage 6: All Unit Tests Together ---
test = {cmd = "python tests/run_all_tests.py {args}", help = "Run all tests (Python and PowerShell) with cleanup."}
cov = {shell = "pytest --cov=src --cov-report=term-missing", help = "Run all tests with coverage analysis."}
cov-html = {shell = "pytest --cov=src --cov-report=html", help = "Run all tests and generate an HTML coverage report."}
test-cov = {shell = "pytest --cov=src --cov-report= {args}", help = "Run tests for a specific file to update .coverage data."}
report-cov = {shell = "coverage report -m --no-skip-covered {args}", help = "Generate a focused coverage report for specific source file(s)."}

# --- Stage 7: Statistical Validation ---
test-stats-study = "pwsh -File ./tests/algorithm_validation/create_statistical_study.ps1"
test-stats-imports = "pwsh -File ./tests/algorithm_validation/generate_graphpad_imports.ps1" 
test-stats-results = "pwsh -File ./tests/algorithm_validation/validate_graphpad_results.ps1"

# --- PowerShell Wrapper Tests (Used by Integration Tests) ---
test-ps-new-exp = "pwsh ./tests/experiment_lifecycle/new_experiment.Tests.ps1"
test-ps-audit-exp = "pwsh ./tests/experiment_lifecycle/audit_experiment.Tests.ps1"
test-ps-fix-exp = "pwsh ./tests/experiment_lifecycle/fix_experiment.Tests.ps1"
test-ps-comp-study = "pwsh ./tests/experiment_lifecycle/compile_study.Tests.ps1"
test-ps-audit-study = "pwsh ./tests/experiment_lifecycle/audit_study.Tests.ps1"
test-ps-all = "pwsh ./tests/run_all_ps_tests.ps1"

# --- Composite Test Runners ---
test-ps-exp = {composite = ["test-ps-new-exp", "test-ps-audit-exp", "test-ps-fix-exp"], help = "Run all PowerShell tests for the experiment lifecycle."}
test-exp-all = {composite = ["test-exp-lc", "test-ps-exp"], help = "Run all Python and PowerShell tests for the experiment lifecycle."}

[tool.commitizen]
name = "cz_conventional_commits"
version = "12.4.0"
version_files = [
    "pyproject.toml:version"
]
tag_format = "v$version"
changelog_file = "CHANGELOG.md"
changelog_format = "keep_a_changelog"

[tool.coverage.run]
source = ["src"]
omit = ["*/__init__.py"]

[tool.coverage.report]
show_missing = true
skip_covered = false
exclude_lines = [
    "pragma: no cover",
    "if __name__ == .__main__.:",
]

[dependency-groups]
dev = [
    "pytest",
    "pre-commit",
    "pypandoc>=1.15",
    "pillow>=10.4.0",
    "pytest-cov>=5.0.0",
    "pytest-mock>=3.14.1",
    "commitizen>=3.31.0",
    "python-docx>=1.2.0",
]

[tool.pylance.exclude]
# Add other folders as needed, e.g., "**/node_modules"
exclude = [
    "**/__pycache__",
    "**/.venv",
    "**/output",
    "**/data/backup",
    "**/temp_test_environment",
    "**/_archive",
    "**/htmlcov"
]