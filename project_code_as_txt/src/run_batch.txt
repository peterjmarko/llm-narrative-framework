#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Filename: src/run_batch.py

"""
Main Batch Runner for the LLM Experiment Pipeline (run_batch.py)

Purpose:
This script serves as the primary Python-based entry point for executing a full
batch of experimental replications. It has replaced the complex logic formerly
in the `run_replications.ps1` PowerShell script.

Workflow:
1.  Reads high-level study parameters (e.g., `num_replications`) from `config.ini`.
2.  Loops through the specified number of replications (e.g., 1 to 30).
3.  For each replication, it calls `orchestrate_experiment.py`, passing the
    replication number and derived seeds as command-line arguments.
4.  It tracks the overall time elapsed and provides an Estimated Time Remaining (ETR)
    after each replication is complete.
5.  After the main loop finishes, it automatically calls the `retry_failed_sessions.py`
    script to fix any in-place failures.
6.  It then calls `verify_pipeline_completeness.py` to audit the entire batch.
7.  If verification passes, it calls `rebuild_batch_log.py` and `compile_results.py`
    to generate the final, clean summary files.

This script is typically launched by the simple `run_replications.ps1` wrapper.
"""

import sys
import os
import subprocess
import logging
import argparse
import time
import glob
import re
import json
import csv
from datetime import datetime
import pandas as pd

# Add src to path to find config_loader
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

try:
    from config_loader import APP_CONFIG, get_config_value
except ImportError as e:
    print(f"FATAL: Could not import config_loader.py. Error: {e}", file=sys.stderr)
    sys.exit(1)

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    stream=sys.stdout)

# ANSI Color Codes for console output
COLOR_CYAN = '\033[96m'
COLOR_GREEN = '\033[92m'
COLOR_END = '\033[0m'

def format_seconds_to_hms(seconds: float) -> str:
    """Formats total seconds into an HH:MM:SS string."""
    if seconds is None or seconds < 0:
        return "N/A"
    
    hours, remainder = divmod(int(seconds), 3600)
    minutes, seconds = divmod(remainder, 60)
    
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

def main():
    """
    Main entry point to run a full batch of experimental replications.
    This script reads the configuration and then calls orchestrate_experiment.py
    for each replication.
    """
    parser = argparse.ArgumentParser(description="Runs a batch of experimental replications.")
    parser.add_argument("--start_rep", type=int, default=1, help="The replication number to start the batch from.")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable detailed, verbose logging for all child scripts.")
    args = parser.parse_args()

    # --- Load Configuration ---
    try:
        num_reps = get_config_value(APP_CONFIG, 'Study', 'num_replications', value_type=int)
        project_root = os.path.dirname(current_dir)
        base_output_dir = get_config_value(APP_CONFIG, 'General', 'base_output_dir', fallback='output')
        final_output_dir = os.path.join(project_root, base_output_dir)
    except Exception as e:
        logging.error(f"Error reading required parameters from config.ini: {e}")
        sys.exit(1)

    if num_reps is None:
        logging.error("Could not find 'num_replications' in the [Study] section of config.ini. Aborting.")
        sys.exit(1)

    # --- Define Paths ---
    orchestrator_script = os.path.join(current_dir, "orchestrate_experiment.py")
    retry_script = os.path.join(current_dir, "retry_failed_sessions.py")
    compile_script = os.path.join(current_dir, "compile_results.py")

    logging.info(f"--- Starting Batch Experiment: {num_reps} Replications (Starting from rep {args.start_rep}) ---")
    
    batch_start_time = time.time()
    batch_log_path = os.path.join(final_output_dir, "batch_run_log.csv")

    # If resuming a run, clean up old failed/interrupted entries from the log
    if args.start_rep > 1 and os.path.exists(batch_log_path):
        try:
            logging.info(f"Resuming run. Cleaning log entries from replication {args.start_rep} onwards.")
            df = pd.read_csv(batch_log_path)
            # Ensure 'ReplicationNum' is numeric, turning errors into NaN
            df['ReplicationNum'] = pd.to_numeric(df['ReplicationNum'], errors='coerce')
            # Keep rows for replications *before* the start number, and drop malformed rows
            df_cleaned = df[df['ReplicationNum'] < args.start_rep].dropna(subset=['ReplicationNum'])
            # Convert the numeric column back to a clean integer for writing
            df_cleaned['ReplicationNum'] = df_cleaned['ReplicationNum'].astype(int)
            
            # Rewrite the cleaned log file, ensuring non-numeric fields are quoted
            df_cleaned.to_csv(batch_log_path, index=False, quoting=csv.QUOTE_NONNUMERIC)
            logging.info("Batch log cleaned successfully.")
        except Exception as e:
            logging.warning(f"Could not automatically clean up batch log file. New entries will be appended. Error: {e}")

    # Create log file with header if it doesn't exist
    if not os.path.exists(batch_log_path):
        try:
            with open(batch_log_path, 'w', newline='', encoding='utf-8') as f_log:
                writer = csv.writer(f_log)
                writer.writerow(["ReplicationNum", "Status", "StartTime", "EndTime", "Duration", "ParsingStatus", "MeanMRR", "MeanTop1Acc", "RunDirectory", "ErrorMessage"])
        except IOError as e:
            logging.error(f"Could not create batch log file at {batch_log_path}: {e}")
            sys.exit(1)
    
    # --- Main Replication Loop ---
    for i in range(args.start_rep, num_reps + 1):
        # Use print with ANSI codes for clean, colored headers
        print(f"\n{COLOR_CYAN}{'='*80}{COLOR_END}")
        print(f"{COLOR_CYAN}### RUNNING REPLICATION {i} of {num_reps} ###{COLOR_END}")
        print(f"{COLOR_CYAN}{'='*80}{COLOR_END}")

        # Seeds are derived from the replication number for reproducibility
        base_seed = 1000 * i
        qgen_seed = base_seed + 500

        cmd = [
            sys.executable, orchestrator_script,
            "--replication_num", str(i),
            "--base_seed", str(base_seed),
            "--qgen_base_seed", str(qgen_seed)
        ]

        # Add the --quiet flag to the child process unless --verbose is used.
        if not args.verbose:
            cmd.append("--quiet")
        
        log_entry = {
            "ReplicationNum": i, "Status": "FAILED", "StartTime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "EndTime": "N/A", "Duration": "N/A", "ParsingStatus": "N/A", "MeanMRR": "N/A",
            "MeanTop1Acc": "N/A", "RunDirectory": "N/A", "ErrorMessage": "Unknown error"
        }
        
        rep_start_time = time.time()

        try:
            # Always stream output. The --quiet flag passed to the child process
            # will control whether it prints detailed logs or just a progress bar.
            subprocess.run(cmd, check=True)

            log_entry["Status"] = "COMPLETED"
            log_entry["ErrorMessage"] = "N/A"

            # --- Timing Calculation and Logging ---
            elapsed_seconds = time.time() - batch_start_time
            completed_in_batch = i - args.start_rep + 1
            avg_time_per_rep = elapsed_seconds / completed_in_batch
            reps_remaining = num_reps - i
            etr_seconds = reps_remaining * avg_time_per_rep
            
            elapsed_str = format_seconds_to_hms(elapsed_seconds)
            etr_str = format_seconds_to_hms(etr_seconds)
            
            # Use print with ANSI codes for a clean, colored summary line
            print(f"{COLOR_GREEN}--- Replication {i} complete. Time Elapsed: {elapsed_str}. ETR: {etr_str} ---{COLOR_END}")

        except subprocess.CalledProcessError as e:
            log_entry["ErrorMessage"] = f"Orchestrator failed with exit code {e.returncode}."
            logging.error(f"!!! Replication {i} failed. See logs above. Continuing with next replication. !!!")
        except KeyboardInterrupt:
            log_entry["Status"] = "INTERRUPTED"
            log_entry["ErrorMessage"] = "Interrupted by user."
            logging.warning("\nBatch run interrupted by user. Halting.")
            # The finally block below will still run to log this entry.
            # Then, we exit cleanly with a non-zero status code.
            sys.exit(1)
        finally:
            # This block runs whether the replication succeeded or failed.
            rep_end_time = time.time()
            log_entry["EndTime"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            log_entry["Duration"] = format_seconds_to_hms(rep_end_time - rep_start_time)

            # Find the most recent run directory to get its name and report
            try:
                run_dirs = sorted(glob.glob(os.path.join(final_output_dir, "run_*")), key=os.path.getmtime)
                if run_dirs:
                    latest_run_dir = run_dirs[-1]
                    # Only associate this log entry if the directory was created for this replication
                    if f"rep-{i:02d}" in os.path.basename(latest_run_dir):
                        log_entry["RunDirectory"] = os.path.basename(latest_run_dir)
                        report_files = glob.glob(os.path.join(latest_run_dir, "replication_report_*.txt"))
                        if report_files:
                            with open(report_files[0], 'r', encoding='utf-8') as f_report:
                                content = f_report.read()
                            
                            # Parse report for metrics
                            parsing_match = re.search(r"Parsing Status:\s*(\d+/\d+.*)", content)
                            if parsing_match: log_entry["ParsingStatus"] = parsing_match.group(1).strip()
                            
                            json_match = re.search(r"<<<METRICS_JSON_START>>>(.*?)<<<METRICS_JSON_END>>>", content, re.DOTALL)
                            if json_match:
                                metrics = json.loads(json_match.group(1).strip())
                                log_entry["MeanMRR"] = f"{metrics.get('mean_mrr', 0.0):.4f}"
                                log_entry["MeanTop1Acc"] = f"{metrics.get('mean_top_1_acc', 0.0):.2%}"
            except Exception as parse_e:
                log_entry["ErrorMessage"] = f"Failed to parse report: {parse_e}"

            # Append the completed log entry to the CSV
            try:
                with open(batch_log_path, 'a', newline='', encoding='utf-8') as f_log:
                    # Use the log_entry dict's keys for the fieldnames to ensure order
                    fieldnames = ["ReplicationNum", "Status", "StartTime", "EndTime", "Duration", "ParsingStatus", "MeanMRR", "MeanTop1Acc", "RunDirectory", "ErrorMessage"]
                    writer = csv.DictWriter(f_log, fieldnames=fieldnames)
                    writer.writerow(log_entry)
            except IOError as e:
                logging.error(f"Could not write to batch log file {batch_log_path}: {e}")

    logging.info("\n" + "="*80)
    logging.info("### ALL REPLICATIONS COMPLETE. BEGINNING AUTO-REPAIR AND COMPILATION. ###")
    logging.info("="*80)

    # --- Auto-Repair Phase ---
    logging.info("\n--- Attempting to repair any failed sessions... ---")
    try:
        subprocess.run([sys.executable, retry_script, final_output_dir])
    except Exception as e:
        logging.error(f"An error occurred while running the retry script: {e}")

    # --- Verification & Finalization Phase ---
    verifier_script = os.path.join(current_dir, "verify_pipeline_completeness.py")
    rebuild_log_script = os.path.join(current_dir, "rebuild_batch_log.py")

    logging.info("\n--- Verifying final batch completeness... ---")
    try:
        # FIX: The verifier script expects a named argument --parent_dir
        subprocess.run([sys.executable, verifier_script, "--parent_dir", final_output_dir], check=True)
        
        # If verification passes, rebuild the log for a pristine record
        logging.info("\n--- Verification passed. Rebuilding batch log for consistency... ---")
        subprocess.run([sys.executable, rebuild_log_script, final_output_dir], check=True)

        # Finally, compile the master summary
        logging.info("\n--- Compiling final batch results... ---")
        subprocess.run([sys.executable, compile_script, final_output_dir], check=True)

    except subprocess.CalledProcessError:
        logging.error("\n!!! Verification FAILED. Batch is incomplete. !!!")
        logging.error("Please re-run the batch with the correct --start_rep to complete the missing runs.")
        logging.error("Skipping final log rebuild and compilation.")
    except Exception as e:
        logging.error(f"An error occurred during the finalization phase: {e}")
    
    logging.info("\n--- Batch Run Finished ---")
    sys.exit(0) # <--- ADD THIS LINE FOR SUCCESSFUL EXIT


if __name__ == "__main__":
    main()

# === End of src/run_batch.py ===