import unittest
import os
import sys
import shutil
import tempfile
from unittest.mock import patch, MagicMock, call
import pytest
import datetime
import json
import glob
import re
import subprocess

# --- Module-level patching to prevent orchestrate_experiment.main() from running on import ---
# No module-level sys.exit patch needed anymore, as it's done per test.

# Import orchestrate_experiment after sys.exit is mocked.
import orchestrate_experiment

# Save the original main function and patch it to a MagicMock.
# This prevents orchestrate_experiment.main() from executing when the module is imported
# by pytest, as it would try to parse pytest's arguments and exit.
_original_orchestrate_experiment_main = orchestrate_experiment.main
orchestrate_experiment.main = MagicMock()


class TestOrchestrateExperiment(unittest.TestCase):

    def setUp(self):
        # Add this line to see full diffs for list comparisons
        self.maxDiff = None

        # Create a temporary directory for the test's output
        self.test_dir_obj = tempfile.TemporaryDirectory(prefix="test_orch_")
        self.test_dir = self.test_dir_obj.name

        # Define a mock base output directory within the temp test_dir
        self.mock_base_output_dir = os.path.join(self.test_dir, 'output')
        os.makedirs(self.mock_base_output_dir, exist_ok=True)

        # Store original sys.argv for restoration in tearDown
        self.original_sys_argv = list(sys.argv)

        # Define REAL_SRC_DIR for use in assertions about subprocess calls
        # This is the *actual* src directory of the project, not a temporary one.
        self.REAL_SRC_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'src'))
        
        # When orchestrate_experiment.PROJECT_ROOT is patched to self.test_dir,
        # the internal src_dir will be os.path.join(self.test_dir, 'src').
        # We need to use this patched path for expected subprocess calls.
        self.MOCKED_PROJECT_SRC_DIR = os.path.join(self.test_dir, 'src')


        # Mock config values that orchestrate_experiment.py will try to load
        self.mock_config_values_map = {
            ('Study', 'num_trials'): 10,
            ('Study', 'group_size'): 2,
            ('LLM', 'model_name'): 'mock_model/test-llm',
            ('LLM', 'temperature'): 0.5,
            ('Filenames', 'personalities_src'): 'personalities_db_test.txt',
            ('General', 'base_output_dir'): os.path.basename(self.mock_base_output_dir), # Should be 'output'
            ('Filenames', 'base_query_src'): 'base_query.txt',
        }

        # Create a mock base_query.txt file for the report
        self.mock_data_dir = os.path.join(self.test_dir, 'data')
        os.makedirs(self.mock_data_dir, exist_ok=True)
        self.mock_base_query_path = os.path.join(self.mock_data_dir, 'base_query.txt')
        with open(self.mock_base_query_path, 'w') as f:
            f.write("Mock base query content.")

    def tearDown(self):
        # Clean up the temporary directory
        self.test_dir_obj.cleanup()
        # Restore original sys.argv using slice assignment (modifies list in place)
        sys.argv[:] = self.original_sys_argv
        
        # Clean up mock data dir if it was created just for the test
        if os.path.exists(self.mock_data_dir):
            shutil.rmtree(self.mock_data_dir)

    @classmethod
    def tearDownClass(cls):
        # Restore original orchestrate_experiment.main after all tests are done
        orchestrate_experiment.main = _original_orchestrate_experiment_main


    def mock_get_config_value(self, config_obj, section, key, fallback=None, value_type=str):
        """Mock function for get_config_value."""
        return self.mock_config_values_map.get((section, key), fallback)

    # --- Mock side effects for subprocess.run ---
    def _mock_run_success_side_effect(self, command, **kwargs):
        """Mock side effect for a successful subprocess run."""
        script_name = os.path.basename(command[1])
        if "analyze_performance.py" in script_name:
            # Return specific output for the analysis stage
            stdout = "Analyze Performance output.\n<<<ANALYSIS_SUMMARY_START>>>\nMock Analysis Summary\n<<<METRICS_JSON_START>>>{\"mean_mrr\":0.75,\"mean_top_1_acc\":0.60}<<<METRICS_JSON_END>>>\nANALYZER_VALIDATION_SUCCESS"
        elif "process_llm_responses.py" in script_name:
            # Return specific output for the process stage to test parsing status
            stdout = "Process Responses output.\n<<<PARSER_SUMMARY:5:10>>>\nPROCESSOR_VALIDATION_SUCCESS"
        else:
            stdout = f"Success from {script_name}"
        
        return MagicMock(returncode=0, stdout=stdout, stderr="")

    def _mock_run_failure_llm_sessions(self, command, **kwargs):
        """Mock side effect for a failing subprocess run at run_llm_sessions."""
        script_name = os.path.basename(command[1])
        if "run_llm_sessions.py" in script_name:
            error_stdout = "LLM runner started..."
            error_stderr = "FATAL: API connection failed. Mocked failure from run_llm_sessions.py"
            raise subprocess.CalledProcessError(
                returncode=1, cmd=command, output=error_stdout, stderr=error_stderr
            )
        # For other stages, simulate success
        return self._mock_run_success_side_effect(command, **kwargs)
    
    def _mock_run_failure_build_queries(self, command, **kwargs):
        """Mock side effect for a failing subprocess run specifically at build_queries.py."""
        script_name = os.path.basename(command[1])
        if "build_queries.py" in script_name:
            error_stdout = "Build queries started..."
            error_stderr = "FATAL: Could not access personalities database. Mocked failure from build_queries.py"
            raise subprocess.CalledProcessError(
                returncode=1, cmd=command, output=error_stdout, stderr=error_stderr
            )
        # For other stages, simulate success
        return self._mock_run_success_side_effect(command, **kwargs)

    @patch('orchestrate_experiment.subprocess.run')
    @patch('orchestrate_experiment.datetime') # Patch the datetime module itself
    @patch('orchestrate_experiment.sys.exit', new_callable=MagicMock) # Patch orchestrate_experiment.sys.exit for this test
    def test_new_run_happy_path(self, mock_sys_exit, mock_datetime_module, mock_subprocess_run):
        """
        Tests the happy path for a new experiment run, ensuring all stages are called
        and a correct report is generated.
        """
        # Arrange
        # For successful runs, orchestrate_experiment.main() does not call sys.exit(0)
        # It just finishes execution, and the interpreter exits with 0.
        # So, we assert that sys.exit was NOT called.
        
        # Mock datetime.datetime.now().strftime()
        mock_datetime_module.datetime.now.return_value = MagicMock(strftime=MagicMock(return_value="20250625_103000"))

        # Mock subprocess.run to simulate successful execution of child scripts
        mock_subprocess_run.side_effect = self._mock_run_success_side_effect

        # Mock sys.argv for orchestrate_experiment.py
        test_args = [
            'orchestrate_experiment.py',
            '--replication_num', '1',
            '--base_seed', '1000',
            '--qgen_base_seed', '1500',
            '--quiet',
            '--notes', 'Test new run happy path.'
        ]
        with patch.object(sys, 'argv', test_args):
            with patch('orchestrate_experiment.PROJECT_ROOT', new=self.test_dir): # Patch PROJECT_ROOT to the temporary directory
                with patch('orchestrate_experiment.get_config_value', side_effect=self.mock_get_config_value):
                    # Act
                    # No pytest.raises(SystemExit) because successful path doesn't call sys.exit
                    _original_orchestrate_experiment_main() # Call the original main

                    # Assert
                    mock_sys_exit.assert_not_called() # Verify sys.exit was NOT called

                    # Verify subprocess calls
                    expected_orch_dir_name = orchestrate_experiment.generate_run_dir_name(
                        model_name='mock_model/test-llm', temperature=0.5, num_iterations=10,
                        k_per_query=2, personalities_db='personalities_db_test.txt', replication_num=1
                    )
                    expected_run_specific_dir_path = os.path.join(self.mock_base_output_dir, expected_orch_dir_name)

                    expected_calls = [
                        call([
                            sys.executable, os.path.join(self.MOCKED_PROJECT_SRC_DIR, 'build_queries.py'), # Use MOCKED_PROJECT_SRC_DIR
                            '-m', '10', '-k', '2', '--mode', 'new', '--quiet-worker',
                            '--run_output_dir', expected_run_specific_dir_path,
                            '--quiet', # Moved --quiet here based on diff
                            '--base_seed', '1000', '--qgen_base_seed', '1500'
                        ]),
                        call([
                            sys.executable, os.path.join(self.MOCKED_PROJECT_SRC_DIR, 'run_llm_sessions.py'), # Use MOCKED_PROJECT_SRC_DIR
                            '--run_output_dir', expected_run_specific_dir_path, '--quiet'
                        ]),
                        call([
                            sys.executable, os.path.join(self.MOCKED_PROJECT_SRC_DIR, 'process_llm_responses.py'), # Use MOCKED_PROJECT_SRC_DIR
                            '--run_output_dir', expected_run_specific_dir_path, '--quiet'
                        ]),
                        call([
                            sys.executable, os.path.join(self.MOCKED_PROJECT_SRC_DIR, 'analyze_performance.py'), # Use MOCKED_PROJECT_SRC_DIR
                            '--run_output_dir', expected_run_specific_dir_path, '--quiet'
                        ]),
                    ]
                    # Use call_args_list to check the exact arguments passed to subprocess.run
                    # The run_script helper adds the 'title' and 'is_interactive' args, which are not part of the actual command list.
                    # So we need to check the first element of each call's args tuple.
                    actual_calls_commands = [c.args[0] for c in mock_subprocess_run.call_args_list]
                    
                    normalized_actual_calls = []
                    for actual_call_list in actual_calls_commands:
                        normalized_call_list = [os.path.normpath(arg) for arg in actual_call_list]
                        normalized_actual_calls.append(normalized_call_list)
                    
                    normalized_expected_calls = []
                    for expected_call_obj in expected_calls: # Iterate over call objects
                        expected_call_list = expected_call_obj.args[0] # Get the actual command list
                        normalized_call_list = [os.path.normpath(arg) for arg in expected_call_list]
                        normalized_expected_calls.append(normalized_call_list)

                    self.assertEqual(normalized_actual_calls, normalized_expected_calls)
                    self.assertEqual(mock_subprocess_run.call_count, len(expected_calls))

                    # Verify report file creation and content
                    report_files = glob.glob(os.path.join(expected_run_specific_dir_path, 'replication_report_*.txt'))
                    self.assertEqual(len(report_files), 1)
                    report_path = report_files[0]

                    with open(report_path, 'r', encoding='utf-8') as f:
                        report_content = f.read()

                    self.assertIn("REPLICATION RUN REPORT", report_content)
                    self.assertIn("Final Status:    COMPLETED", report_content)
                    self.assertIn(f"Run Directory:   {os.path.basename(expected_run_specific_dir_path)}", report_content)
                    self.assertIn("Parsing Status:  5/10 responses parsed", report_content)
                    self.assertIn("Validation Status: OK (All checks passed)", report_content)
                    self.assertIn("Num Iterations (m): 10", report_content)
                    self.assertIn("Items per Query (k): 2", report_content)
                    self.assertIn("LLM Model:       mock_model/test-llm", report_content)
                    self.assertIn("Run Notes:       Test new run happy path.", report_content)
                    self.assertIn("Mock base query content.", report_content)
                    self.assertIn("Mock Analysis Summary", report_content)
                    self.assertNotIn("FULL DIAGNOSTIC LOG", report_content) # Should not be present on success
    
    @patch('orchestrate_experiment.subprocess.run')
    @patch('orchestrate_experiment.datetime') # Patch the datetime module itself
    @patch('orchestrate_experiment.sys.exit', new_callable=MagicMock) # Patch orchestrate_experiment.sys.exit for this test
    def test_reprocess_mode_happy_path(self, mock_sys_exit, mock_datetime_module, mock_subprocess_run):
        """
        Tests the happy path for reprocess mode, ensuring query building and LLM calls are skipped.
        """
        # Arrange
        # For successful runs, orchestrate_experiment.main() does not call sys.exit(0)
        # It just finishes execution, and the interpreter exits with 0.
        # So, we assert that sys.exit was NOT called.

        # Mock datetime.datetime.now().strftime()
        mock_datetime_module.datetime.now.return_value = MagicMock(strftime=MagicMock(return_value="20250625_110000"))
    
        # Create a mock existing run directory with a name that allows k/m discovery
        existing_run_dir_name = "run_20240101_120000_rep-01_model_tmp-0.50_db_sbj-05_trl-020"
        self.existing_run_path = os.path.join(self.mock_base_output_dir, existing_run_dir_name)
        os.makedirs(self.existing_run_path, exist_ok=True)
        # Create dummy session_queries and session_responses for reprocess mode
        os.makedirs(os.path.join(self.existing_run_path, 'session_queries'), exist_ok=True)
        os.makedirs(os.path.join(self.existing_run_path, 'session_responses'), exist_ok=True)
        # Create a dummy query JSON for model discovery
        with open(os.path.join(self.existing_run_path, 'session_queries', 'llm_query_001_full.json'), 'w') as f:
            json.dump({"model": "reprocessed_model/test-llm"}, f)

        # Mock subprocess.run for process and analyze stages
        mock_subprocess_run.side_effect = self._mock_run_success_side_effect

        # Mock sys.argv for orchestrate_experiment.py
        test_args = [
            'orchestrate_experiment.py',
            '--reprocess',
            '--run_output_dir', self.existing_run_path,
            '--quiet'
        ]
        with patch.object(sys, 'argv', test_args):
            with patch('orchestrate_experiment.PROJECT_ROOT', new=self.test_dir): # Patch PROJECT_ROOT to the temporary directory
                with patch('orchestrate_experiment.get_config_value', side_effect=self.mock_get_config_value):
                    # Act
                    # No pytest.raises(SystemExit) because successful path doesn't call sys.exit
                    _original_orchestrate_experiment_main() # Call the original main

                    # Assert
                    mock_sys_exit.assert_not_called() # Verify sys.exit was NOT called

                    # Verify subprocess calls - only process and analyze should be called
                    expected_calls = [
                        call([
                            sys.executable, os.path.join(self.MOCKED_PROJECT_SRC_DIR, 'process_llm_responses.py'), # Use MOCKED_PROJECT_SRC_DIR
                            '--run_output_dir', self.existing_run_path, '--quiet'
                        ]),
                        call([
                            sys.executable, os.path.join(self.MOCKED_PROJECT_SRC_DIR, 'analyze_performance.py'), # Use MOCKED_PROJECT_SRC_DIR
                            '--run_output_dir', self.existing_run_path, '--quiet'
                        ]),
                    ]
                    actual_calls_commands = [c.args[0] for c in mock_subprocess_run.call_args_list]
                    # FIX: Normalize paths for comparison
                    normalized_actual_calls = []
                    for actual_call_list in actual_calls_commands:
                        normalized_call_list = [os.path.normpath(arg) for arg in actual_call_list]
                        normalized_actual_calls.append(normalized_call_list)
                    
                    normalized_expected_calls = []
                    for expected_call_obj in expected_calls: # Iterate over call objects
                        expected_call_list = expected_call_obj.args[0] # Get the actual command list
                        normalized_call_list = [os.path.normpath(arg) for arg in expected_call_list]
                        normalized_expected_calls.append(normalized_call_list)

                    self.assertEqual(normalized_actual_calls, normalized_expected_calls)
                    self.assertEqual(mock_subprocess_run.call_count, len(expected_calls))

                    # Verify report file creation and content
                    report_files = glob.glob(os.path.join(self.existing_run_path, 'replication_report_*.txt'))
                    self.assertEqual(len(report_files), 1)
                    report_path = report_files[0]

                    with open(report_path, 'r', encoding='utf-8') as f:
                        report_content = f.read()

                    self.assertIn("REPLICATION RUN REPORT", report_content)
                    self.assertIn("Final Status:    COMPLETED", report_content)
                    self.assertIn(f"Run Directory:   {os.path.basename(self.existing_run_path)}", report_content)
                    self.assertIn("Parsing Status:  5/10 responses parsed", report_content) # This is from mock_run_success_side_effect
                    self.assertIn("Validation Status: OK (All checks passed)", report_content)
                    self.assertIn("Num Iterations (m): 20", report_content) # Verify via report
                    self.assertIn("Items per Query (k): 5", report_content) # Verify via report
                    self.assertIn("LLM Model:       reprocessed_model/test-llm", report_content) # Discovered model
                    self.assertIn("Run Notes:       N/A", report_content) # Default notes
                    self.assertIn("Mock Analysis Summary", report_content) # This is from mock_run_success_side_effect
                    self.assertNotIn("FULL DIAGNOSTIC LOG", report_content)

    @patch('orchestrate_experiment.subprocess.run')
    @patch('orchestrate_experiment.datetime') # Patch the datetime module itself
    @patch('orchestrate_experiment.sys.exit', new_callable=MagicMock) # Patch orchestrate_experiment.sys.exit for this test
    def test_subprocess_called_process_error_handling(self, mock_sys_exit, mock_datetime_module, mock_subprocess_run):
        """
        Tests that the orchestrator handles a CalledProcessError from a child script
        and generates a diagnostic report.
        """
        # Arrange
        # orchestrate_experiment.main() does not call sys.exit(0) after reporting a CalledProcessError
        # It just finishes execution, and the interpreter exits with 0.
        
        # Mock datetime.datetime.now().strftime()
        mock_datetime_module.datetime.now.return_value = MagicMock(strftime=MagicMock(return_value="20250625_120000"))

        # Simulate build_queries.py failing
        mock_subprocess_run.side_effect = self._mock_run_failure_llm_sessions

        test_args = [
            'orchestrate_experiment.py',
            '--replication_num', '2',
            '--quiet'
        ]
        with patch.object(sys, 'argv', test_args):
            with patch('orchestrate_experiment.PROJECT_ROOT', new=self.test_dir):
                with patch('orchestrate_experiment.get_config_value', side_effect=self.mock_get_config_value):
                    # Act
                    # No pytest.raises(SystemExit) because the exception is caught internally
                    _original_orchestrate_experiment_main() # Call the original main

                    # Assert
                    mock_sys_exit.assert_not_called() # Verify sys.exit was NOT called

                    # Verify subprocess calls - only build_queries and run_llm_sessions should have been called
                    self.assertEqual(mock_subprocess_run.call_count, 2)
                    
                    # Verify report file creation and content
                    # Need to get the actual run directory name generated by the script
                    created_dirs = [d for d in os.listdir(self.mock_base_output_dir) if os.path.isdir(os.path.join(self.mock_base_output_dir, d))]
                    self.assertEqual(len(created_dirs), 1)
                    run_specific_dir_path = os.path.join(self.mock_base_output_dir, created_dirs[0])

                    report_files = glob.glob(os.path.join(run_specific_dir_path, 'replication_report_*.txt'))
                    self.assertEqual(len(report_files), 1)
                    report_path = report_files[0]

                    with open(report_path, 'r', encoding='utf-8') as f:
                        report_content = f.read()

                    self.assertIn("Final Status:    FAILED", report_content)
                    self.assertIn("FULL DIAGNOSTIC LOG", report_content)
                    # FIX: Change assertion to match mock output
                    self.assertIn("Success from build_queries.py", report_content) 
                    self.assertIn("FATAL: API connection failed. Mocked failure from run_llm_sessions.py", report_content) # Error message should be in diagnostic log
                    self.assertNotIn("Mock Analysis Summary", report_content) # Should not have reached analysis stage

    @patch('orchestrate_experiment.subprocess.run')
    @patch('orchestrate_experiment.datetime') # Patch the datetime module itself
    @patch('orchestrate_experiment.sys.exit', new_callable=MagicMock) # Patch orchestrate_experiment.sys.exit for this test
    def test_failure_in_build_queries_aborts_run(self, mock_sys_exit, mock_datetime_module, mock_subprocess_run):
        """Test that a failure in the first stage (build_queries) aborts the pipeline correctly."""
        # Arrange
        # orchestrate_experiment.main() does not call sys.exit(0) after reporting a CalledProcessError
        # It just finishes execution, and the interpreter exits with 0.
        
        # Mock datetime.datetime.now().strftime()
        mock_datetime_module.datetime.now.return_value = MagicMock(strftime=MagicMock(return_value="20250625_140000"))

        mock_subprocess_run.side_effect = self._mock_run_failure_build_queries
        
        orchestrator_args = ['orchestrate_experiment.py']
        with patch.object(sys, 'argv', orchestrator_args):
            with patch('orchestrate_experiment.PROJECT_ROOT', new=self.test_dir):
                with patch('orchestrate_experiment.get_config_value', side_effect=self.mock_get_config_value):
                    # Act
                    # No pytest.raises(SystemExit) because the exception is caught internally
                    _original_orchestrate_experiment_main() # Call the original main

                    # Assert
                    mock_sys_exit.assert_not_called() # Verify sys.exit was NOT called

                    # Assert that the pipeline was aborted immediately
                    self.assertEqual(mock_subprocess_run.call_count, 1, "Should have stopped after the 1st stage failed.")

                    # Assert that a report was still created
                    created_dirs = [d for d in os.listdir(self.mock_base_output_dir) if os.path.isdir(os.path.join(self.mock_base_output_dir, d))]
                    self.assertEqual(len(created_dirs), 1) # Should still be only one new dir created by this test
                    run_specific_dir_path = os.path.join(self.mock_base_output_dir, created_dirs[0])
                    
                    report_files = glob.glob(os.path.join(run_specific_dir_path, 'replication_report_*.txt'))
                    self.assertEqual(len(report_files), 1) # A report file should still be created on early failure.

                    # Need to read the report content from the actual report_path
                    with open(report_files[0], 'r', encoding='utf-8') as f:
                        report_content = f.read()
                        
                    self.assertIn("Final Status:    FAILED", report_content)
                    self.assertIn("FULL DIAGNOSTIC LOG", report_content)
                    self.assertIn("Mocked failure from build_queries.py", report_content)
    
    @patch('orchestrate_experiment.subprocess.run')
    @patch('orchestrate_experiment.datetime') # Patch the datetime module itself
    @patch('orchestrate_experiment.sys.exit', new_callable=MagicMock) # Patch orchestrate_experiment.sys.exit for this test
    def test_keyboard_interrupt_handling(self, mock_sys_exit, mock_datetime_module, mock_subprocess_run):
        """
        Tests that the orchestrator handles a KeyboardInterrupt gracefully
        and generates an interrupted report.
        """
        # Arrange
        # orchestrate_experiment.main() handles KeyboardInterrupt internally
        
        # Mock datetime.datetime.now().strftime()
        mock_datetime_module.datetime.now.return_value = MagicMock(strftime=MagicMock(return_value="20250625_130000"))

        # Simulate KeyboardInterrupt during run_llm_sessions.py
        mock_subprocess_run.side_effect = [
                MagicMock(returncode=0, stdout="Build Queries output.", stderr=""), # Simulates successful build_queries
                KeyboardInterrupt("User interrupted.") # Raises KeyboardInterrupt for run_llm_sessions
            ]

        test_args = [
            'orchestrate_experiment.py',
            '--replication_num', '3',
            '--quiet'
        ]
        with patch.object(sys, 'argv', test_args):
            with patch('orchestrate_experiment.PROJECT_ROOT', new=self.test_dir):
                with patch('orchestrate_experiment.get_config_value', side_effect=self.mock_get_config_value):
                    # Act
                    # No pytest.raises(SystemExit) because the exception is caught internally
                    _original_orchestrate_experiment_main() # Call the original main

                    # Assert
                    mock_sys_exit.assert_not_called() # Verify sys.exit was NOT called

                    # Verify subprocess calls - only build_queries should have been called fully
                    self.assertEqual(mock_subprocess_run.call_count, 2) # build_queries + run_llm_sessions (which raises)

                    # Verify report file creation and content
                    created_dirs = [d for d in os.listdir(self.mock_base_output_dir) if os.path.isdir(os.path.join(self.mock_base_output_dir, d))]
                    self.assertEqual(len(created_dirs), 1)
                    run_specific_dir_path = os.path.join(self.mock_base_output_dir, created_dirs[0])

                    report_files = glob.glob(os.path.join(run_specific_dir_path, 'replication_report_*.txt'))
                    self.assertEqual(len(report_files), 1)
                    report_path = report_files[0]

                    with open(report_path, 'r', encoding='utf-8') as f:
                        report_content = f.read()

                    self.assertIn("Final Status:    INTERRUPTED BY USER", report_content)
                    self.assertIn("FULL DIAGNOSTIC LOG", report_content)
                    # FIX: Change assertion to match mock output (from _mock_run_success_side_effect)
                    self.assertIn("Build Queries output.", report_content) 
                    self.assertNotIn("Mock Analysis Summary", report_content) # Should not have reached analysis stage
    
    def test_generate_run_dir_name(self):
        """
        Tests the generate_run_dir_name function for correct formatting and sanitization.
        """
        # Arrange
        # Mock datetime.datetime.now() to ensure predictable timestamps
        # Patch datetime.datetime directly for this test
        with patch('orchestrate_experiment.datetime.datetime') as mock_dt_class:
            mock_dt_class.now.return_value = MagicMock(strftime=MagicMock(return_value="20250625_140000"))

            # Act
            dir_name = orchestrate_experiment.generate_run_dir_name(
                model_name="anthropic/claude-3-opus",
                temperature=0.2,
                num_iterations=100,
                k_per_query=10,
                personalities_db="data/personalities_db_01.txt",
                replication_num=5
            )

            # Assert
            expected_name = "run_20250625_140000_rep-05_claude-3-opus_tmp-0.20_personalities_db_01_sbj-10_trl-100"
            self.assertEqual(dir_name, expected_name)

            # Test with problematic characters in model name and db path
            dir_name_sanitized = orchestrate_experiment.generate_run_dir_name(
                model_name="bad/model name with spaces!",
                temperature=0.15,
                num_iterations=123,
                k_per_query=7,
                personalities_db="data/personalities_db_02 (copy).txt",
                replication_num=12
            )
            # FIX: Adjusted expected_sanitized_name based on the likely behavior of
            # generate_run_dir_name using os.path.basename on the model_name before sanitization.
            expected_sanitized_name = "run_20250625_140000_rep-12_model_name_with_spaces__tmp-0.15_personalities_db_02__copy__sbj-07_trl-123"
            self.assertEqual(dir_name_sanitized, expected_sanitized_name)

            # Test with None/empty values (these should now be handled by generate_run_dir_name itself)
            dir_name_defaults = orchestrate_experiment.generate_run_dir_name(
                model_name=None,
                temperature=None,
                num_iterations=None, # Changed to None
                k_per_query=None,    # Changed to None
                personalities_db=None,
                replication_num=0
            )
            expected_defaults_name = "run_20250625_140000_rep-00_unknown_model_tmp-0.00_unknown_db_sbj-XX_trl-XXX" # Updated expected
            self.assertEqual(dir_name_defaults, expected_defaults_name)
    
    @patch('orchestrate_experiment.subprocess.run')
    def test_run_script_interactive_mode(self, mock_subprocess_run):
        """
        Tests the run_script helper function in interactive mode.
        """
        # Arrange
        mock_subprocess_run.return_value = MagicMock(returncode=0)
        command = [sys.executable, "mock_script.py"]
        title = "Interactive Test"

        # Act
        output = orchestrate_experiment.run_script(command, title, is_interactive=True)

        # Assert
        mock_subprocess_run.assert_called_once_with(
            command,
            check=True,
            text=True,
            encoding='utf-8',
            errors='replace'
        )
        self.assertIn("Interactive stage output was displayed on the console and not captured in this report.", output)
        self.assertIn(f"### STAGE: {title} ###", output)

    @patch('orchestrate_experiment.subprocess.run')
    def test_run_script_non_interactive_mode(self, mock_subprocess_run):
        """
        Tests the run_script helper function in non-interactive mode,
        including output capture and RuntimeWarning filtering.
        """
        # Arrange
        mock_subprocess_run.return_value = MagicMock(
            returncode=0,
            stdout="Normal output line 1.\nRuntimeWarning: This is a warning.\nNormal output line 2.",
            stderr="Error output line 1."
        )
        command = [sys.executable, "mock_script.py"]
        title = "Non-Interactive Test"

        # Act
        output = orchestrate_experiment.run_script(command, title, is_interactive=False)

        # Assert
        mock_subprocess_run.assert_called_once_with(
            command,
            capture_output=True,
            check=True,
            text=True,
            encoding='utf-8',
            errors='replace'
        )
        self.assertIn("Normal output line 1.", output)
        self.assertNotIn("RuntimeWarning", output) # Warning should be filtered
        self.assertIn("Normal output line 2.", output)
        self.assertIn("Error output line 1.", output)
        self.assertIn(f"### STAGE: {title} ###", output)
    
    @patch('orchestrate_experiment.subprocess.run')
    def test_run_script_error_propagation(self, mock_subprocess_run):
        """
        Tests that run_script propagates subprocess errors.
        """
        # Arrange
        mock_subprocess_run.side_effect = subprocess.CalledProcessError(returncode=1, cmd="failed_script.py", stderr="Script failed!")
        command = [sys.executable, "failed_script.py"]
        title = "Failing Test"

        # Act & Assert
        with self.assertRaises(subprocess.CalledProcessError) as cm:
            orchestrate_experiment.run_script(command, title)
        self.assertIn("Script failed!", cm.exception.stderr)
    
    @patch('orchestrate_experiment.sys.exit', new_callable=MagicMock) # Patch sys.exit directly
    @patch('orchestrate_experiment.subprocess.run')
    @patch('orchestrate_experiment.datetime') # Patch the datetime module itself
    def test_reprocess_mode_missing_dir_arg(self, mock_datetime_module, mock_subprocess_run, mock_sys_exit): # Corrected argument order
        """
        Tests that reprocess mode exits with an error if --run_output_dir is not provided.
        """
        # Arrange - missing --run_output_dir
        # orchestrate_experiment.main() calls sys.exit(1) here
        mock_sys_exit.side_effect = SystemExit(1) # Make the mock raise SystemExit(1) when called
        
        # Mock datetime.datetime.now().strftime()
        mock_datetime_module.datetime.now.return_value = MagicMock(strftime=MagicMock(return_value="20250625_150000"))

        test_args_missing = [
            'orchestrate_experiment.py',
            '--reprocess',
            '--quiet'
        ]
        with patch.object(sys, 'argv', test_args_missing):
            with patch('orchestrate_experiment.PROJECT_ROOT', new=self.test_dir):
                with patch('orchestrate_experiment.get_config_value', side_effect=self.mock_get_config_value):
                    # Act & Assert
                    with pytest.raises(SystemExit) as excinfo:
                        _original_orchestrate_experiment_main() # Call the original main
                    self.assertEqual(excinfo.value.code, 1) # Expect error exit
                    mock_sys_exit.assert_called_once_with(1)
    
    @patch('orchestrate_experiment.sys.exit', new_callable=MagicMock) # Patch sys.exit directly
    @patch('orchestrate_experiment.subprocess.run')
    @patch('orchestrate_experiment.datetime') # Patch the datetime module itself
    def test_reprocess_mode_non_existent_dir(self, mock_datetime_module, mock_subprocess_run, mock_sys_exit): # Corrected argument order
        """
        Tests that reprocess mode exits with an error if --run_output_dir points to a non-existent directory.
        """
        # Arrange - non-existent --run_output_dir
        # orchestrate_experiment.main() calls sys.exit(1) here
        mock_sys_exit.side_effect = SystemExit(1) # Make the mock raise SystemExit(1) when called
        
        # Mock datetime.datetime.now().strftime()
        mock_datetime_module.datetime.now.return_value = MagicMock(strftime=MagicMock(return_value="20250625_160000"))

        non_existent_path = os.path.join(self.test_dir, "non_existent_run_dir")
        test_args_non_existent = [
            'orchestrate_experiment.py',
            '--reprocess',
            '--run_output_dir', non_existent_path,
            '--quiet'
        ]
        with patch.object(sys, 'argv', test_args_non_existent):
            with patch('orchestrate_experiment.PROJECT_ROOT', new=self.test_dir):
                with patch('orchestrate_experiment.get_config_value', side_effect=self.mock_get_config_value):
                    # Act & Assert
                    with pytest.raises(SystemExit) as excinfo:
                        _original_orchestrate_experiment_main() # Call the original main
                    self.assertEqual(excinfo.value.code, 1) # Expect error exit
                    mock_sys_exit.assert_called_once_with(1)
    
    @patch('orchestrate_experiment.sys.exit', new_callable=MagicMock) # Patch sys.exit directly
    @patch('orchestrate_experiment.subprocess.run')
    @patch('orchestrate_experiment.datetime') # Patch the datetime module itself
    def test_config_error_handling(self, mock_datetime_module, mock_subprocess_run, mock_sys_exit): # Corrected argument order
        """
        Tests that the orchestrator handles errors in config loading gracefully.
        """
        # Arrange: Make get_config_value return None for a critical value
        # orchestrate_experiment.main() calls sys.exit(1) here
        mock_sys_exit.side_effect = SystemExit(1) # Make the mock raise SystemExit(1) when called
        self.mock_config_values_map[('Study', 'num_trials')] = None
        
        # Mock datetime.datetime.now().strftime()
        mock_datetime_module.datetime.now.return_value = MagicMock(strftime=MagicMock(return_value="20250625_170000"))

        test_args = [
            'orchestrate_experiment.py',
            '--replication_num', '1',
            '--quiet'
        ]
        with patch.object(sys, 'argv', test_args):
            with patch('orchestrate_experiment.PROJECT_ROOT', new=self.test_dir):
                with patch('orchestrate_experiment.get_config_value', side_effect=self.mock_get_config_value):
                    # Act & Assert
                    with pytest.raises(SystemExit) as excinfo:
                        _original_orchestrate_experiment_main() # Call the original main
                    self.assertEqual(excinfo.value.code, 1) # Expect error exit
                    mock_sys_exit.assert_called_once_with(1)

        # Restore config map for other tests (important if tests are not isolated by process)
        # This is handled by the test runner's isolation for each test method.
        self.mock_config_values_map[('Study', 'num_trials')] = 10

    @patch('orchestrate_experiment.subprocess.run')
    @patch('orchestrate_experiment.datetime') # Patch the datetime module itself
    @patch('orchestrate_experiment.sys.exit', new_callable=MagicMock) # Patch orchestrate_experiment.sys.exit for this test
    def test_report_old_reports_cleanup(self, mock_sys_exit, mock_datetime_module, mock_subprocess_run):
        """
        Tests that old replication reports are cleaned up before a new one is written.
        """
        # Arrange
        # For successful runs, orchestrate_experiment.main() does not call sys.exit(0)
        # It just finishes execution, and the interpreter exits with 0.
        # So, we assert that sys.exit was NOT called.

        # Mock datetime.datetime.now().strftime()
        mock_datetime_module.datetime.now.return_value = MagicMock(strftime=MagicMock(return_value="20250625_103000"))

        # Mock subprocess.run for success
        mock_subprocess_run.side_effect = self._mock_run_success_side_effect

        # Create a mock run directory and some old reports within it
        expected_orch_dir_name = orchestrate_experiment.generate_run_dir_name(
            model_name='mock_model/test-llm', temperature=0.5, num_iterations=10,
            k_per_query=2, personalities_db='personalities_db_test.txt', replication_num=1
        )
        run_specific_dir_path = os.path.join(self.mock_base_output_dir, expected_orch_dir_name)
        os.makedirs(run_specific_dir_path, exist_ok=True)

        old_report1 = os.path.join(run_specific_dir_path, "replication_report_20240101-100000.txt")
        old_report2 = os.path.join(run_specific_dir_path, "replication_report_20240102-110000.txt")
        with open(old_report1, 'w') as f: f.write("Old report 1")
        with open(old_report2, 'w') as f: f.write("Old report 2")
        self.assertTrue(os.path.exists(old_report1))
        self.assertTrue(os.path.exists(old_report2))

        test_args = ['orchestrate_experiment.py', '--replication_num', '1', '--quiet']
        with patch.object(sys, 'argv', test_args):
            with patch('orchestrate_experiment.PROJECT_ROOT', new=self.test_dir):
                with patch('orchestrate_experiment.get_config_value', side_effect=self.mock_get_config_value):
                    # Act
                    # No pytest.raises(SystemExit) because successful path doesn't call sys.exit
                    _original_orchestrate_experiment_main() # Call the original main

                    # Assert
                    mock_sys_exit.assert_not_called() # Verify sys.exit was NOT called
                    self.assertFalse(os.path.exists(old_report1)) # Old reports should be gone
                    self.assertFalse(os.path.exists(old_report2))
                    
                    # A new report should exist
                    new_report_files = glob.glob(os.path.join(run_specific_dir_path, 'replication_report_*.txt'))
                    self.assertEqual(len(new_report_files), 1)
    

if __name__ == '__main__':
    unittest.main()