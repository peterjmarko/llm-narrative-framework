import unittest
from unittest.mock import patch, mock_open, MagicMock
import os
import sys
import json
import subprocess
import shutil
import tempfile
import logging # Import logging for log level checks

# Determine the absolute path to llm_prompter.py and config_loader.py
# This assumes test_llm_prompter.py is in tests/ and llm_prompter.py/config_loader.py are in ../src/
SCRIPT_DIR_TEST = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT_TEST = os.path.abspath(os.path.join(SCRIPT_DIR_TEST, '..'))
SRC_DIR_TEST = os.path.join(PROJECT_ROOT_TEST, 'src')

LLM_PROMPTER_SCRIPT_PATH = os.path.join(SRC_DIR_TEST, "llm_prompter.py")
CONFIG_LOADER_SCRIPT_PATH = os.path.join(SRC_DIR_TEST, "config_loader.py")

# Mock config.ini content for llm_prompter.py to load
MOCK_CONFIG_INI_CONTENT = """
[General]
default_log_level = INFO
base_output_dir = ./output
responses_subdir = session_responses_test_dir

[LLM]
model_name = mock-model/from-config-for-prompter
api_endpoint = https://this.is.a.mock.endpoint.com/api/v1
api_timeout_seconds = 10
referer_header = http://prompter-config-referer
max_tokens = 80
temperature = 0.6

[Filenames]
# Ensure these keys exist if get_config_value in llm_prompter tries to read them
personalities_src = personalities_test.txt
base_query_src = base_query_test.txt
temp_subset_personalities = temp_subset_test.txt
used_indices_log = used_indices_test.log
aggregated_mappings_in_queries_dir = mappings_agg_test.txt
all_scores_for_analysis = scores_final_test.txt
all_mappings_for_analysis = mappings_final_test.txt
qgen_temp_prefix = ""
"""

MOCK_DOTENV_CONTENT = "OPENROUTER_API_KEY=env_mock_api_key_for_prompter_tests\n"

class TestLLMPrompterEndToEnd(unittest.TestCase):

    def setUp(self):
        # Create a temporary directory for each test run
        self.test_run_dir_obj = tempfile.TemporaryDirectory(prefix="test_llm_prompter_run_")
        self.test_run_dir = self.test_run_dir_obj.name

        # Define paths for files within the temporary directory
        self.config_file = os.path.join(self.test_run_dir, "config.ini")
        self.dotenv_file = os.path.join(self.test_run_dir, ".env")
        self.input_query_file = os.path.join(self.test_run_dir, "test_query_input.txt")
        self.output_response_file = os.path.join(self.test_run_dir, "test_response_output.txt")
        self.output_error_file = os.path.join(self.test_run_dir, "test_error_output.txt")

        # Write mock config and dotenv content to the temporary directory
        with open(self.config_file, "w", encoding='utf-8') as f:
            f.write(MOCK_CONFIG_INI_CONTENT)
        with open(self.dotenv_file, "w", encoding='utf-8') as f:
            f.write(MOCK_DOTENV_CONTENT)

        # Copy config_loader.py into the temporary directory
        # This ensures the subprocess finds config_loader.py relative to its CWD
        if os.path.exists(CONFIG_LOADER_SCRIPT_PATH):
            shutil.copy2(CONFIG_LOADER_SCRIPT_PATH, self.test_run_dir)
        else:
            self.fail(f"Critical test setup error: config_loader.py not found at {CONFIG_LOADER_SCRIPT_PATH}")

    def tearDown(self):
        # Clean up the temporary directory after each test
        self.test_run_dir_obj.cleanup()

        # Clean up files created by llm_prompter.py in SRC_DIR_TEST during interactive_test_mode
        interactive_input_file_in_src = os.path.join(SRC_DIR_TEST, "interactive_test_query.txt")
        interactive_response_file_in_src = os.path.join(SRC_DIR_TEST, "interactive_test_response.txt")
        interactive_error_file_in_src = os.path.join(SRC_DIR_TEST, "interactive_test_error.txt")
        debug_json_filename_default_in_src = os.path.join(SRC_DIR_TEST, "interactive_test_response_full.json")

        for f_path in [interactive_input_file_in_src, interactive_response_file_in_src,
                       interactive_error_file_in_src, debug_json_filename_default_in_src]:
            if os.path.exists(f_path):
                try:
                    os.remove(f_path)
                except OSError as e:
                    print(f"WARNING: Could not remove test cleanup file {f_path}: {e}")


    def create_test_input_query_file(self, content="Default test query content."):
        """Helper to create the input query file with specified content."""
        with open(self.input_query_file, "w", encoding='utf-8') as f:
            f.write(content)

    def run_llm_prompter_subprocess(self, query_id, cli_extra_args=None, pass_io_paths=True):
        """
        Helper to run llm_prompter.py as a subprocess.
        The CWD for the subprocess is self.test_run_dir, allowing it to find
        the mock config.ini, .env, and config_loader.py.
        `pass_io_paths`: If False, --input_query_file, --output_response_file, --output_error_file
                         will NOT be passed, forcing interactive_test_mode behavior.
        """
        base_cmd = [
            sys.executable, LLM_PROMPTER_SCRIPT_PATH,
            query_id,
        ]
        
        if pass_io_paths:
            base_cmd.extend([
                "--input_query_file", str(self.input_query_file),
                "--output_response_file", str(self.output_response_file),
                "--output_error_file", str(self.output_error_file),
            ])

        if cli_extra_args:
            base_cmd.extend(cli_extra_args)

        # --- IMPORTANT: Pass COVERAGE_PROCESS_START to the subprocess ---
        # This tells coverage.py to start monitoring in the new process.
        # It needs to point to the .coveragerc file (or pytest.ini if it contains coverage config).
        # Do NOT set COVERAGE_FILE here. Let pytest-cov handle it automatically for parallel runs.
        env = os.environ.copy()
        env['COVERAGE_PROCESS_START'] = os.path.join(PROJECT_ROOT_TEST, 'pytest.ini') # Point to your pytest.ini
        # ------------------------------------------------------------------

        return subprocess.run(
            base_cmd,
            capture_output=True,
            text=True, # Decode stdout/stderr as text using default encoding (usually UTF-8)
            cwd=self.test_run_dir,
            encoding='utf-8', # Explicitly set encoding for consistent output
            env=env # Pass the modified environment variables
        )

    def _assert_log_message_present(self, log_output, level, message_part):
        """Helper to check if a log message exists in the stderr output."""
        # Adjust log_pattern to be more flexible, as the timestamp and filename/line number vary.
        # We only care about the level and the message part.
        # Example: 2025-06-25 05:11:37 - INFO - llm_prompter.py:266 - Running in worker mode (paths provided by orchestrator or user).
        # We look for "- LEVEL - " followed by the message part.
        log_pattern = f"- {level.upper()} - "
        
        # Split log_output into lines and check each line for the pattern
        found = False
        for line in log_output.splitlines():
            if log_pattern in line and message_part in line:
                found = True
                break
        
        self.assertTrue(found, f"Log message part '{message_part}' not found at level {level} in output:\n{log_output}")

    # --- Test Cases ---

    def test_llm_prompter_success_scenario(self):
        query_content = "This is a query for the successful scenario."
        self.create_test_input_query_file(content=query_content)
        mock_response_content_for_test = "Mocked success via CLI flag."
        query_id = "test_success_001"

        extra_args = [
            "--test_mock_api_outcome", "success",
            "--test_mock_api_content", mock_response_content_for_test,
            "-vv" # Set verbosity to DEBUG for more log output
        ]
        result = self.run_llm_prompter_subprocess(query_id, extra_args)

        print(f"\n--- Test: {self.id()} ---")
        print("LLM Prompter STDOUT:\n", result.stdout)
        print("LLM Prompter STDERR:\n", result.stderr)

        self.assertEqual(result.returncode, 0, f"Script should exit 0. STDERR: {result.stderr}")
        self.assertTrue(os.path.exists(self.output_response_file), "Response file was not created.")
        with open(self.output_response_file, "r", encoding='utf-8') as f:
            self.assertEqual(f.read(), mock_response_content_for_test)
        self.assertFalse(os.path.exists(self.output_error_file), "Error file should not exist on success.")

        # Verify JSON output to stdout
        self.assertIn("---LLM_RESPONSE_JSON_START---", result.stdout)
        self.assertIn("---LLM_RESPONSE_JSON_END---", result.stdout)
        json_str = result.stdout.split("---LLM_RESPONSE_JSON_START---")[1].split("---LLM_RESPONSE_JSON_END---")[0].strip()
        parsed_json = json.loads(json_str)
        self.assertEqual(parsed_json["choices"][0]["message"]["content"], mock_response_content_for_test)

        # Verify log messages
        # When mocking API outcome, the DEBUG log for API payload is NOT generated.
        # The "Log level: DEBUG" message is logged at INFO level by llm_prompter.py itself.
        self._assert_log_message_present(result.stderr, "INFO", f"LLM Prompter for ID '{query_id}' started. Log level: DEBUG")
        self._assert_log_message_present(result.stderr, "WARNING", "!!! RUNNING IN API MOCK MODE: success FOR QUERY")
        self._assert_log_message_present(result.stderr, "INFO", "LLM Prompter: Success. Wrote response to 'test_response_output.txt'.")


    def test_llm_prompter_api_failure_scenario(self):
        query_content = "This query will simulate an API failure (mocked)."
        self.create_test_input_query_file(content=query_content)
        query_id = "test_fail_002"

        extra_args = ["--test_mock_api_outcome", "api_returns_none"]
        result = self.run_llm_prompter_subprocess(query_id, extra_args)

        print(f"\n--- Test: {self.id()} ---")
        print("LLM Prompter STDOUT (API Failure):\n", result.stdout)
        print("LLM Prompter STDERR (API Failure):\n", result.stderr)

        self.assertNotEqual(result.returncode, 0, f"Script should exit non-zero. STDERR: {result.stderr}")
        self.assertTrue(os.path.exists(self.output_error_file), "Error file was not created on API failure.")
        self.assertFalse(os.path.exists(self.output_response_file))
        with open(self.output_error_file, "r", encoding='utf-8') as f:
            self.assertIn("LLM API call returned None or failed", f.read())
        # Assert on the generic error message from main() when mock API returns None
        self._assert_log_message_present(result.stderr, "ERROR", f"LLM call failed for '{os.path.basename(self.input_query_file)}'. No response data.")


    def test_llm_prompter_input_file_not_found(self):
        # Ensure the input file does NOT exist for this test
        if os.path.exists(self.input_query_file):
            os.remove(self.input_query_file)

        query_id = "test_nofile_003"
        result = self.run_llm_prompter_subprocess(query_id)

        print(f"\n--- Test: {self.id()} ---")
        print("LLM Prompter STDOUT (Input File Not Found):\n", result.stdout)
        print("LLM Prompter STDERR (Input File Not Found):\n", result.stderr)

        self.assertNotEqual(result.returncode, 0, "Script should exit non-zero if input file not found.")
        self.assertTrue(os.path.exists(self.output_error_file))
        with open(self.output_error_file, "r", encoding='utf-8') as f:
            error_content = f.read()
            self.assertIn("Input query file not found", error_content)
            self.assertIn(os.path.basename(self.input_query_file), error_content)
        self._assert_log_message_present(result.stderr, "ERROR", f"File error: Input query file not found: {self.input_query_file}")


    def test_llm_prompter_api_timeout(self):
        query_content = "This query will simulate an API timeout."
        self.create_test_input_query_file(content=query_content)
        query_id = "test_timeout_004"

        extra_args = ["--test_mock_api_outcome", "api_timeout"]
        result = self.run_llm_prompter_subprocess(query_id, extra_args)

        print(f"\n--- Test: {self.id()} ---")
        print("LLM Prompter STDOUT (API Timeout):\n", result.stdout)
        print("LLM Prompter STDERR (API Timeout):\n", result.stderr)

        self.assertNotEqual(result.returncode, 0, f"Script should exit non-zero. STDERR: {result.stderr}")
        self.assertTrue(os.path.exists(self.output_error_file))
        with open(self.output_error_file, "r", encoding='utf-8') as f:
            self.assertIn("LLM API call returned None or failed", f.read())
        # Assert on the generic error message from main() when mock API returns None
        self._assert_log_message_present(result.stderr, "ERROR", f"LLM call failed for '{os.path.basename(self.input_query_file)}'. No response data.")


    def test_llm_prompter_api_http_401_error(self):
        query_content = "This query will simulate an HTTP 401 error."
        self.create_test_input_query_file(content=query_content)
        query_id = "test_http401_005"

        extra_args = ["--test_mock_api_outcome", "api_http_401"]
        result = self.run_llm_prompter_subprocess(query_id, extra_args)

        print(f"\n--- Test: {self.id()} ---")
        print("LLM Prompter STDOUT (HTTP 401):\n", result.stdout)
        print("LLM Prompter STDERR (HTTP 401):\n", result.stderr)

        self.assertNotEqual(result.returncode, 0, f"Script should exit non-zero. STDERR: {result.stderr}")
        self.assertTrue(os.path.exists(self.output_error_file))
        with open(self.output_error_file, "r", encoding='utf-8') as f:
            self.assertIn("LLM API call returned None or failed", f.read())
        # Assert on the generic error message from main() when mock API returns None
        self._assert_log_message_present(result.stderr, "ERROR", f"LLM call failed for '{os.path.basename(self.input_query_file)}'. No response data.")
        # The specific HTTP error message is logged by call_openrouter_api, but it's skipped in mock mode.
        # So we don't assert it here.


    def test_llm_prompter_api_http_500_error(self):
        query_content = "This query will simulate an HTTP 500 error."
        self.create_test_input_query_file(content=query_content)
        query_id = "test_http500_006"

        extra_args = ["--test_mock_api_outcome", "api_http_500"]
        result = self.run_llm_prompter_subprocess(query_id, extra_args)

        print(f"\n--- Test: {self.id()} ---")
        print("LLM Prompter STDOUT (HTTP 500):\n", result.stdout)
        print("LLM Prompter STDERR (HTTP 500):\n", result.stderr)

        self.assertNotEqual(result.returncode, 0, f"Script should exit non-zero. STDERR: {result.stderr}")
        self.assertTrue(os.path.exists(self.output_error_file))
        with open(self.output_error_file, "r", encoding='utf-8') as f:
            self.assertIn("LLM API call returned None or failed", f.read())
        # Assert on the generic error message from main() when mock API returns None
        self._assert_log_message_present(result.stderr, "ERROR", f"LLM call failed for '{os.path.basename(self.input_query_file)}'. No response data.")
        # The specific HTTP error message is logged by call_openrouter_api, but it's skipped in mock mode.
        # So we don't assert it here.


    def test_llm_prompter_verbosity_info(self):
        query_content = "Test for INFO verbosity."
        self.create_test_input_query_file(content=query_content)
        mock_response_content_for_test = "Info verbosity test."
        query_id = "test_info_verbose_007"

        extra_args = [
            "--test_mock_api_outcome", "success",
            "--test_mock_api_content", mock_response_content_for_test,
            "-v" # Set verbosity to INFO
        ]
        result = self.run_llm_prompter_subprocess(query_id, extra_args)

        print(f"\n--- Test: {self.id()} ---")
        print("LLM Prompter STDOUT:\n", result.stdout)
        print("LLM Prompter STDERR:\n", result.stderr)

        self.assertEqual(result.returncode, 0)
        self._assert_log_message_present(result.stderr, "INFO", f"LLM Prompter for ID '{query_id}' started. Log level: INFO")
        self._assert_log_message_present(result.stderr, "WARNING", "!!! RUNNING IN API MOCK MODE: success FOR QUERY")
        self.assertNotIn("DEBUG", result.stderr) # Should not contain DEBUG messages


    def test_llm_prompter_verbosity_debug(self):
        query_content = "Test for DEBUG verbosity."
        self.create_test_input_query_file(content=query_content)
        mock_response_content_for_test = "Debug verbosity test."
        query_id = "test_debug_verbose_008"

        extra_args = [
            "--test_mock_api_outcome", "success",
            "--test_mock_api_content", mock_response_content_for_test,
            "-vv" # Set verbosity to DEBUG
        ]
        result = self.run_llm_prompter_subprocess(query_id, extra_args)

        print(f"\n--- Test: {self.id()} ---")
        print("LLM Prompter STDOUT:\n", result.stdout)
        print("LLM Prompter STDERR:\n", result.stderr)

        self.assertEqual(result.returncode, 0)
        self._assert_log_message_present(result.stderr, "INFO", f"LLM Prompter for ID '{query_id}' started. Log level: DEBUG")
        self._assert_log_message_present(result.stderr, "WARNING", "!!! RUNNING IN API MOCK MODE: success FOR QUERY")
        # No DEBUG messages are generated by llm_prompter.py itself when mocking API outcome.
        # The only DEBUG logs are inside call_openrouter_api, which is skipped.
        # So, we remove the assertion for "API Request Payload for Query" here.


    def test_llm_prompter_quiet_mode(self):
        query_content = "Test for quiet mode."
        self.create_test_input_query_file(content=query_content)
        mock_response_content_for_test = "Quiet mode test."
        query_id = "test_quiet_009"

        extra_args = [
            "--test_mock_api_outcome", "success",
            "--test_mock_api_content", mock_response_content_for_test,
            "--quiet" # Enable quiet mode
        ]
        result = self.run_llm_prompter_subprocess(query_id, extra_args)

        print(f"\n--- Test: {self.id()} ---")
        print("LLM Prompter STDOUT:\n", result.stdout)
        print("LLM Prompter STDERR:\n", result.stderr)

        self.assertEqual(result.returncode, 0)
        # In quiet mode, initial INFO logs are suppressed.
        # The first log we expect to see at WARNING level is the mock API message.
        self._assert_log_message_present(result.stderr, "WARNING", "!!! RUNNING IN API MOCK MODE: success FOR QUERY")
        self.assertNotIn("INFO", result.stderr) # Should not contain INFO messages
        self.assertNotIn("DEBUG", result.stderr) # Should not contain DEBUG messages
        # The "LLM Prompter: Success." message is INFO level and suppressed in quiet mode.


    def test_llm_prompter_interactive_test_mode(self):
        # In interactive mode, llm_prompter creates a default query file if it doesn't exist.
        # It creates these files in its own script directory (SRC_DIR_TEST).
        # We ensure they don't exist initially.
        interactive_input_file_in_src = os.path.join(SRC_DIR_TEST, "interactive_test_query.txt")
        interactive_response_file_in_src = os.path.join(SRC_DIR_TEST, "interactive_test_response.txt")
        interactive_error_file_in_src = os.path.join(SRC_DIR_TEST, "interactive_test_error.txt")
        
        # Clean up any remnants from previous runs in the src directory before the test
        # (This is also handled in tearDown, but good to ensure clean state for this specific test)
        for f_path in [interactive_input_file_in_src, interactive_response_file_in_src, interactive_error_file_in_src]:
            if os.path.exists(f_path):
                os.remove(f_path)

        mock_response_content_for_test = "Interactive test mode response."
        query_id = "interactive_test" # Default for interactive mode

        extra_args = [
            "--interactive_test_mode",
            "--test_mock_api_outcome", "success",
            "--test_mock_api_content", mock_response_content_for_test
        ]
        # Pass pass_io_paths=False to ensure llm_prompter uses its internal defaults
        # and creates files in its own directory (SRC_DIR_TEST)
        result = self.run_llm_prompter_subprocess(query_id, extra_args, pass_io_paths=False)

        print(f"\n--- Test: {self.id()} ---")
        print("LLM Prompter STDOUT:\n", result.stdout)
        print("LLM Prompter STDERR:\n", result.stderr)

        self.assertEqual(result.returncode, 0)

        # Verify default files are created in the SRC_DIR_TEST
        self.assertTrue(os.path.exists(interactive_input_file_in_src), f"Input file not created in {SRC_DIR_TEST}")
        self.assertTrue(os.path.exists(interactive_response_file_in_src), f"Response file not created in {SRC_DIR_TEST}")
        self.assertFalse(os.path.exists(interactive_error_file_in_src), f"Error file should not exist in {SRC_DIR_TEST} on success")

        with open(interactive_input_file_in_src, "r", encoding='utf-8') as f:
            self.assertIn("This is an interactive test query.", f.read())
        with open(interactive_response_file_in_src, "r", encoding='utf-8') as f:
            self.assertEqual(f.read(), mock_response_content_for_test)

        self._assert_log_message_present(result.stderr, "INFO", "Running in standalone interactive test mode with default file names.")
        self._assert_log_message_present(result.stderr, "INFO", "Created sample query file:")
        self._assert_log_message_present(result.stderr, "INFO", "LLM Prompter: Success. Wrote response to 'interactive_test_response.txt'.")


if __name__ == '__main__':
    unittest.main(verbosity=2)