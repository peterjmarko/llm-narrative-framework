# tests/test_run_anova.py

import unittest
from unittest.mock import patch, MagicMock
import os
import sys
import tempfile
import pandas as pd
import numpy as np
import pytest # For filterwarnings

# Adjust path to import run_anova
SCRIPT_DIR_TEST = os.path.dirname(os.path.abspath(__file__))
SRC_DIR_REAL_PROJECT = os.path.abspath(os.path.join(SCRIPT_DIR_TEST, '..', 'src'))

if SRC_DIR_REAL_PROJECT not in sys.path:
    sys.path.insert(0, SRC_DIR_REAL_PROJECT)

from run_anova import main as anova_main

class TestAnovaScript(unittest.TestCase):

    def setUp(self):
        self.test_dir_obj = tempfile.TemporaryDirectory(prefix="test_anova_")
        self.test_dir = self.test_dir_obj.name
        self.original_sys_argv = list(sys.argv)

        # Create dummy summary files for aggregation test
        # These are the files that run_anova.py will aggregate
        self.summary_dir_1 = os.path.join(self.test_dir, 'model_A_run_1')
        self.summary_dir_2 = os.path.join(self.test_dir, 'model_B_run_1')
        os.makedirs(self.summary_dir_1)
        os.makedirs(self.summary_dir_2)

        # Define data for two models, including all expected metrics
        # Values are arbitrary but designed to show some difference for ANOVA
        self.model_a_data_run1 = {
            'model': ['ModelA'],
            'mean_mrr': [0.8],
            'mean_top_1_acc': [0.75],
            'mean_top_3_acc': [0.9],
            'mean_effect_size_r': [0.6],
            'mwu_stouffer_z': [2.0],
            'mwu_stouffer_p': [0.02],
            'mwu_fisher_chi2': [5.0],
            'mwu_fisher_p': [0.05],
            'mean_normalized_mrr': [0.7],
            'mean_normalized_top_1_acc': [0.65],
            'mean_normalized_top_3_acc': [0.85],
            'run_directory': [self.summary_dir_1] # ADDED THIS LINE
        }
        self.model_b_data_run1 = {
            'model': ['ModelB'],
            'mean_mrr': [0.6],
            'mean_top_1_acc': [0.55],
            'mean_top_3_acc': [0.7],
            'mean_effect_size_r': [0.3],
            'mwu_stouffer_z': [0.5],
            'mwu_stouffer_p': [0.5],
            'mwu_fisher_chi2': [1.0],
            'mwu_fisher_p': [0.3],
            'mean_normalized_mrr': [0.4],
            'mean_normalized_top_1_acc': [0.35],
            'mean_normalized_top_3_acc': [0.55],
            'run_directory': [self.summary_dir_2] # ADDED THIS LINE
        }

        # Create dummy summary files for the first runs
        pd.DataFrame(self.model_a_data_run1).to_csv(os.path.join(self.summary_dir_1, 'final_summary_results.csv'), index=False)
        pd.DataFrame(self.model_b_data_run1).to_csv(os.path.join(self.summary_dir_2, 'final_summary_results.csv'), index=False)

        # Add another run for ModelA and ModelB to have N=2 for each
        self.summary_dir_3 = os.path.join(self.test_dir, 'model_A_run_2')
        self.summary_dir_4 = os.path.join(self.test_dir, 'model_B_run_2')
        os.makedirs(self.summary_dir_3)
        os.makedirs(self.summary_dir_4)

        self.model_a_data_run2 = {
            'model': ['ModelA'],
            'mean_mrr': [0.85],
            'mean_top_1_acc': [0.80],
            'mean_top_3_acc': [0.92],
            'mean_effect_size_r': [0.65],
            'mwu_stouffer_z': [2.1],
            'mwu_stouffer_p': [0.01],
            'mwu_fisher_chi2': [5.5],
            'mwu_fisher_p': [0.04],
            'mean_normalized_mrr': [0.72],
            'mean_normalized_top_1_acc': [0.68],
            'mean_normalized_top_3_acc': [0.88],
            'run_directory': [self.summary_dir_3] # ADDED THIS LINE
        }
        self.model_b_data_run2 = {
            'model': ['ModelB'],
            'mean_mrr': [0.65],
            'mean_top_1_acc': [0.60],
            'mean_top_3_acc': [0.75],
            'mean_effect_size_r': [0.35],
            'mwu_stouffer_z': [0.6],
            'mwu_stouffer_p': [0.45],
            'mwu_fisher_chi2': [1.2],
            'mwu_fisher_p': [0.25],
            'mean_normalized_mrr': [0.42],
            'mean_normalized_top_1_acc': [0.38],
            'mean_normalized_top_3_acc': [0.58],
            'run_directory': [self.summary_dir_4] # ADDED THIS LINE
        }
        pd.DataFrame(self.model_a_data_run2).to_csv(os.path.join(self.summary_dir_3, 'final_summary_results.csv'), index=False)
        pd.DataFrame(self.model_b_data_run2).to_csv(os.path.join(self.summary_dir_4, 'final_summary_results.csv'), index=False)


    def tearDown(self):
        """Clean up the temporary directory."""
        self.test_dir_obj.cleanup()

    @patch('run_anova.plt.savefig') # Mock savefig to prevent plot windows
    def test_aggregation_and_analysis_from_directory(self, mock_savefig):
        """Test the full pipeline: aggregate from a directory and then analyze."""
        # Arrange: Arguments to run aggregation on the base test directory
        cli_args = ['run_anova.py', self.test_dir]

        with patch.object(sys, 'argv', cli_args):
            anova_main()

        # --- Assert Aggregation ---
        master_csv_path = os.path.join(self.test_dir, 'MASTER_ANOVA_DATASET.csv')
        self.assertTrue(os.path.exists(master_csv_path))

        # Check that the aggregated file has the correct content (4 data rows)
        df_master = pd.read_csv(master_csv_path)
        self.assertEqual(len(df_master), 4)
        self.assertFalse(df_master['model'].isnull().any(), "Junk rows were not filtered out")

        # --- Assert Analysis ---
        # Corrected path to include 'anova' subdirectory
        log_file_path = os.path.join(self.test_dir, 'anova', 'MASTER_ANOVA_DATASET_analysis_log.txt')
        self.assertTrue(os.path.exists(log_file_path))

        # Check that plots were generated (by checking if savefig was called)
        self.assertTrue(mock_savefig.called)
        # We now have 7 metrics being analyzed (mrr, top_1, top_3, effect_size_r, normalized_mrr, normalized_top_1, normalized_top_3)
        # Each metric generates at least one plot (boxplot) and potentially a diagnostic plot (qqplot).
        # So, 7 boxplots + 7 qqplots = 14 plots.
        self.assertGreaterEqual(mock_savefig.call_count, 14)

        # Check that the log file contains key analysis sections for all expected metrics
        with open(log_file_path, 'r') as f:
            log_content = f.read()
        self.assertIn("ANALYSIS FOR METRIC: 'mean_mrr'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_top_1_acc'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_top_3_acc'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_effect_size_r'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_normalized_mrr'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_normalized_top_1_acc'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_normalized_top_3_acc'", log_content)
        self.assertIn("Descriptive Statistics by Model", log_content)
        self.assertIn("Formatted ANOVA Summary", log_content)
        self.assertIn("Performance Tiers", log_content)

    @patch('run_anova.plt.savefig')
    def test_analysis_from_pre_aggregated_file(self, mock_savefig):
        """Test the analysis logic when providing a direct file path."""
        # Arrange: Manually create the master CSV
        master_csv_path = os.path.join(self.test_dir, 'pre_made_data.csv')
        
        # Combine all 4 runs into a single DataFrame for the pre-aggregated file
        combined_data = pd.concat([
            pd.DataFrame(self.model_a_data_run1),
            pd.DataFrame(self.model_b_data_run1),
            pd.DataFrame(self.model_a_data_run2),
            pd.DataFrame(self.model_b_data_run2)
        ])
        combined_data.to_csv(master_csv_path, index=False)

        cli_args = ['run_anova.py', master_csv_path]

        with patch.object(sys, 'argv', cli_args):
            anova_main()

        # Assert Analysis
        # Corrected path to include 'anova' subdirectory
        log_file_path = os.path.join(self.test_dir, 'anova', 'pre_made_data_analysis_log.txt')
        self.assertTrue(os.path.exists(log_file_path))

        # Check that plots were generated
        self.assertTrue(mock_savefig.called)
        # Expect 14 plots (7 metrics * 2 plot types)
        self.assertGreaterEqual(mock_savefig.call_count, 14)

        # Check that the log file contains key analysis sections for all expected metrics
        with open(log_file_path, 'r') as f:
            log_content = f.read()
        self.assertIn("ANALYSIS FOR METRIC: 'mean_mrr'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_top_1_acc'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_top_3_acc'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_effect_size_r'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_normalized_mrr'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_normalized_top_1_acc'", log_content)
        self.assertIn("ANALYSIS FOR METRIC: 'mean_normalized_top_3_acc'", log_content)
        self.assertIn("Formatted ANOVA Summary", log_content)
        self.assertIn("Performance Tiers", log_content)

    def test_no_files_found(self):
        """Test that the script handles the case where no summary files are found."""
        # Arrange: Use a new, empty subdirectory
        empty_dir = os.path.join(self.test_dir, 'empty')
        os.makedirs(empty_dir)
        cli_args = ['run_anova.py', empty_dir]

        with patch.object(sys, 'argv', cli_args), \
             patch('logging.error') as mock_log_error:
            anova_main()
        
        # Assert that the correct error message was logged
        mock_log_error.assert_called_with("ERROR: No 'final_summary_results.csv' files found. Cannot proceed.")

if __name__ == '__main__':
    unittest.main(verbosity=2)