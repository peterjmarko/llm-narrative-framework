================================================================================
### OVERALL META-ANALYSIS RESULTS ###
================================================================================
Number of Valid Responses: <n_valid_responses>

--- Response Parsing Summary ---
llm_response_001.txt: SUCCESS - parsed matrix
llm_response_002.txt: SUCCESS - parsed matrix
llm_response_003.txt: REJECTED - parsing failed
llm_response_004.txt: SUCCESS - parsed matrix
llm_response_005.txt: SUCCESS - parsed matrix
llm_response_006.txt: SUCCESS - parsed matrix

1. Overall Ranking Performance (MRR) (vs Chance=<chance_val>):
   Mean: <mean_val>, Wilcoxon p-value: p = <p_value>

2. Overall Ranking Performance (Top-1 Accuracy) (vs Chance=<chance_val>):
   Mean: <mean_val>, Wilcoxon p-value: p = <p_value>

3. Overall Ranking Performance (Top-3 Accuracy) (vs Chance=<chance_val>):
   Mean: <mean_val>, Wilcoxon p-value: p = <p_value>

4. Bias and Other Metrics:
   Top-1 Prediction Bias (StdDev of choice counts): <std_dev>
   Mean Score Difference (Correct - Incorrect): <score_diff>


<<<METRICS_JSON_START>>>
{
    "mean_mrr": <value>,
    "mrr_p": <value>,
    "mean_top_1_acc": <value>,
    "top_1_acc_p": <value>,
    "mean_top_3_acc": <value>,
    "top_3_acc_p": <value>,
    "mean_mrr_lift": <value>,
    "mean_top_1_acc_lift": <value>,
    "mean_top_3_acc_lift": <value>,
    "mean_rank_of_correct_id": <value>,
    "rank_of_correct_id_p": <value>,
    "top1_pred_bias_std": <value>,
    "true_false_score_diff": <value>,
    "bias_slope": <value>,
    "bias_intercept": <value>,
    "bias_r_value": <value>,
    "bias_p_value": <value>,
    "bias_std_err": <value>,
    "n_valid_responses": <value>,
    "positional_bias_metrics": {
        "top1_pred_bias_std": <value>,
        "true_false_score_diff": <value>
    }
}
<<<METRICS_JSON_END>>>