<!-- 
!! DO NOT EDIT THIS FILE DIRECTLY !!
This file is generated from README.template.md by the build_docs.py script.
Any changes made here will be overwritten.
-->

# LLM Personality Matching Experiment Framework

This project provides a fully automated and reproducible pipeline for testing a Large Language Model's (LLM) ability to solve a "who's who" personality matching task. It handles everything from data preparation and query generation to LLM interaction, response parsing, and final statistical analysis. The framework is ideal for researchers investigating LLM capabilities and developers who need a robust system for automated, large-scale experimentation.

## Research Question
At its core, this project investigates whether a Large Language Model (LLM) can solve a complex matching task: given a set of sanitized, narrative personality descriptions (derived from birth data) and a corresponding set of general biographical profiles, can the LLM correctly pair them at a rate significantly greater than chance?

This study introduces a novel methodological twist to probe the limits of LLM pattern recognition. The link between the narrative descriptions and the biographical profiles is a faint, systematic signal generated by a deterministic, non-scientific system (an astrology program). This transforms the experiment into a rigorous test of an LLM's ability to detect subtle, rule-based patterns within a noisy, high-dimensional dataset. The central question is not about the validity of the generating system, but about the capability of the AI to find a signal in its output.

## A Note on Stimulus Generation and Experimental Design
The experiment is built upon a custom database of 5,000 famous historical individuals, for whom accurate and verified birth data (date, time, place) was meticulously collected. This population was chosen for two reasons:

*   **Signal Integrity**: Accurate birth data ensures the deterministic generation of consistent personality narratives.
*   **Task Feasibility**: The public prominence of these individuals makes it plausible that LLMs have encountered their biographical information during training, making the matching task tractable.

To create a uniquely challenging test, we employed a two-step process to generate textual stimuli for each trial:

1.  **Generation**: A commercial astrology program was used as a "black box" function to deterministically map an individual's birth data to a narrative description. The generation was intentionally constrained to a foundational subset of the algorithm's rules (e.g., categorical placements and dominances), creating a signal of limited complexity.
2.  **Sanitization**: An LLM was then used to programmatically rewrite these descriptions, removing all explicit references to astrology, planets, or other esoteric terms.

The result is a clean dataset of personality profiles where the connection to the individual's biographical profile is systematic but non-obvious.

**Crucially, this study does not seek to validate astrology.** Instead, it treats the generating program as an arbitrary, complex algorithm. The scientific objective is to determine whether an LLM, a third-party pattern-recognition system, can detect the subtle statistical regularities produced by this algorithm and use them to perform a successful matching task. The findings speak to the profound capabilities of LLMs to find signal in noisy, high-dimensional data, regardless of the source's theoretical basis.

## Key Features

-   **Automated Batch Execution**: The `experiment_manager.py` script, driven by a simple PowerShell wrapper, manages entire experimental batches. It can run hundreds of replications, intelligently skipping completed ones to resume interrupted runs, and provides real-time ETA updates.
-   **Powerful Reprocessing Engine**: The manager's `--reprocess` mode allows for re-running the data processing and analysis stages on existing results without repeating expensive LLM calls. Combined with a `--depth` parameter, a single command can update the analysis for an entire nested study.
-   **Guaranteed Reproducibility**: On every new run, the `config.ini` file is automatically archived in the run's output directory, permanently linking the results to the exact parameters that generated them.
-   **Standardized, Comprehensive Reporting**: Each replication produces a `replication_report.txt` file containing run parameters, status, a human-readable statistical summary, and a machine-parsable JSON block with all key metrics. This format is identical for new runs and reprocessed runs.
-   **Hierarchical Analysis & Aggregation**: The `compile_study_results.py` script performs a bottom-up aggregation of all data, generating level-aware summary files (`REPLICATION_results.csv`, `EXPERIMENT_results.csv`, and a master `STUDY_results.csv`) for a fully auditable research archive.
-   **Resilient and Idempotent Operations**: The pipeline is designed for resilience. The `replication_log_manager.py` script can `rebuild` experiment logs from scratch, and its `finalize` command is idempotent, ensuring that data summaries are always correct even after interruptions.
-   **Streamlined ANOVA Workflow**: The final statistical analysis is a simple two-step process. `compile_study_results.py` prepares a master dataset, which `analyze_study_results.py` then automatically analyzes to generate tables and publication-quality plots using user-friendly display names defined in `config.ini`.

## Visual Architecture

The project's architecture can be understood through three different views: the code architecture, the data flow, and the experimental logic.

### 1. Code Architecture Diagrams
The project's functionality is divided into three main workflows, each initiated by a PowerShell script.

#### Codebase Architecture
This diagram provides a comprehensive map of the entire Python codebase, showing how scripts execute (solid lines) or import (dotted lines) one another.

{{diagram:docs/diagrams/codebase_architecture.mmd | scale=2.5 | width=110%}}

### Workflow 1: Run an Experiment

This is the primary workflow for generating new experimental data. The PowerShell entry point (`run_experiment.ps1`) calls the Python batch controller (`experiment_manager.py`), which in turn executes a loop of single replications using `orchestrate_replication.py`.

Each replication executes the four core pipeline stages in sequence:
1.  **Query Generation**: `orchestrate_replication.py` first calls `build_llm_queries.py`. This script samples personalities from the master database (without replacement) and orchestrates the `query_generator.py` worker to create the prompt, a ground-truth mapping, and an audit manifest for each trial.
2.  **LLM Interaction**: Next, `run_llm_sessions.py` is called. It manages sending each generated query to the LLM via the `llm_prompter.py` worker and saves the responses.
3.  **Response Processing**: `process_llm_responses.py` parses the raw text responses from the LLM into a structured table of similarity scores.
4.  **Performance Analysis**: Finally, `analyze_llm_performance.py` performs the final statistical analysis for the replication. It calculates key metrics (MRR, Top-1 Accuracy, effect size), uses non-parametric tests to assess significance against chance, and embeds a comprehensive JSON summary of the results into the final report.

{{diagram:docs/diagrams/architecture_workflow_1_run_experiment.mmd | scale=2.5 | width=105%}}

### Workflow 2: Process a Study

This workflow is used after an experiment is complete to aggregate results from all replications and perform statistical analysis.

{{diagram:docs/diagrams/architecture_workflow_2_process_study.mmd | scale=2.5 | width=105%}}

### Workflow 3: Migrate Old Experiment Data

This utility workflow helps bring older experimental data (generated before `config.ini.archived` was standard) into compliance with modern analysis scripts.

{{diagram:docs/diagrams/architecture_workflow_4_migrate_data.mmd | scale=2.5 | width=50%}}

### Data Flow Diagram

This diagram shows how data artifacts (files) are created and transformed by the pipeline scripts.

{{diagram:docs/diagrams/architecture_data_flow.mmd | scale=2.5 | width=55%}}

### Experimental Logic Flowchart

This diagram illustrates the scientific methodology for a single replication run.

{{diagram:docs/diagrams/architecture_experimental_logic.mmd | scale=2.5 | width=50%}}

## Experimental Hierarchy

The project's experiments are organized in a logical hierarchy:

-   **Study**: The highest-level grouping, representing a major research question (e.g., "Performance on Random vs. Correct Mappings").
-   **Experiment**: A complete set of runs for a single condition within a study (e.g., "Gemini 2.0 Flash with k=10 Subjects").
-   **Replication**: A single, complete run of an experiment, typically repeated 30 times for statistical power.
-   **Trial**: An individual matching task performed within a replication, typically repeated 100 times.

## Directory Structure

This logical hierarchy is reflected in the physical layout of the repository:

{{diagram:docs/diagrams/directory_structure.txt | scale=2.5 | width=110%}}

## Setup and Installation

This project uses **PDM** for dependency and environment management.

1.  **Install PDM (One-Time Setup)**:
    If you don't have PDM, install it once with pip. It's best to run this from a terminal *outside* of any virtual environment.
    ```bash
    pip install --user pdm
    ```
    > **Note:** If `pdm` is not found in a new terminal, use `python -m pdm` instead.

2.  **Install Project Environment & Dependencies**:
    From the project's root directory, run the main PDM installation command. The `-G dev` flag installs all packages, including the development tools needed to run the test suite.
    ```bash
    pdm install -G dev
    ```
    This command creates a local `.venv` folder and installs all necessary packages into it.

3.  **Configure API Key**:
    *   Create a file named `.env` in the project root.
    *   Add your API key: `OPENROUTER_API_KEY=sk-or-your-key`.

To run any project command, such as the test suite, prefix it with `pdm run`:
```bash
pdm run test
```

> **For Developers:** If you intend to contribute to the project or encounter issues with the simple setup, please see the **[Developer Setup Guide in CONTRIBUTING.md](CONTRIBUTING.md#getting-started-development-environment-setup)** for more detailed instructions and troubleshooting.

## Configuration (`config.ini`)

The `config.ini` file is the central hub for defining all parameters for your experiments. The pipeline automatically archives this file with the results for guaranteed reproducibility.

### Display Name Settings

-   **`[ModelDisplayNames]`**: Maps a model's API identifier to a friendly, human-readable name for plots and reports (e.g., `meta-llama/llama-3-70b-instruct = Llama 3 70B Instruct`).
-   **`[FactorDisplayNames]`**: Maps factor names to plot labels (e.g., `mapping_strategy = Mapping Strategy`).
-   **`[MetricDisplayNames]`**: Maps metric names to plot titles (e.g., `mean_mrr = Mean Reciprocal Rank (MRR)`).

### Experiment Settings (`[Study]`)

-   **`num_replications`**: The number of times the experiment will be repeated (e.g., `30`).
-   **`mapping_strategy`**: A key experimental variable. Can be `correct` or `random`.

### LLM Settings (`[LLM]`)

-   **`model_name`**: The API identifier for the LLM to be tested (e.g., from a provider like OpenRouter: `mistralai/mistral-7b-instruct`).

#### Model Selection Philosophy and Future Work
The selection of models for this study was guided by a balance of performance, cost, speed, and technical compatibility with the automated framework. Several top-tier models were not included for one of the following reasons:
-   **Prohibitive Cost**: Models like `o1 pro`, `GPT 4.5 Preview`, and `Claude 4 Opus` were excluded as a single experiment (requiring ~3,000 queries) was financially infeasible.
-   **Technical Incompatibility**: Models like `Gemini 2.5 Pro` lacked a "non-thinking" mode, making the automated parsing of a structured response table overly challenging.
-   **Excessive Runtime**: A number of large models, including `Qwen3 235B` and `Llama 3.1 Nemotron Ultra 253B`, were excluded as a full experimental run would take longer than 20 hours.

A follow-up study is planned to evaluate other powerful, medium-cost models as API costs decrease and technical features evolve. Candidates include: `Grok 3`, `Claude 4 Sonnet`, `Claude 3.7 Sonnet`, `GPT-4o`, `o3`, `GPT-4.1`, `Mistral Large 2`, `Gemini 1.5 Pro`, and various `o1`/`o3`/`o4` mini-variants.

### Analysis Settings (`[Analysis]`)

-   **`min_valid_response_threshold`**: Minimum average number of valid responses (`n_valid_responses`) for an experiment to be included in the final analysis. Set to `0` to disable.

## Standard Workflow: The 2-Phase Process

The project is orchestrated by PowerShell wrapper scripts that provide a clear, two-phase workflow. First, you generate data by running experiments. Second, you process the data from one or more experiments to produce a final analysis.

### Phase 1: Running an Experiment (`run_experiment.ps1`)

This PowerShell script is the primary entry point for executing a full experimental batch based on the settings in `config.ini`. It provides a clean, high-level summary of progress by default.

**To run a standard experiment:**
This will execute the full batch of replications defined in `config.ini` and place the results in a new, timestamped experiment directory.
```powershell
.\run_experiment.ps1
```

**Common Examples:**

```powershell
# Organize results into a specific directory for a study, with embedded notes
.\run_experiment.ps1 -TargetDirectory "output/reports/Study_6" -Notes "First run with Llama 3"

# Resume an interrupted batch by running only replications 15 through 30
.\run_experiment.ps1 -TargetDirectory "output/reports/Study_6" -StartRep 15 -EndRep 30

# Run a full batch with detailed, real-time logging for debugging purposes
.\run_experiment.ps1 -Verbose
```
> **Note:** The main wrapper scripts use PowerShell. While PowerShell is pre-installed on Windows and available for macOS and Linux, the core scientific logic is contained in cross-platform Python scripts.

### Phase 2: Processing a Study (`process_study.ps1`)

After running one or more experiments, this script aggregates all their results and performs the final statistical analysis for the entire study.

**To run the analysis:**
Point the script at the top-level directory containing all relevant experiment folders. It will provide a clean, high-level summary of its progress.

```powershell
.\process_study.ps1 -StudyDirectory "output/reports"
```
For detailed, real-time logs, add the `-Verbose` switch.

**Final Artifacts:**
The script generates two key outputs:
1.  A master `STUDY_results.csv` file in your study directory, containing the aggregated data from all experiments.
2.  A new `anova/` subdirectory containing the final analysis:
    *   `STUDY_analysis_log.txt`: A comprehensive text report of the statistical findings.
    *   `boxplots/`: Publication-quality plots visualizing the results.
    *   `diagnostics/`: Q-Q plots for checking statistical assumptions.

## Standardized Output

The pipeline now generates a consistent, standardized `replication_report.txt` for every run, whether it's a new experiment or a reprocessed one. This ensures that all output is easily comparable and machine-parsable.

### Replication Report Format

Each report contains a clear header, the base query used, a human-readable analysis summary, and a machine-readable JSON block with all calculated metrics.

{{diagram:docs/diagrams/replication_report_format.txt}}

**Date Handling by Mode:**
-   **Normal Mode**: The report title is `REPLICATION RUN REPORT` and the `Date` field shows the time of the original run.
-   **`--reprocess` Mode**: The report title is `REPLICATION RUN REPORT (YYYY-MM-DD HH:MM:SS)` with the reprocessing timestamp. The `Date` field continues to show the time of the **original** run for clear traceability.

### Study Analysis Log Format

The final analysis script (`analyze_study_results.py`) produces a comprehensive log file detailing the full statistical analysis of the entire study. The report is structured by metric, with each section providing descriptive statistics, the ANOVA summary, post-hoc results (if applicable), and performance groupings.

{{diagram:docs/diagrams/analysis_log_format.txt}}

## Migrating Old Experiment Data (`migrate_old_experiment.ps1`)

To upgrade older experiment directories to be compatible with the current analysis pipeline, a dedicated PowerShell script automates the entire migration process.

**What it does:**
The `migrate_old_experiment.ps1` script orchestrates a four-step data migration:
1.  **Patches Configs**: Runs `patch_old_experiment.py` to create and archive a `config.ini` file for each run.
2.  **Rebuilds Reports**: Executes `rebuild_reports.py` to regenerate all replication reports into the modern, standardized format.
3.  **Cleans Artifacts**: Programmatically deletes legacy files like old summary CSVs, `.corrupted` report backups, and obsolete `analysis_inputs` directories.
4.  **Final Reprocess**: Triggers `experiment_manager.py` in reprocess mode to generate clean, modern summary files based on the newly rebuilt reports.

**How to use it:**
Execute the script and point it at the top-level directory of the old experiment you want to migrate using the `-TargetDirectory` parameter.

```powershell
# Migrate the experiment data located in the "6_Study_4" directory
.\migrate_old_experiment.ps1 -TargetDirectory "output/reports/6_Study_4"
```
The script will run all four steps in sequence, providing clear progress updates for each stage.

---

## Maintenance and Utility Scripts

*   **`experiment_manager.py`**:
    *   The primary controller for an entire experiment, functioning as a state machine.
    *   It continuously verifies the experiment's state and automatically takes the correct action (`NEW`, `REPAIR`, or `REPROCESS`) until the experiment is `COMPLETE`.
    *   This self-healing design makes the pipeline resilient to interruptions.
    *   Usage: `python src/experiment_manager.py path/to/experiment`

*   **`orchestrate_replication.py`**:
    *   The engine for a **single** experimental run. Called in a loop by `experiment_manager.py`.
    *   Handles the sequential execution of the four pipeline stages: `build_llm_queries`, `run_llm_sessions`, `process_llm_responses`, and `analyze_llm_performance`.
    *   Generates the final, comprehensive `replication_report.txt` for the run.

*   **`run_bias_analysis.py`**:
    *   Calculates specific metrics for positional bias (e.g., standard deviation of top-1 choice counts) from a run's raw scores.
    *   It injects these metrics into the `replication_report.txt`'s JSON block for later aggregation and analysis.
    *   This is called automatically by `experiment_manager.py` during the `NEW` and `REPROCESS` modes.

*   **`compile_study_results.py`**:
    *   The core script for hierarchical data aggregation. Recursively scans a study directory, performing a bottom-up summary.
    *   Generates level-aware summary files: `REPLICATION_results.csv`, `EXPERIMENT_results.csv`, and `STUDY_results.csv`.
    *   Usage: `python src/compile_study_results.py path/to/study`

*   **`analyze_study_results.py`**:
    *   Performs the final, comprehensive statistical analysis (Two-Way ANOVA, post-hoc tests) on a study's master CSV file.
    *   Automatically generates diagnostic plots (Q-Q) and intelligently selects the appropriate post-hoc test (Tukey HSD vs. Games-Howell).
    *   Uses a sophisticated clique-finding algorithm to determine robust performance tiers.
    *   Produces a detailed analysis log and publication-quality boxplots using display names from `config.ini`.
    *   Usage: `python src/analyze_study_results.py path/to/study`

*   **`replication_log_manager.py`**:
    *   Core utility for managing the human-readable `batch_run_log.csv`.
    *   `start`: Creates a new, empty log with a header.
    *   `rebuild`: Recreates the log from scratch by parsing all existing `replication_report.txt` files in a directory. This is the primary method for ensuring log integrity.
    *   `finalize`: Appends a final summary section to the log after `compile_study_results.py` has run.

*   **`rebuild_reports.py`**:
    *   A powerful utility to regenerate all `replication_report.txt` files from the ground-truth `analysis_inputs` data. Useful for applying fixes to the processing or analysis stages across an entire study.
    *   Usage: `python src/rebuild_reports.py path/to/study`

*   **`patch_old_experiment.py`**:
    *   **Utility for historical data.** A batch controller that recursively scans an experiment directory and calls `restore_config.py` on every `run_*` subfolder. This ensures all legacy runs have a proper `config.ini.archived` file for modern reprocessing.
    *   Usage: `python src/patch_old_experiment.py "path/to/old/experiments"`

*   **`verify_experiment_completeness.py`**:
    *   A standalone diagnostic tool for manually auditing an experiment directory. It compares file counts at each stage to verify data integrity. Its core logic is also used internally by `experiment_manager.py` for its automated state verification.

*   **`rebuild_reports.py`**:
    *   A powerful utility to regenerate all `replication_report.txt` files from the ground-truth `analysis_inputs` data. Useful for applying fixes to the processing or analysis stages across an entire study.
    *   Usage: `python src/rebuild_reports.py path/to/study`

*   **`retry_llm_sessions.py`**:
    *   Finds and retries failed API calls in parallel. After retrying, it automatically re-runs the entire downstream analysis pipeline to ensure reports are fully updated.

*   **`verify_experiment_completeness.py`**:
    *   A diagnostic tool that audits an experiment directory, comparing file counts at each stage to verify data integrity and completeness.

---

## Testing

The project includes a comprehensive test suite managed by PDM, covering both the Python source code and the PowerShell orchestration scripts.

-   **To run the Python unit tests (Pytest):**
    ````bash
    pdm run test
    ````

-   **To run the PowerShell script tests:**
    -   For the study processor (`process_study.ps1`):
        ````bash
        pdm run test-ps-stu
        ````
    -   For the data migration script (`migrate_old_experiment.ps1`):
        ````bash
        pdm run test-ps-mig
        ````

