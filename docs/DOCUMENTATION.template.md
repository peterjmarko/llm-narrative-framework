<!--
================================================================================
!!! DO NOT EDIT 'docs/DOCUMENTATION.md' DIRECTLY !!!

This file is automatically generated from the template:
/docs/DOCUMENTATION.template.md

To make changes, please edit the template file and then run 'pdm run build-docs'.
================================================================================
-->

# A Resilient Framework for Large-Scale LLM Experimentation

This project provides a fully automated, resilient, and reproducible framework for conducting large-scale LLM experiments. It offers an end-to-end pipeline that manages the entire experimental lifecycle, from data preparation and query generation to LLM interaction, response parsing, hierarchical data aggregation, and final statistical analysis.

While the framework is modular and adaptable, its primary application here is to test an LLM's ability to solve a complex "who's who" personality matching task. It is ideal for researchers investigating LLM capabilities and developers who need a robust system for managing and analyzing automated experiments.

## Research Question
At its core, this project investigates whether a Large Language Model (LLM) can solve a complex matching task: given a set of sanitized, narrative personality descriptions (derived from birth data) and a corresponding set of general biographical profiles, can the LLM correctly pair them at a rate significantly greater than chance?

This study introduces a novel methodological twist to probe the limits of LLM pattern recognition. The link between the narrative descriptions and the biographical profiles is a faint, systematic signal generated by a deterministic, esoteric system (an astrology program). This transforms the experiment into a rigorous test of an LLM's ability to detect subtle, rule-based patterns within a noisy, high-dimensional dataset. The central question is not about the validity of the generating system, but about the capability of the AI to find a signal in its output.

## A Note on Stimulus Generation and Experimental Design
The experiment is built upon a custom database of 5,000 famous historical individuals, for whom accurate and verified birth data (date, time, place) was meticulously collected. This population was chosen for two reasons:

*   **Signal Integrity**: Accurate birth data ensures the deterministic generation of consistent personality narratives.
*   **Task Feasibility**: The public prominence of these individuals makes it plausible that LLMs have encountered their biographical information during training, making the matching task tractable.

To create a uniquely challenging test, we employed a two-step process to generate textual stimuli for each trial:

1.  **Generation**: A commercial astrology program was used as a "black box" function to deterministically map an individual's birth data to a narrative description. The generation was intentionally constrained to a foundational subset of the algorithm's rules (e.g., categorical placements and dominances), creating a signal of limited complexity.
2.  **Sanitization**: An LLM was then used to programmatically rewrite these descriptions, removing all explicit references to astrology, planets, or other esoteric terms.

The result is a clean dataset of personality profiles where the connection to the individual's biographical profile is systematic but non-obvious.

**Crucially, this study does not seek to validate astrology.** Instead, it treats the generating program as an arbitrary, complex algorithm. The scientific objective is to determine whether an LLM, a third-party pattern-recognition system, can detect the subtle statistical regularities produced by this algorithm and use them to perform a successful matching task. The findings speak to the profound capabilities of LLMs to find signal in noisy, high-dimensional data, regardless of the source's theoretical basis.

## Key Features

-   **Automated Batch Execution**: The `experiment_manager.py` script, driven by a simple PowerShell wrapper, manages entire experimental batches. It can run hundreds of replications, intelligently skipping completed ones to resume interrupted runs, and provides real-time progress updates, including a detailed spinner showing individual trial timers and overall replication batch ETA.
-   **Powerful Reprocessing Engine**: The manager's `--reprocess` mode allows for re-running the data processing and analysis stages on existing results without repeating expensive LLM calls. This makes it easy to apply analysis updates or bug fixes across an entire experiment.
-   **Guaranteed Reproducibility**: On every new run, the `config.ini` file is automatically archived in the run's output directory, permanently linking the results to the exact parameters that generated them.
-   **Standardized, Comprehensive Reporting**: Each replication produces a `replication_report.txt` file containing run parameters, status, a human-readable statistical summary, and a machine-parsable JSON block with all key metrics. This format is identical for new runs and reprocessed runs.
-   **Hierarchical Analysis & Aggregation**: The `experiment_aggregator.py` script performs a bottom-up aggregation of all data, generating level-aware summary files (`REPLICATION_results.csv`, `EXPERIMENT_results.csv`, and a master `STUDY_results.csv`) for a fully auditable research archive.
-   **Resilient and Idempotent Operations**: The pipeline is designed for resilience. The `replication_log_manager.py` script can `rebuild` experiment logs from scratch, and its `finalize` command is idempotent, ensuring that data summaries are always correct even after interruptions.
-   **Enhanced Console Readability**: All console outputs for file paths, commands, and key statuses are now formatted with consistent newlines and indentation, greatly improving log clarity and user experience during long runs.
-   **Streamlined ANOVA Workflow**: The final statistical analysis is a simple two-step process. `experiment_aggregator.py` prepares a master dataset, which `study_analysis.py` then automatically analyzes to generate tables and publication-quality plots using user-friendly display names defined in `config.ini`.

## Visual Architecture

The project's architecture can be understood through four different views: the code architecture, the workflows, the data flow, and the experimental logic.

### Code Architecture Diagram
The codebase can be divided into the following components:

1.  **Main User Entry Points**: The PowerShell scripts that users directly execute to initiate various workflows (e.g., `run_experiment.ps1`, `analyze_study.ps1`).

2.  **Experiment Lifecycle**: Python scripts primarily responsible for conducting a single experiment, including query generation, LLM interaction, response processing, and performance analysis.

3.  **Data Migration Processes**: Utility scripts used specifically for updating and cleaning older experiment data to ensure compatibility with the current analysis pipeline.

4.  **Study-Level Analysis**: Python scripts that aggregate results from multiple experiments and perform comprehensive statistical analysis for an entire study.

5.  **Shared Utilities**: Helper scripts and modules, like `config_loader.py`, that provide common functionality and are imported by multiple other scripts across different workflows.

{{grouped_figure:docs/diagrams/codebase_architecture.mmd | scale=2.5 | width=100% | caption=Codebase Architecture: A comprehensive map of the entire Python codebase, showing how scripts execute (solid lines) or import (dotted lines) one another.}}

### Workflow Diagrams
The project's functionality is organized into six primary workflows, each initiated by a dedicated PowerShell script (Main User Entry Points):

1.  **Run an Experiment**: The most common action; used for starting a new experiment or resuming/healing an interrupted one.

2.  **Audit an Experiment**: Provides a read-only, detailed completeness report for an experiment, acting as the primary diagnostic tool.

3.  **Update an Experiment**: A specific action for existing experiments, used to reapply analysis or bug fixes without re-running expensive LLM calls.

4.  **Migrate Old Experiment Data**: A utility workflow designed to bring older, legacy experimental data into compliance with the modern pipeline.

5.  **Audit a Study**: Provides a consolidated, read-only audit of all experiments in a study to verify their readiness for final analysis.

6.  **Update a Study**: A batch operation that automatically updates all out-of-date experiments within a study.

7.  **Analyze a Study**: The highest-level analytical tool, used after a study is validated to aggregate results and perform comprehensive statistical analysis.

#### Workflow 1: Run an Experiment

This is the primary workflow for generating new experimental data or for resuming/repairing an interrupted experiment. The PowerShell entry point (`run_experiment.ps1`) calls the Python batch controller (`experiment_manager.py`), which functions as a state machine. It repeatedly audits the experiment's state and, if incomplete, calls `orchestrate_replication.py` to run or repair the necessary replications. This self-healing loop continues until the experiment is complete, at which point the manager performs a final aggregation.

The `orchestrate_replication.py` script executes the full pipeline for a single run, which is broken into six distinct stages:

1.  **Build Queries**: Generates all necessary query files and trial manifests.
2.  **Run LLM Sessions**: Interacts with the LLM API in parallel to get responses.
3.  **Process LLM Responses**: Parses the raw text responses from the LLM into structured score files.
4.  **Analyze LLM Performance**: A unified two-part process that first calculates core performance metrics and then injects diagnostic bias metrics.
5.  **Generate Final Report**: Assembles the final `replication_report.txt` from the analysis results and captured logs.
6.  **Create Replication Summary**: Creates the final `REPLICATION_results.csv`, marking the run as valid.

{{grouped_figure:docs/diagrams/architecture_workflow_1_run_experiment.mmd | scale=2.5 | width=65% | caption=Workflow 1: Run an Experiment, showing the main control loop and the internal replication pipeline.}}


#### Workflow 2: Audit an Experiment

This workflow provides a read-only, detailed completeness report for an experiment without performing any modifications. The `audit_experiment.ps1` wrapper calls `experiment_manager.py` with the `--verify-only` flag. The full audit report, including subprocess outputs, is also saved to `audit_log.txt` within the audited directory.

{{grouped_figure:docs/diagrams/architecture_workflow_2_audit_experiment.mmd | scale=2.5 | width=100% | caption=Workflow 2: Audit an Experiment. Provides a read-only, detailed completeness report for an experiment.}}

##### Interpreting the Audit Report
The audit script is the primary diagnostic tool for identifying issues in a failed or incomplete experiment. It outputs a summary table with a high-level status for each replication run. The `Details` column provides granular error codes that pinpoint the exact problem. The final `Audit Result` and `Recommendation` suggest the next steps, if any.

**VALIDATED**  
The run is complete and all checks passed. No action is needed.

**INVALID_NAME**  
The run directory name is malformed and does not match the required `run_*_sbj-NN_trl-NNN` pattern. The folder must be renamed or repaired via a `run_experiment.ps1` initiated process.

**CONFIG_ISSUE**  
The run's `config.ini.archived` is missing, corrupted, or lacks required keys. This typically indicates a legacy experiment requiring **migration**.

**QUERY_ISSUE**  
There is a problem with the fundamental input files needed for an LLM session, such as missing query files or trial manifests. This requires **repair** by running `run_experiment.ps1` (or `migrate_experiment.ps1` if legacy).

**RESPONSE_ISSUE**  
The query files are intact, but one or more corresponding LLM response files are missing. This is typically caused by an interrupted run and requires **repair** by running `run_experiment.ps1` (or `migrate_experiment.ps1` if legacy).

**ANALYSIS_ISSUE**  
All core data files (queries, responses) are present, but there is a problem with derivative artifacts like analysis files, summary CSVs, or the final report. This requires an **update** (reprocessing) and can be fixed by running `update_experiment.ps1`.

The `Details` string provides specific error flags, such as `MANIFESTS_INCOMPLETE`, `QUERY_RESPONSE_INDEX_MISMATCH`, or `REPORT_INCOMPLETE_METRICS`, which help diagnose the root cause quickly.

In addition to the per-replication table, the audit provides an `Overall Summary` that includes the `Experiment Aggregation Status`. This checks for the presence and completeness of top-level summary files (`EXPERIMENT_results.csv`, `batch_run_log.csv`), confirming whether the last aggregation step for the experiment was successfully completed.


#### Workflow 3: Update an Experiment

This workflow allows you to re-run the data processing and analysis stages on an existing experiment without repeating expensive LLM calls. The `update_experiment.ps1` wrapper calls `experiment_manager.py` with the `--reprocess` flag.
It first performs an audit. If the experiment has analysis errors, it proceeds to update it. If the experiment is already `VALIDATED`, it will prompt for user confirmation before forcing a full reprocessing.
The reprocessing is a two-step action: first, `orchestrate_replication.py --reprocess` is called on each individual run to regenerate its `replication_report.txt`; second, `experiment_aggregator.py` is called to perform a full re-aggregation, ensuring all top-level summary files are brought up to date.

{{grouped_figure:docs/diagrams/architecture_workflow_3_update_experiment.mmd | scale=2.5 | width=100% | caption=Workflow 3: Update an Experiment. Re-runs the data processing and analysis stages on an existing experiment.}}


#### Workflow 4: Migrate Old Experiment Data

This utility workflow provides a safe, non-destructive process to transform older experimental data into the current pipeline's format, leaving the original data untouched. The `migrate_experiment.ps1` script orchestrates a clear, four-step process:

1.  **Audit Source**: A read-only audit is performed on the source directory to assess its state and recommend migration.
2.  **Copy Data**: After user confirmation, the script creates a clean, timestamped copy of the source experiment in the `output/migrated_experiments/` directory.
3.  **Transform Copy**: The script calls `experiment_manager.py --migrate` on the new copy. The manager then automates the internal transformation, including patching configurations, reprocessing all runs, and running its self-healing loop until the new experiment copy is valid.
4.  **Final Validation**: The wrapper script runs a final read-only audit on the newly migrated experiment, providing explicit confirmation that the process was successful.

{{grouped_figure:docs/diagrams/architecture_workflow_4_migrate_data.mmd | scale=2.5 | width=100% | caption=Workflow 4: Migrate Old Experiment Data, a safe, non-destructive process for upgrading legacy data.}}


#### Workflow 5: Audit a Study

This workflow provides a read-only, consolidated completeness report for all experiments in a study. The `audit_study.ps1` wrapper iterates through each experiment folder and calls `experiment_manager.py` with the `--verify-only` flag (running quietly by default). It then compiles the results into a summary table for the console and a comprehensive `study_audit_log.txt` file in the study directory. This workflow is the primary diagnostic tool for assessing overall study readiness.

{{grouped_figure:docs/diagrams/architecture_workflow_5_audit_study.mmd | scale=2.5 | width=100% | caption=Workflow 5: Audit a Study. Consolidated completeness report for all experiments in a study.}}


#### Workflow 6: Analyze a Study

This workflow is used after all experiments are complete to aggregate results and perform statistical analysis for the study. The `analyze_study.ps1` wrapper calls `experiment_aggregator.py` and then `study_analysis.py`.

{{grouped_figure:docs/diagrams/architecture_workflow_6_analyze_study.mmd | scale=2.5 | width=100% | caption=Workflow 6: Analyze a Study. Aggregates results of all experiments and performs statistical analysis for the study.}}


#### Workflow 7: Update a Study

This workflow provides a convenient batch operation to update all out-of-date experiments within a study directory. It first runs a full study audit to identify which experiments need updating. If any require more serious intervention (like repair or migration), it halts. Otherwise, it prompts the user for confirmation and then systematically calls `update_experiment.ps1` on each experiment that needs it.

{{grouped_figure:docs/diagrams/architecture_workflow_7_update_study.mmd | scale=2.5 | width=100% | caption=Workflow 7: Update a Study. A batch operation to update all out-of-date experiments in a study.}}

{{pagebreak}}
### Data Flow Diagram

This diagram shows how data artifacts (files) are created and transformed by the pipeline scripts. It traces the flow from initial inputs like `config.ini` and the personalities database, through intermediate query and response files, to the final aggregated results and analysis plots.

{{grouped_figure:docs/diagrams/architecture_data_flow.mmd | scale=2.5 | width=75% | caption=Data Flow Diagram: Creation and transformation of data artifacts (files) by the pipeline scripts.}}

### Experimental Logic Flowchart

This diagram illustrates the scientific methodology for a single replication run.

{{grouped_figure:docs/diagrams/architecture_experimental_logic.mmd | scale=2.5 | width=65% | caption=Experimental Logic Flowchart: Scientific methodology for a single replication run.}}

## Experimental Hierarchy

The project's experiments are organized in a logical hierarchy:

-   **Study**: The highest-level grouping, representing a major research question (e.g., "Performance on Random vs. Correct Mappings").
-   **Experiment**: A complete set of runs for a single condition within a study (e.g., "Gemini 2.0 Flash with k=10 Subjects").
-   **Replication**: A single, complete run of an experiment, typically repeated 30 times for statistical power.
-   **Trial**: An individual matching task performed within a replication, typically repeated 100 times.

## Directory Structure

This logical hierarchy is reflected in the physical layout of the repository:

{{diagram:docs/diagrams/directory_structure.txt | scale=2.5 | width=90%}}

## Setup and Installation

This project uses **PDM** for dependency and environment management.

1.  **Install PDM (One-Time Setup)**:
    If you don't have PDM, install it once with pip. It's best to run this from a terminal *outside* of any virtual environment.
    ```bash
    pip install --user pdm
    ```
    > **Note:** If `pdm` is not found in a new terminal, use `python -m pdm` instead.

2.  **Install Project Environment & Dependencies**:
    From the project's root directory, run the main PDM installation command. The `-G dev` flag installs all packages, including the development tools needed to run the test suite.
    ```bash
    pdm install -G dev
    ```
    This command creates a local `.venv` folder and installs all necessary packages into it.

3.  **Configure API Key**:
    *   Create a file named `.env` in the project root.
    *   Add your API key from OpenRouter. The key will start with `sk-or-`.
        `OPENROUTER_API_KEY=your-actual-api-key`

To run any project command, such as the test suite, prefix it with `pdm run`:
```bash
pdm run test
```

> **For Developers:** If you intend to contribute to the project or encounter issues with the simple setup, please see the **[Developer Setup Guide in CONTRIBUTING.md](CONTRIBUTING.md#getting-started-development-environment-setup)** for more detailed instructions and troubleshooting.

## Configuration (`config.ini`)

The `config.ini` file is the central hub for defining all parameters for your experiments. The pipeline automatically archives this file with the results for guaranteed reproducibility.

### Display Name Settings

-   **`[ModelDisplayNames]`**: Maps a model's API identifier to a friendly, human-readable name for plots and reports (e.g., `meta-llama/llama-3-70b-instruct = Llama 3 70B Instruct`).
-   **`[FactorDisplayNames]`**: Maps factor names to plot labels (e.g., `mapping_strategy = Mapping Strategy`).
-   **`[MetricDisplayNames]`**: Maps metric names to plot titles (e.g., `mean_mrr = Mean Reciprocal Rank (MRR)`).

### Experiment Settings (`[Study]`)

-   **`num_replications`**: The number of times the experiment will be repeated (e.g., `30`).
-   **`mapping_strategy`**: A key experimental variable. Can be `correct` or `random`.

### LLM Settings (`[LLM]`)

-   **`model_name`**: The API identifier for the LLM to be tested (e.g., from a provider like OpenRouter: `google/gemini-flash-1.5`).
-   **`temperature`**: Controls the randomness of the model's output. A lower value (e.g., `0.2`) makes the output more deterministic and focused, while a higher value (e.g., `0.8`) encourages more creative responses.
-   **`max_tokens`**: The maximum number of tokens the model is allowed to generate in its response. This acts as a safeguard against overly long or runaway outputs.
-   **`max_parallel_sessions`**: The maximum number of concurrent API calls to make. Increasing this can speed up experiments but may encounter API rate limits.

#### Model Selection Philosophy and Future Work
The selection of models for this study was guided by a balance of performance, cost, speed, and technical compatibility with the automated framework. Several top-tier models were not included for one of the following reasons:

-   **Prohibitive Cost**: Models like `o1 pro`, `GPT 4.5 Preview`, and `Claude 4 Opus` were excluded as a single experiment (requiring ~3,000 queries) was financially infeasible.

-   **Technical Incompatibility**: Models like `Gemini 2.5 Pro` lacked a "non-thinking" mode, making the automated parsing of a structured response table overly challenging.

-   **Excessive Runtime**: A number of large models, including `Qwen3 235B` and `Llama 3.1 Nemotron Ultra 253B`, were excluded as a full experimental run would take longer than 20 hours.

A follow-up study is planned to evaluate other powerful, medium-cost models as API costs decrease and technical features evolve. Candidates include: `Grok 3`, `Grok 4`, `Claude 4 Sonnet`, `Claude 3.7 Sonnet`, `GPT-4o`, `o3`, `GPT-4.1`, `Mistral Large 2`, `Gemini 1.5 Pro`, and various `o1`/`o3`/`o4` mini-variants.

### Analysis Settings (`[Analysis]`)

-   **`min_valid_response_threshold`**: Minimum average number of valid responses (`n_valid_responses`) for an experiment to be included in the final analysis. Set to `0` to disable.

## Choosing the Right Workflow: Separation of Concerns

The framework is designed around three primary user actions, each handled by a dedicated script. This separation of concerns ensures that each workflow is simple, predictable, and safe. Use the following diagram and descriptions to choose the correct tool for your task.

{{grouped_figure:docs/diagrams/decision_tree_workflow.mmd | scale=2.5 | width=100% | caption=Choosing the Right Workflow: Separation of Concerns.}}

-   **`run_experiment.ps1` (Data Generation & Repair)**: This is your primary tool. Use it to start a new experiment from scratch or to resume/repair an interrupted one. Its sole focus is to ensure the raw data (queries and LLM responses) is complete according to your `config.ini`.

-   **`update_experiment.ps1` (Analysis Reprocessing)**: Use this *after* your experiment's raw data is complete. It regenerates all reports and summaries from the existing raw data. It is the correct tool for applying analysis bug fixes or adding new metrics without re-running expensive LLM calls.

-   **`migrate_experiment.ps1` (Structural Transformation)**: This is a special utility for upgrading experiments created with older versions of the framework. It safely copies the legacy data and transforms the copy into the modern, compatible format.

## Core Workflows

The project is orchestrated by several PowerShell wrapper scripts that handle distinct user workflows, from running new experiments to analyzing and migrating data.

### Running an Experiment (`run_experiment.ps1`)

This PowerShell script is the primary entry point for executing a full experimental batch based on the settings in `config.ini`. It provides a clean, high-level summary of progress by default.

**To run a standard experiment:**
This will execute the full batch of replications defined in `config.ini` and place the results in a new, timestamped experiment directory.
```powershell
.\run_experiment.ps1
```

**Common Examples:**

```powershell
# Run a new experiment (will be saved in output/new_experiments/experiment_DATE_TIME/)
.\run_experiment.ps1

# Resume an interrupted experiment by running only replications 15 through 30
# (Replace 'experiment_...' with the actual folder name of your experiment)
.\run_experiment.ps1 -TargetDirectory "output/new_experiments/experiment_20250712_081954" -StartRep 15 -EndRep 30

# Run a full experiment with detailed, real-time logging for debugging purposes
.\run_experiment.ps1 -Verbose
```
> **Note:** The main wrapper scripts use PowerShell. While PowerShell is pre-installed on Windows and available for macOS and Linux, the core scientific logic is contained in cross-platform Python scripts.

### Updating an Experiment (`update_experiment.ps1`)

This script allows you to re-run the data processing and analysis stages on an existing, complete experiment without repeating expensive LLM calls. It's the ideal tool for applying analysis updates, bug fixes, or new metrics to your results.

**To update an experiment:**
Point the script at the directory of the experiment you wish to reprocess.

```powershell
# Update an experiment in the Study_2/Experiment_3/ directory
.\update_experiment.ps1 -TargetDirectory "output/reports/Study_2/Experiment_3"
```
This calls `experiment_manager.py` with the `--reprocess` flag. This action first regenerates the primary report for each replication and then performs a full re-aggregation, ensuring all summary files are consistent and up-to-date. For detailed, real-time logs, add the `-Verbose` switch.

### Migrating Old Experiment Data (`migrate_experiment.ps1`)

This script provides a safe, non-destructive workflow to upgrade older, legacy experiment directories to be compatible with the current analysis pipeline. The original data is always preserved.

### Auditing a Study (`audit_study.ps1`)

This script audits all experiments within a study directory to verify their readiness for final analysis. It iterates through each experiment, runs a quiet audit, and presents a consolidated summary report table. This is the recommended first step before running `analyze_study.ps1`.

**To audit a study:**
Point the script at the top-level directory containing all relevant experiment folders.
```powershell
# Example: Audit all experiments located in the "My_First_Study" directory
.\audit_study.ps1 -StudyDirectory "output/studies/My_First_Study"
```
The summary table will show the status (`VALIDATED`, `NEEDS UPDATE`, etc.) and the recommended action for each experiment. For a detailed, verbose output that shows the full audit for each experiment, add the `-Verbose` switch.

**What it does:**
The `migrate_experiment.ps1` script automates a copy-then-migrate process:

1.  **Copy**: It takes a source directory and copies it to a new, timestamped folder inside `output/migrated_experiments/`.
2.  **Migrate**: It then calls `experiment_manager.py --migrate` on this new directory. The manager orchestrates the internal upgrade steps:
    *   **Patching Configs**: Creating `config.ini.archived` files from old reports.
    *   **Rebuilding Reports**: Regenerating all reports into the modern format.
    *   **Finalizing**: Generating clean, modern summary files for the migrated experiment.

This approach leaves the original data completely untouched.

**How to use it:**
Point the script at the source directory of the legacy experiment. The script will automatically create a timestamped destination folder.

```powershell
# Migrate data from "Legacy_Experiment_1", saving the result to a new timestamped folder.
.\migrate_experiment.ps1 -SourceDirectory "output/legacy/Legacy_Experiment_1"
```

### Analyzing a Study (`analyze_study.ps1`)

After running one or more experiments, this script aggregates all their results and performs the final statistical analysis for the entire study. This is the final step for a study.

**Important:** Before running the aggregation and analysis, this script first automatically performs a full study audit (by calling `audit_study.ps1`). If any experiment is found to be incomplete, corrupt, or out-of-date, the process will halt with a detailed error report. This ensures that analysis is only performed on a complete and validated set of data.

For organizational purposes, one would typically move all experiment folders belonging to a single study into a common directory (e.g., `output/studies/My_First_Study/`).

**To run the analysis:**
Point the script at the top-level directory containing all relevant experiment folders. It will provide a clean, high-level summary of its progress.

```powershell
# Example: Analyze all experiments located in the "My_First_Study" directory
.\analyze_study.ps1 -StudyDirectory "output/studies/My_First_Study"
```
For detailed, real-time logs, add the `-Verbose` switch.

**Final Artifacts:**
The script generates two key outputs:
1.  A master `STUDY_results.csv` file in your study directory, containing the aggregated data from all experiments.
2.  A new `anova/` subdirectory containing the final analysis:
    *   `STUDY_analysis_log.txt`: A comprehensive text report of the statistical findings.
    *   `boxplots/`: Publication-quality plots visualizing the results.
    *   `diagnostics/`: Q-Q plots for checking statistical assumptions.

## Standardized Output

The pipeline generates a consistent, standardized `replication_report.txt` for every run, whether it's a new, an updated (reprocessed), or migrated experiment. This ensures that all output is easily comparable and machine-parsable.

### Replication Report Format

Each report contains a clear header, the base query used, a human-readable analysis summary, and a machine-readable JSON block with all calculated metrics.

{{diagram:docs/diagrams/replication_report_format.txt}}

**Date Handling by Mode:**
-   **Normal Mode**: The report title is `REPLICATION RUN REPORT` and the `Date` field shows the time of the original run.
-   **`--reprocess` Mode**: The report title is `REPLICATION RUN REPORT (YYYY-MM-DD HH:MM:SS)` with the reprocessing timestamp. The `Date` field continues to show the time of the **original** run for clear traceability.

### Study Analysis Log Format

The final analysis script (`study_analysis.py`) produces a comprehensive log file detailing the full statistical analysis of the entire study. The report is structured by metric, with each section providing descriptive statistics, the ANOVA summary, post-hoc results (if applicable), and performance groupings.

{{diagram:docs/diagrams/analysis_log_format.txt}}

---

## User Entry Points

*   **`run_experiment.ps1`**: The primary entry point to run a new experiment or resume/repair an interrupted one.

*   **`audit_experiment.ps1`**: Provides a read-only, detailed completeness report for a specified experiment, acting as the primary diagnostic tool for a *single experiment*.

*   **`audit_study.ps1`**: Audits all experiments within a study directory and provides a consolidated report to check for final analysis readiness.

*   **`update_experiment.ps1`**: Re-runs the data processing and analysis stages on a *single experiment*, ideal for applying analysis updates or bug fixes.

*   **`update_study.ps1`**: A powerful batch script that audits an entire study and calls `update_experiment.ps1` on all experiments that require an update.

*   **`migrate_experiment.ps1`**: Safely migrates a legacy experiment by first copying it to a new timestamped directory and then upgrading the copy to be compatible with the current pipeline.

*   **`analyze_study.ps1`**: Aggregates all results within a study and performs the final statistical analysis (e.g., ANOVA).

## Main Orchestrator

*   **`experiment_manager.py`**: The top-level controller for an entire experiment, functioning as a state machine. It verifies the experiment's state and automatically takes the correct action (`NEW`, `REPAIR`, `REPROCESS`, or `MIGRATE`) until the experiment is complete. It can also be forced to reprocess an entire experiment via the `--reprocess` command-line flag.

## Replication Pipeline Scripts

*   **`orchestrate_replication.py`**: The self-contained engine for a **single** replication. It manages the entire lifecycle, including a consolidated 6-stage pipeline: query generation, parallel LLM sessions, response processing, a comprehensive two-part analysis (performance and bias), report generation, and final summary creation. It can also reprocess or repair a run.

*   **`build_llm_queries.py`**: **Stage 1.** Called by the orchestrator. Samples personalities and calls the `query_generator.py` worker to create all files needed for the trials.

*   **`run_llm_sessions.py`**: **Stage 2.** Called in a parallel loop by `orchestrate_replication.py`. It manages the LLM API calls for all trials within the replication.

*   **`process_llm_responses.py`**: **Stage 3.** Parses the raw text responses from the LLM into structured score and mapping files.

*   **`analyze_llm_performance.py`**: **Stage 4 (Part 1).** Performs the primary statistical analysis for the replication. It calculates core performance metrics (MRR, Top-K accuracy) and generates the initial `replication_metrics.json` file, along with a human-readable summary that is captured by the orchestrator for the final report.

*   **`run_bias_analysis.py`**: **Stage 4 (Part 2).** Called by the orchestrator immediately after the primary analysis. It calculates diagnostic metrics for positional bias, reads the `replication_metrics.json` file, injects its new metrics, and overwrites the file to complete the analysis stage.

## Study-Level Analysis Scripts

*   **`experiment_aggregator.py`**: Recursively scans a study directory, performing a bottom-up aggregation and generating level-aware summary files (`REPLICATION_results.csv`, `EXPERIMENT_results.csv`, and `STUDY_results.csv`).

*   **`study_analysis.py`**: Performs the final statistical analysis (Two-Way ANOVA, post-hoc tests) on a study's master CSV file and produces a detailed analysis log and publication-quality boxplots.

## Worker Scripts

*   **`query_generator.py`**: Takes a small subset of personalities (`k`) and deterministically creates a single, formatted LLM query prompt, a ground-truth mapping, and an audit manifest for that trial.

*   **`llm_prompter.py`**: Handles the direct API call to the specified LLM for a single query, manages retry logic, and returns the raw text response from the API.

## Maintenance & Utility Scripts

*   **`replication_log_manager.py`**: Manages the `batch_run_log.csv` file, with commands to rebuild it from scratch or finalize it with a summary.

*   **`rebuild_reports.py`**: A powerful utility to regenerate all `replication_report.txt` files from ground-truth data, useful for applying analysis fixes across an entire study.

*   **`patch_old_experiment.py`**: A utility for historical data that calls `restore_config.py` to create `config.ini.archived` files for legacy runs.

*   **`restore_config.py`**: Reverse-engineers a `config.ini.archived` file by parsing a legacy `replication_report.txt`.

*   **`config_loader.py`**: A central, non-executable utility module for loading settings from `config.ini` and providing them to all other scripts.

*   **`docx_postprocessor.py`**: A helper utility called by `build_docs.py` to reliably insert page breaks into `.docx` files after they are generated by Pandoc.

---

## Testing

The project includes a comprehensive test suite managed by PDM scripts, which provides shortcuts for running tests with and without code coverage.

### Running the Test Suite

-   **To run all tests (Python and PowerShell) at once:**
    ```bash
    pdm run test
    ```
-   **To run only the PowerShell script tests:**
    ```bash
    pdm run test-ps-all
    ```
    You can also test individual PowerShell scripts (e.g., `pdm run test-ps-exp`, `pdm run test-ps-stu`).

### Code Coverage

The test suite is configured for detailed code coverage analysis using the `coverage` package.

-   **To run all tests and view a coverage report in the console:**
    ```bash
    pdm run cov
    ```
-   **To generate a detailed HTML coverage report (saved to `htmlcov/`):**
    ```bash
    pdm run cov-html
    ```
    Open `htmlcov/index.html` in your browser to explore the report.