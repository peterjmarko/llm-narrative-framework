# A Framework for Testing Complex Narrative Systems

This document is the **Framework Manual** for the project. It provides a comprehensive technical guide to the framework, which is designed for the resilient and reproducible testing of large-scale LLM experiments with complex narrative systems. It offers a fully automated, end-to-end pipeline that manages the entire experimental workflow, from data preparation and query generation to LLM interaction, response parsing, hierarchical data aggregation, and final statistical analysis.

This manual is intended for developers, contributors, and researchers who wish to understand the system's architecture or use the framework for several types of scientific validation, including **direct, methodological, and conceptual replication, as well as for new research**.

{{toc}}

{{grouped_figure:docs/diagrams/arch_project_overview.mmd | scale=2.5 | width=100% | caption=Project Architecture: A high-level overview of the project's main functional components and their relationships.}}

## Research Question
At its core, this project investigates whether a Large Language Model (LLM) can solve a complex matching task: given a set of sanitized, narrative personality descriptions (derived from birth data) and a corresponding set of general biographical profiles, can the LLM correctly pair them at a rate significantly greater than chance?

This study introduces a novel methodological twist to probe the limits of LLM pattern recognition, as illustrated below. The link between the narrative descriptions and the biographical profiles is a faint, systematic signal generated by a deterministic, esoteric system (an astrology program). This transforms the experiment into a rigorous test of an LLM's ability to detect subtle, rule-based patterns within a noisy, high-dimensional dataset. The central question is not about the validity of the generating system, but about the capability of the AI to find a signal in its output.

{{grouped_figure:docs/diagrams/logic_matching_task.mmd | scale=2.5 | width=60% | caption=The Core Research Question: An LLM-driven matching task to detect a non-random signal.}}

## A Note on Stimulus Generation and Experimental Design
The experiment is built upon a custom database of 4,954 famous historical individuals, for whom accurate and verified birth data (date, time, place) was meticulously collected. This population was chosen for two reasons:

*   **Signal Integrity**: Accurate birth data ensures the deterministic generation of consistent personality narratives.
*   **Task Feasibility**: The public prominence of these individuals makes it plausible that LLMs have encountered their biographical information during training, making the matching task tractable.

To create a uniquely challenging test, we employed a multi-step, deterministic process to generate the textual stimuli, organized into four main stages:

1.  **Data Sourcing & Candidate Qualification:** A `Raw Subject Database` of 10,619 famous individuals was derived from the Astro-Databank (ADB) and subjected to a rigorous, deterministic filtering pass to create a high-quality cohort of 7,234 "eligible candidates."
2.  **LLM-based Candidate Selection:** This eligible cohort was then processed by two LLM-driven scripts (`generate_eminence_scores.py`, `generate_ocean_scores.py`) that scored every subject for historical eminence and personality diversity. The final selection is then performed by a third script (`select_final_candidates.py`), which applies a data-driven cutoff based on score variance to determine the final subject pool.
3.  **Manual Data Processing:** The final subject list was processed by a commercial astrology program (Solar Fire) to calculate the precise celestial positions for each person.
4.  **Profile Generation:** A custom Python script (`generate_personalities_db.py`) then processed this celestial data. Using a `Neutralized Component Library` of pre-written sentences, the script deterministically assembled a unique personality narrative for each individual based on a validated **personality assembly algorithm**.
    - The script's algorithm loads its core logic‚Äîpoint weights and balance thresholds‚Äîfrom external data files (`point_weights.csv`, `balance_thresholds.csv`). It uses these to calculate weighted scores for various astrological factors (elements, modes, quadrants, etc.) and classify them as 'strong' or 'weak' based on their prominence.
    - These classifications, along with simple placements, serve as keys to look up and combine the corresponding neutralized descriptions from the component library. **The personality assembly algorithm has been rigorously validated against a ground-truth dataset generated by the source Solar Fire software, ensuring its output is bit-for-bit identical.**

{{grouped_figure:docs/diagrams/flow_stimulus_generation.mmd | scale=2.5 | width=40% | caption=The four-stage process for generating the experimental stimuli.}}

The result is a clean dataset of personality profiles where the connection to an individual's biographical profile is systematic but non-obvious.

**Crucially, this study does not seek to validate astrology.** Instead, it treats the generating program as an arbitrary, complex algorithm. The scientific objective is to determine whether an LLM, a third-party pattern-recognition system, can detect the subtle statistical regularities produced by this algorithm and use them to perform a successful matching task. The findings speak to the profound capabilities of LLMs to find signal in noisy, high-dimensional data, regardless of the source's theoretical basis.

## Data Preparation Pipeline

The data preparation pipeline is a fully automated, multi-stage workflow. It begins with data extraction from the live Astro-Databank website and concludes with the generation of the final `personalities_db.txt` file used in the experiments.

**Replication Paths:** Researchers can approach this project in several ways to validate and extend the findings, as illustrated below.

{{grouped_figure:docs/diagrams/flow_replication_paths.mmd | scale=2.5 | width=100% | caption=The Three Research Replication Paths.}}

1.  **Direct Replication (Computational Reproducibility):** To verify that the framework produces the exact findings reported in the article, clone this repository and use the static data and randomization seeds as provided. This is a bit-for-bit validation of the original results.

2.  **Methodological Replication (Testing Robustness):** To validate that the findings are robust and not an artifact of a specific dataset or randomization seed, use the framework as-is but vary the inputs. This can be done in two ways:
    *   **With a New Dataset:** Run the full `prepare_data.ps1` pipeline to generate a fresh dataset from the live Astro-Databank. This tests the statistical robustness of the method on a new sample.
    *   **With New Randomization:** Use the provided static dataset but specify a different set of randomization seeds in `config.ini`. This validates the stability of the results across different random permutations.

3.  **Conceptual Replication (Extending the Research):** To test the underlying scientific concepts with an improved or modified method, researchers can alter the framework itself. This could involve using a different LLM, modifying the analysis scripts, or changing other core parameters to conduct new research built upon this study's foundation.

#### The Automated Workflow

The automated data preparation pipeline is orchestrated by a single, intelligent PowerShell script: `prepare_data.ps1`. This is the recommended method for both initial runs and for resuming interrupted processes.

**Execution:**
```powershell
# Run the entire data preparation pipeline interactively
.\prepare_data.ps1

# Force a full re-run, deleting all existing data
.\prepare_data.ps1 -Force

# Get a read-only status report of the pipeline's progress
.\prepare_data.ps1 -ReportOnly
```
> **Warning on Using `-Force`**: The `-Force` flag triggers a full, destructive re-run of the entire pipeline. It backs up and deletes all existing data, re-downloads the full raw dataset, and re-runs all expensive LLM scoring steps. This process is very time-consuming and will incur API costs.
> **Note on Learning the Pipeline:** A step-by-step "guided tour" of this workflow is available as part of the project's testing harness. This is an excellent way for new users to learn how the pipeline works. See the **[üß™ Testing Guide (TESTING_GUIDE.md)](../TESTING_GUIDE.md)** for details on running the Layer 3 Interactive Mode.

The script is fully resumable. It automatically detects which steps have already been completed and picks up from the first missing data artifact, ensuring a smooth and efficient workflow.

#### Individual Script Details

The `prepare_data.ps1` script orchestrates a sequence of individual Python scripts followed by a manual processing step. The following is a detailed breakdown of this underlying workflow.

##### Stage 1: Data Sourcing

This stage uses `fetch_adb_data.py` to create the initial raw dataset by querying the live Astro-Databank with a specific set of pre-filters. It performs two crucial transformations at the source:
-   **Identifier Standardization:** It replaces the unstable, temporary record number (`ARN`) with a file-specific sequential `Index`, and renames the permanent Astro-Databank ID (`ADBNo`) to `idADB`.
-   **Timezone Calculation:** It immediately processes the raw timezone code from the API into the final `ZoneAbbr` and `ZoneTimeOffset` values required by downstream software.

The query only includes subjects who meet all of the following criteria:
-   **High-Quality Birth Data:** The record must have a Rodden Rating of 'A' or 'AA', indicating the birth time is from a reliable source.
-   **Deceased Individuals:** Inclusion in the **Personal > Death** category.
-   **Eminence:** Inclusion in the **Notable > Famous > Top 5% of Profession** category.

##### Stage 2: Candidate Qualification

This stage performs a rigorous, deterministic filtering pass on the raw data to create a high-quality cohort of "eligible candidates."

1.  **Link Finding (`find_wikipedia_links.py`):** This script takes the raw ADB export and finds the best-guess English Wikipedia URL for each subject. It scrapes the ADB page and uses a Wikipedia search as a fallback. The output is an intermediate file, `adb_wiki_links.csv`.
2.  **Page Validation (`validate_wikipedia_pages.py`):** This script takes the list of found links and performs an intensive content validation on each page. It handles redirects, resolves disambiguation pages, validates the subject's name, and confirms their death date. The final output is the detailed `adb_validation_report.csv`.
3.  **Final Filtering (`select_eligible_candidates.py`):** This script integrates the raw data with the Wikipedia validation report and applies the following additional criteria in order:

| Criteria | Rule | Purpose |
| :--- | :--- | :--- |
| **Wikipedia Validation** | `Status` must be `OK` | Ensures a valid English Wikipedia page was found where the name and death date were confirmed. |
| **Entry Type** | `EntryType` must be `Person` | Filters out non-person records (e.g., events, research entries). |
| **Birth Year Range** | Must be between 1900-1999 | Controls for cohort-specific confounds by ensuring a homogenous historical period. |
| **Hemisphere** | `Latitude` must contain 'N' | Controls for the potential confound of a zodiacal shift for Southern Hemisphere births. |
| **Valid Time Format** | Birth time must be present and `HH:MM` | Ensures data is complete for precise calculations. |
| **Deduplication** | Unique Name + Birth Date | Removes duplicate entries from the source database. |
    
    The final output of this stage is the `adb_eligible_candidates.txt` file.

##### Stage 3: LLM-based Candidate Selection (Optional)

This stage is a second, optional filtering pass that uses LLMs to score the "eligible candidates" to determine the final, smaller subject pool. This entire stage can be skipped by setting `bypass_candidate_selection = true` in `config.ini`.

1.  **Eminence Scoring (`generate_eminence_scores.py`):** Processes the eligible candidates list to generate a calibrated eminence score for each, producing a rank-ordered list that now includes `BirthYear`.
2.  **OCEAN Scoring (`generate_ocean_scores.py`):** A fully automated, resilient script that generates OCEAN personality scores for every subject in the eminence-ranked list.
3.  **Final Selection & Cutoff (`select_final_candidates.py`):** Performs the final filtering and selection. It takes the complete OCEAN scores list and applies a sophisticated, data-driven algorithm to determine the optimal cohort size. The script first calculates the cumulative personality variance curve for the entire cohort, smooths this curve using a moving average to eliminate local noise, and then performs a slope analysis to find the "plateau"‚Äîthe point of diminishing returns where adding more subjects no longer meaningfully contributes to the psychological diversity of the pool. It then resolves country codes and sorts the final list by eminence.

##### Stage 4: Profile Generation

This is the final stage, which assembles the personality profiles for the selected candidates. It involves a mix of automated and manual steps.

1.  **Formatting (`prepare_sf_import.py`):** Formats the final subject list for import into the Solar Fire software. It performs a critical data integrity step by encoding each subject's unique `idADB` into a Base58 string and injecting it into the `ZoneAbbr` field. This technique allows a unique identifier to pass through the manual software step unharmed, ensuring a perfect data merge later. The `ZoneAbbr` field was specifically chosen as it is a text field that is passed through the software without modification and is not used in any astrological calculations, making it a safe channel for this data.
2.  **Manual Processing (Solar Fire):** The formatted file is imported into Solar Fire, which calculates the required celestial data and exports it as `sf_chart_export.csv`.
3.  **Neutralization (`neutralize_delineations.py`):** This script uses a powerful hybrid strategy to rewrite the esoteric source texts.
    *   **Fast Mode (`--fast`):** For initial runs, this mode bundles tasks into large, high-speed API calls (e.g., all 12 "Sun in Signs" delineations at once). This is highly efficient but may fail on some large tasks.
    *   **Robust/Resume Mode (default):** For resuming or fixing failed runs, this mode processes each of the 149 delineations as a separate, atomic task. This granular approach is slower but guarantees completion by solving potential response truncation issues from the LLM.
4.  **Integration (`create_subject_db.py`):** Bridges the manual step by reading the Solar Fire chart export, decoding the unique `idADB` from the `ZoneAbbr` field, and merging the chart data with the final subject list to produce a clean master database.
5.  **Generation (`generate_personalities_db.py`):** Assembles the final `personalities_db.txt` by combining the subject data with the neutralized delineation library according to a deterministic algorithm.

This combination of automated scripts and well-defined manual steps ensures the final dataset is both high-quality and computationally reproducible.

The pipeline can be understood through the following architecture, workflow, data flow, and logic diagrams.

### Data Preparation: Architecture

This diagram provides a map of the scripts in the data preparation pipeline, showing how they are orchestrated and which utilities they share.

{{grouped_figure:docs/diagrams/arch_prep_codebase.mmd | scale=2.5 | width=100% | caption=Data Preparation Code Architecture: The execution flow of the data processing scripts.}}

### Data Preparation: Workflow

This diagram shows the high-level, multi-stage workflow for the entire data preparation pipeline, including both automated and manual processes.

{{grouped_figure:docs/diagrams/flow_prep_pipeline.mmd | scale=2.5 | width=35% | caption=Data Preparation Workflow: The end-to-end pipeline from raw data extraction to the final generated databases, showing both manual and automated steps.}}

### Data Preparation: Data Flow

These diagrams show the sequence of data artifacts (files) created and transformed by the pipeline scripts at each major stage.

{{grouped_figure:docs/diagrams/flow_prep_1_qualification.mmd | scale=2.5 | width=75% | caption=Data Prep Flow 1: Data Sourcing and Candidate Qualification.}}

{{grouped_figure:docs/diagrams/flow_prep_2_selection.mmd | scale=2.5 | width=80% | caption=Data Prep Flow 2: LLM-based Candidate Selection.}}

{{grouped_figure:docs/diagrams/flow_prep_3_generation.mmd | scale=2.5 | width=100% | caption=Data Prep Flow 3: Profile Generation.}}

### Data Preparation: Logic

These diagrams illustrate the internal decision-making logic and control flow of each script in the data preparation pipeline.

{{grouped_figure:docs/diagrams/logic_prep_pipeline.mmd | scale=2.5 | width=50% | caption=Overall Logic for the Data Preparation Pipeline: A high-level view of the four main stages.}}

{{grouped_figure:docs/diagrams/logic_prep_find_links.mmd | scale=2.5 | width=70% | caption=Logic for Link Finding (`find_wikipedia_links.py`): The algorithm for finding Wikipedia URLs by scraping ADB and using a Wikipedia search fallback.}}

{{grouped_figure:docs/diagrams/logic_prep_validate_pages.mmd | scale=2.5 | width=80% | caption=Logic for Page Validation (`validate_wikipedia_pages.py`): The algorithm for validating Wikipedia page content, including redirect and disambiguation handling.}}

{{grouped_figure:docs/diagrams/logic_prep_eligible_candidates.mmd | scale=2.5 | width=65% | caption=Logic for Final Filtering (`select_eligible_candidates.py`): The algorithm for applying all deterministic data quality rules to create the final "eligible candidates" list.}}

{{grouped_figure:docs/diagrams/logic_prep_eminence_scoring.mmd | scale=2.5 | width=60% | caption=Logic for Eminence Scoring (`generate_eminence_scores.py`): The algorithm for batch processing, LLM interaction, and finalization of eminence scores.}}

{{grouped_figure:docs/diagrams/logic_prep_ocean_scoring.mmd | scale=2.5 | width=55% | caption=Logic for OCEAN Scoring (`generate_ocean_scores.py`): The algorithm for generating OCEAN personality scores for all eligible subjects. A robust pre-flight check ensures that interrupted runs can be safely resumed.}}

{{grouped_figure:docs/diagrams/logic_prep_final_candidates.mmd | scale=2.5 | width=35% | caption=Logic for Final Selection (`select_final_candidates.py`): The algorithm for finding the optimal cohort size by performing a slope analysis on a smoothed cumulative personality variance curve.}}

{{grouped_figure:docs/diagrams/logic_prep_neutralization.mmd | scale=2.5 | width=60% | caption=Logic for Delineation Neutralization (`neutralize_delineations.py`): The hybrid algorithm for rewriting texts. Fast mode bundles tasks for speed, while the robust default mode processes each item individually to guarantee completion.}}

{{grouped_figure:docs/diagrams/logic_prep_generation.mmd | scale=2.5 | width=70% | caption=Logic for Database Generation (`generate_personalities_db.py`): The algorithm for assembling the final description text for each subject.}}

## Experiment Lifecycle & Analysis

### Key Features

-   **Automated Batch Execution**: The `experiment_manager.py` script, driven by a simple PowerShell wrapper, manages entire experimental batches. It can run hundreds of replications, intelligently skipping completed ones to resume interrupted runs, and provides real-time progress updates, including a detailed spinner showing individual trial timers and overall replication batch ETA.
-   **Powerful Reprocessing Engine**: The manager's `--reprocess` mode allows for re-running the data processing and analysis stages on existing results without repeating expensive LLM calls. This makes it easy to apply analysis updates or bug fixes across an entire experiment.
-   **Guaranteed Reproducibility**: On every new run, the `config.ini` file is automatically archived in the run's output directory, permanently linking the results to the exact parameters that generated them.
-   **Standardized, Comprehensive Reporting**: Each replication produces a `replication_report.txt` file containing run parameters, status, a human-readable statistical summary, and a machine-parsable JSON block with all key metrics. This format is identical for new runs and reprocessed runs.
-   **Hierarchical Analysis & Aggregation**: The pipeline uses a set of dedicated compiler scripts for a fully auditable, bottom-up aggregation of results. `compile_replication_results.py` creates a summary for each run, `compile_experiment_results.py` combines those into an experiment-level summary, and finally `compile_study_results.py` creates a master `STUDY_results.csv` for the entire study.
-   **Comprehensive Self-Healing Capabilities**: The framework automatically detects and repairs multiple types of experiment corruption, including missing response files, corrupted analysis data, damaged configuration files, and malformed reports. The audit system classifies corruption severity and applies appropriate repair strategies, ensuring data integrity even after network interruptions, storage errors, or process crashes.
-   **Session Failure Tolerance**: Experiments continue when individual LLM API calls fail, requiring only that failure rates remain below 50% to maintain data quality while accommodating intermittent service disruptions.
-   **Graceful Repair Handling**: When repair operations fail, the pipeline continues to final audit and aggregation stages, providing complete experiment status reports rather than halting execution.
-   **Response Parsing Diagnostics**: Generates detailed parsing summaries showing success/failure status for each response, included in replication reports for troubleshooting diverse LLM output formats.
-   **Flexible Response Processing**: Extracts k√ók numerical score matrices from diverse LLM response formats by identifying exactly k consecutive lines containing exactly k numeric values at the end of each line, regardless of headers, explanations, or column formatting variations.
-   **Standardized Console Banners**: All audit results, whether for success, failure, or a required update, are presented in a consistent, easy-to-read, 4-line colored banner, providing clear and unambiguous status reports.
-   **Streamlined ANOVA Workflow**: The final statistical analysis is a simple two-step process. `compile_study_results.py` prepares a master dataset, which `analyze_study_results.py` then automatically analyzes to generate tables and publication-quality plots using user-friendly display names defined in `config.ini`.

### Visual Architecture

The main pipeline's architecture can be understood through four different views: the code architecture, the workflows, the data flow, and the experimental logic.

#### Code Architecture Diagram
The codebase for the experiment workflow and analysis is organized into a clear hierarchy:

1.  **Main User Entry Points**: User-facing PowerShell scripts (`.ps1`) that orchestrate high-level workflows like creating, auditing, or fixing experiments and studies.
2.  **Experiment Lifecycle Management**: The core Python backend for managing a single experiment. This includes primary orchestrators (`experiment_manager.py`, `experiment_auditor.py`) and dedicated finalization scripts (`manage_experiment_log.py`, `compile_experiment_results.py`).
3.  **Single Replication Pipeline**: A set of scripts, managed by `replication_manager.py`, that execute the end-to-end process for a single run, from query generation to final reporting.
4.  **Study-Level Analysis**: Python scripts that operate on the outputs of multiple experiments to perform study-wide aggregation and statistical analysis.
5.  **Utility & Other Scripts**: Shared modules and standalone utility scripts that provide common functionality (e.g., `config_loader.py`) or perform auxiliary tasks.

{{grouped_figure:docs/diagrams/arch_main_codebase.mmd | scale=2.5 | width=100% | caption=Codebase Architecture: A comprehensive map of the entire Python codebase. PowerShell scripts (blue) are user-facing entry points that execute core Python logic. Solid lines indicate execution, while dotted lines show module imports.}}

#### Workflow Diagrams
The framework's functionality is organized into a clear hierarchy of workflows, initiated by dedicated PowerShell scripts.

**Experiment-Level Workflows:**
-   **Create a New Experiment (`new_experiment.ps1`):** The primary workflow for generating new experimental data for a single condition.
-   **Audit an Experiment (`audit_experiment.ps1`):** A read-only diagnostic tool that provides a detailed completeness report for an experiment.
-   **Fix or Update an Experiment (`fix_experiment.ps1`):** The main "fix-it" tool for resuming interrupted runs or reapplying analysis updates to existing data.

**Study-Level Workflows:**
-   **Audit a Study (`audit_study.ps1`):** A read-only diagnostic tool that provides a consolidated audit of all experiments in a study directory.
-   **Compile a Study (`compile_study.ps1`):** The final step in the research process. This script aggregates data from all experiments in a study, runs the statistical analysis, and generates the final reports and plots.

#### Workflow 1: Create a New Experiment

This is the primary workflow for generating new experimental data. The PowerShell entry point (`new_experiment.ps1`) calls the Python batch controller (`experiment_manager.py`). The manager creates a new, timestamped directory and runs the full set of replications from scratch.

The `replication_manager.py` script executes the full pipeline for a single run, which is broken into six distinct stages:

1.  **Build Queries**: Generates all necessary query files and trial manifests.
2.  **Run LLM Sessions**: Interacts with the LLM API in parallel to get responses.
3.  **Process LLM Responses**: Parses the raw text responses from the LLM into structured score files.
4.  **Analyze LLM Performance**: A unified two-part process that first calculates core performance metrics and then injects diagnostic bias metrics.
5.  **Generate Final Report**: Assembles the final `replication_report.txt` from the analysis results and captured logs.
6.  **Create Replication Summary**: Creates the final `REPLICATION_results.csv`, marking the run as valid.

{{grouped_figure:docs/diagrams/flow_main_1_new_experiment.mmd | scale=2.5 | width=60% | caption=Workflow 1: Create a New Experiment, showing the main control loop and the internal replication pipeline.}}

#### Workflow 2: Audit an Experiment

This workflow provides a read-only, detailed completeness report for an experiment without performing any modifications. The `audit_experiment.ps1` wrapper calls the dedicated `experiment_auditor.py` script. The full audit report, including subprocess outputs, is also saved to `experiment_audit_log.txt` within the audited directory.

{{grouped_figure:docs/diagrams/flow_main_2_audit_experiment.mmd | scale=2.5 | width=100% | caption=Workflow 2: Audit an Experiment. Provides a read-only, detailed completeness report for an experiment.}}

##### Interpreting the Audit Report
The audit script is the primary diagnostic tool for identifying issues in a failed or incomplete experiment. It uses a simple but robust rule to classify problems: the number of distinct errors found in a single replication run.

**Repairable Issues (Single Error)**
If a replication run has **exactly one** identifiable problem, it is considered safe to repair in-place. The `Status` column will show a specific, targeted error code:

| Status Code | Description | Recommended Action |
| :--- | :--- | :--- |
| **`INVALID_NAME`** | The run directory name is malformed. | Run `fix_experiment.ps1` to repair. |
| **`CONFIG_ISSUE`** | The `config.ini.archived` is missing or inconsistent. | Run `fix_experiment.ps1` to repair. |
| **`QUERY_ISSUE`** | Core query files or manifests are missing. | Run `fix_experiment.ps1` to repair. |
| **`RESPONSE_ISSUE`** | One or more LLM response files are missing. | Run `fix_experiment.ps1` to repair. |
| **`ANALYSIS_ISSUE`** | Core data is present, but analysis files are missing/outdated. | Run `fix_experiment.ps1` to repair. |

Any of these single-error states will result in an overall audit recommendation to run **`fix_experiment.ps1`**.

**Corrupted Runs (Multiple Errors)**
If a replication run has **two or more** distinct problems (e.g., a missing config file *and* missing responses), it is flagged with the status `RUN_CORRUPTED`. A corrupted run indicates a systemic issue that cannot be safely repaired automatically. The audit will recommend investigating the issue manually. Corrupted experiments should generally be deleted and re-run.

The `Details` string provides a semicolon-separated list of all detected issues (e.g., `CONFIG_MISSING; RESPONSE_FILES_INCOMPLETE`).

A key part of this validation is a strict schema check on the `replication_report.txt` file. The audit verifies that the JSON block of metrics contains *exactly* the set of required keys‚Äîno more, and no less. A report with missing (`REPORT_INCOMPLETE_METRICS`) or extra, obsolete metrics (`REPORT_UNEXPECTED_METRICS`) will be flagged with an `ANALYSIS_ISSUE`. This ensures that analysis is only ever performed on data with a correct and up-to-date schema.

In addition to the per-replication table, the audit provides an `Overall Summary` that includes the `Experiment Aggregation Status`. This checks for the presence and completeness of top-level summary files (`EXPERIMENT_results.csv`, `experiment_log.csv`), confirming whether the last aggregation step for the experiment was successfully completed.

#### Workflow 3: Fixing or Updating an Experiment

This workflow is the main "fix-it" tool for any existing experiment. The `fix_experiment.ps1` script is an intelligent wrapper. It first performs a full audit by calling `experiment_auditor.py` to diagnose the experiment's state. Based on the audit result, it then calls `experiment_manager.py` to apply the correct repairs.

-   If the audit finds missing data or outdated analysis files, the script proceeds to automatically apply the correct repair.
-   If the audit finds the experiment is already complete and valid, it becomes interactive, presenting a menu that allows the user to force a full data repair, an analysis update, or a simple re-aggregation of results.

{{grouped_figure:docs/diagrams/flow_main_3_fix_experiment.mmd | scale=2.5 | width=100% | caption=Workflow 3: Fixing or Updating an Experiment, showing both automatic and interactive repair paths.}}

#### Workflow 4: Compile a Study

This workflow is used after all experiments are validated to compile, analyze, and evaluate the entire study. It performs a robust pre-flight check by calling `audit_study.ps1`. If the study is not ready for processing (or is already complete), it will halt with a clear recommendation. Otherwise, it proceeds to compile all results and run the final statistical analysis.

{{grouped_figure:docs/diagrams/flow_main_4_compile_study.mmd | scale=2.5 | width=90% | caption=Workflow 4: Compile a Study. Audits, compiles, and analyzes all experiments in a study.}}

#### Workflow 5: Audit a Study

This script is the primary diagnostic tool for assessing the overall state of a study. It performs a two-part, read-only audit:

1.  **Readiness Audit**: It iterates through every experiment folder and runs a quiet, individual audit on each to determine its status (e.g., `VALIDATED`, `NEEDS REPAIR`).
2.  **Completeness Audit**: It verifies the existence of top-level study artifacts, such as `STUDY_results.csv` and the `anova/` analysis directory.

Based on the combined results from both audits, it presents a consolidated summary table and provides a final, context-aware recommendation for the correct next step.

{{grouped_figure:docs/diagrams/flow_main_5_audit_study.mmd | scale=2.5 | width=70% | caption=Workflow 5: Audit a Study. Consolidated completeness report for all experiments in a study.}}

{{pagebreak}}
#### Data Flow Diagram

This diagram shows how data artifacts (files) are created and transformed by the experiment workflow and analysis scripts. It traces the flow from initial inputs like `config.ini` and the personalities database, through intermediate query and response files, to the final aggregated results and analysis plots.

{{grouped_figure:docs/diagrams/data_main_flow.mmd | scale=2.5 | width=80% | caption=Data Flow Diagram: Creation and transformation of data artifacts (files) by the experiment workflow and analysis scripts.}}

#### Logic Flowcharts

These diagrams illustrate the scientific and procedural methodology at each level of the experimental hierarchy.

{{grouped_figure:docs/diagrams/logic_main_replication.mmd | scale=2.5 | width=60% | caption=Replication Logic: The scientific methodology for a single replication run.}}

{{grouped_figure:docs/diagrams/logic_main_experiment.mmd | scale=2.5 | width=100% | caption=Experiment Logic: The aggregation of multiple replication results to produce final experiment-level summaries.}}

{{grouped_figure:docs/diagrams/logic_main_study.mmd | scale=2.5 | width=100% | caption=Study Logic: The complete workflow for processing a study, from auditing and aggregation to final statistical analysis.}}

## Experimental Hierarchy

The project's experiments are organized in a logical hierarchy:

-   **Study**: The highest-level grouping, representing a major research question (e.g., "Performance on Random vs. Correct Mappings").
-   **Experiment**: A complete set of runs for a single condition within a study (e.g., "Gemini 2.0 Flash with k=10 Subjects").
-   **Replication**: A single, complete run of an experiment, typically repeated 30 times for statistical power.
-   **Trial**: An individual matching task performed within a replication, typically repeated 100 times.

## Study Design

This section provides comprehensive guidance for designing and executing multi-factor experimental studies using the framework. It covers factorial design principles, sample size determination, and practical execution strategies.

### Factorial Design Overview

The framework supports multi-factor experimental designs where each factor can have multiple levels. A typical study design might include:

**Factor 1: Mapping Strategy** (between-subjects)
- Correct mappings (personalities matched to correct individuals)
- Random mappings (personalities randomly shuffled)

**Factor 2: Group Size (k)** (within-subjects)
- Multiple difficulty levels (e.g., k ‚àà {7, 10, 14})
- Determines number of comparisons per trial (k¬≤)

**Factor 3: Model** (within-subjects)
- Multiple LLMs to test generalizability
- Different architectures and capabilities

### Design Specification Example

#### Three-Factor Factorial Design

**Design Parameters:**
```
Factor 1: mapping_strategy (2 levels)
  - correct
  - random

Factor 2: group_size (3 levels)
  - k = 7  (49 comparisons per trial)
  - k = 10 (100 comparisons per trial)
  - k = 14 (196 comparisons per trial)

Factor 3: model (3 levels)
  - Selected from low-cost, high-reliability tier
  - Examples: Llama 3.3 70B, Gemini 2.5 Flash Lite, DeepSeek V3

Sample Size:
  - 30 replications per condition
  - 80 trials per replication
```

**Total Experimental Scope:**

- Conditions: 2 √ó 3 √ó 3 = **18 conditions**
- Experiments: 18 √ó 30 = **540 experiments**
- Trials: 540 √ó 80 = **43,200 trials**

#### Design Table

| Factor | Type | Levels | Values |
|--------|------|--------|--------|
| `mapping_strategy` | Between-subjects | 2 | correct, random |
| `k` (group_size) | Within-subjects | 3 | 7, 10, 14 |
| `model` | Within-subjects | 3 | gemini-2.5-flash-lite, gpt-4.1-nano, llama-3.3-70b-instruct |

### Design Rationale

#### Group Size Selection (k-values)

**K=7 (Lower Boundary):**
- MRR chance level: 0.3704
- 49 comparisons per trial
- Tests whether performance plateaus at easier difficulties
- Avoids K=4 issues (chance = 0.5208, too close to coin flip)

**K=10 (Middle Range):**
- MRR chance level: 0.2929
- 100 comparisons per trial
- Provides continuity with pilot studies
- Balanced difficulty

**K=14 (Upper Boundary):**
- MRR chance level: 0.2323
- 196 comparisons per trial
- Tests performance degradation at higher complexity
- Pushes LLM capabilities without exceeding context limits

**Difficulty Progression:**
- K=7 to K=10: 20.9% decrease in MRR chance level
- K=10 to K=14: 20.7% decrease in MRR chance level
- Even spacing ensures systematic difficulty gradient

#### Model Selection

**Low-Cost, High-Reliability Tier:**

The study prioritizes models that demonstrate both cost-effectiveness and high reliability (95%+ parsing success rate). All costs are based on OpenRouter.ai rates as of October 2025.

**Candidate Models (by cost tier):**

**Ultra-Low Cost ($0.13 per experiment):**
- Meta Llama 3.3 70B Instruct ($0.13)

**Low Cost ($0.24-0.32 per experiment):**
- Mistral Small 3.2 24B ($0.24)
- Google Gemini 2.0 Flash Lite ($0.24)
- Google Gemini 2.5 Flash Lite ($0.32)
- Google Gemini 2.5 Flash Lite Preview 06-17 ($0.32)
- OpenAI GPT-4.1 Nano ($0.32)

**Mid Cost ($0.49-0.72 per experiment):**
- Meta Llama 4 Maverick ($0.49)
- Qwen Qwen3 Coder 480B A35B ($0.72)

**High Cost ($0.81-1.30 per experiment):**
- DeepSeek V3 0324 ($0.81)
- OpenAI GPT-4.1 Mini ($1.30)

**Recommended 3-Model Combinations:**

1. **Ultra-Budget** (Llama 3.3 + Mistral Small + Gemini 2.0 Flash Lite): **$110 total** ($6.10 per condition)
2. **Budget Balanced** (Llama 3.3 + Gemini 2.5 Flash Lite + GPT-4.1 Nano): **$139 total** ($7.70 per condition) ‚úì **Recommended**
3. **Mid-Range Performance** (Gemini 2.5 Preview + Llama 4 + Qwen3 Coder): **$275 total** ($15.30 per condition)
4. **Architecture Diversity** (Llama 3.3 + DeepSeek V3 + GPT-4.1 Mini): **$403 total** ($22.40 per condition)
5. **Performance Focused** (Qwen3 Coder + DeepSeek V3 + GPT-4.1 Mini): **$509 total** ($28.30 per condition)

**Cost Savings:** All strategies provide 92-98% savings vs premium model baseline ($6,624).

**Selection Criteria:**
- Cost efficiency: 92-98% savings vs premium models ($6,624 baseline)
- Parsing reliability: All models demonstrate 95%+ structured output compliance
- Architectural diversity: Mix of open-source (Meta, Mistral) and proprietary (Google, OpenAI, DeepSeek, Qwen) approaches
- Parameter range: 24B to 480B parameters
- Training diversity: Different training datasets and objectives
- Source: OpenRouter.ai rates (October 2025)

**Note:** The "Budget Balanced" strategy ($139 total, $7.70 per condition) provides excellent cost-performance tradeoff while maintaining strong architectural diversity across Meta, Google, and OpenAI providers.

### Sample Size and Statistical Power

#### Power Analysis

**Target:** Detect small effect sizes (Cohen's d < 0.20) with 80%+ power

**Sample Size Requirements:**

For **main effects** with 30 replications per condition:
- Mapping strategy (correct vs random): **~82% power** for d=0.20
- Group size (3 levels, repeated measures): **>90% power** for d=0.20
- Model (3 levels, repeated measures): **>90% power** for d=0.20

For **two-way interactions:**
- Power: **75-85%** for d=0.20

For **three-way interaction:**
- Power: **70-75%** for d=0.20

For **within-replication tests** (vs chance) with 80 trials:
- Power: **~94%** for d=0.20

#### Justification for 30 Replications √ó 80 Trials

**30 Replications:**

- Sufficient for detecting d=0.20 with 82% power in factorial ANOVA
- Provides robust estimates for interaction effects
- Sufficient for post-hoc comparisons with Tukey HSD
- Matches published study replication count for comparable statistical rigor

**80 Trials per Replication:**

- Standard Error = 0.0168 (assuming SD=0.15 for MRR)
- Provides 1.79:1 signal-to-noise ratio for d=0.20 effects
- Reduces within-condition variance for precise between-condition comparisons
- Handles occasional parsing failures with strong redundancy
  - K=7: Expected 77 valid responses (8% buffer over baseline)
  - K=10: Expected 76 valid responses (7% buffer)
  - K=14: Expected 74 valid responses (5% buffer, 194% above minimum threshold)

**Comparison to Alternative Designs:**

| Design | Replications | Trials | Total per Condition | Power (d=0.20) | Cost Efficiency |
|--------|--------------|--------|---------------------|----------------|-----------------|
| Minimal | 15 | 40 | 600 | ~65% | High |
| Balanced | 20 | 50 | 1,000 | ~75% | Good |
| Recommended | 25 | 60 | 1,500 | ~78% | Moderate |
| **Selected** | **30** | **80** | **2,400** | **~82%** | **Moderate** |
| Conservative | 30 | 100 | 3,000 | ~82% | Lower |

**Design Choice Rationale:**
The 30√ó80 design provides optimal balance between statistical power (82% for d < 0.20), cost efficiency (20% savings vs 30√ó100), and resilience to parsing failures (strong buffer across all k-values). It maintains the same between-subjects power as the 30√ó100 baseline while achieving meaningful resource savings.

### Resource Requirements

#### Subject Pool

**Database Capacity Needed:**
- Unique subjects required: ~1,000-1,500 (with strategic reuse)
- Recommended database size: 5,000+ entries
- Reuse strategy: Random sampling from larger pool minimizes overlap

**Subject Allocation:**
- K=7: ~400-500 unique subjects
- K=10: ~600-700 unique subjects
- K=14: ~700-800 unique subjects

#### Computational Resources

**API Costs (estimated for 43,200 trials with low-cost models):**

**Recommended Cost Scenarios (18 conditions √ó 30 reps = 540 experiments):**

| Strategy | Models | Total Cost | Per Condition |
|----------|--------|------------|---------------|
| Ultra-Budget | Llama 3.3 + Mistral Small + Gemini 2.0 Lite | $110 | $6.10 |
| **Budget Balanced** ‚úì | **Llama 3.3 + Gemini 2.5 Lite + GPT-4.1 Nano** | **$139** | **$7.70** |
| Mid-Range | Gemini 2.5 Prev + Llama 4 + Qwen3 Coder | $275 | $15.30 |
| Architecture Mix | Llama 3.3 + DeepSeek V3 + GPT-4.1 Mini | $403 | $22.40 |
| Performance | Qwen3 + DeepSeek V3 + GPT-4.1 Mini | $509 | $28.30 |

**Per-Model Cost Breakdown (180 experiments per model):**
- Ultra-low tier ($0.13): $23.40 per model
- Low tier ($0.24-0.32): $43.20-57.60 per model
- Mid tier ($0.49-0.72): $88.20-129.60 per model
- High tier ($0.81-1.30): $145.80-234.00 per model

**Cost Comparison:** 92-98% lower than premium model baseline ($6,624 for Gemini 2.0 Flash + GPT-4o + Claude 3.5 Sonnet)

*All costs based on OpenRouter.ai rates (October 2025)*

**Execution Time:**

| Parallelization | Concurrent Trials | Total Time | Risk Level |
|-----------------|-------------------|------------|------------|
| Conservative | 10-20 | ~32-48 hours | Low (rate limits) |
| Moderate | 30-40 | ~16-24 hours | Medium |
| Aggressive | 50+ | ~10-14 hours | Higher (API throttling) |

### Execution Strategy

#### Phase-Based Implementation

**Phase 1: Pilot Run (1 condition)**
- Purpose: Validate pipeline, identify issues
- Scope: 1 condition √ó 30 reps √ó 80 trials = 2,400 trials
- Duration: ~1-2 hours (parallelized)
- Cost: ~$1.54 (Budget Balanced: $0.26 + $0.64 + $0.64)
- Monitors: Parsing success rate, execution stability, API limits

**Phase 2: Core Study (remaining 17 conditions)**
- Purpose: Execute main experimental design
- Scope: 17 conditions √ó 30 reps √ó 80 trials = 40,800 trials
- Duration: ~32-44 hours (parallelized at 20-30 concurrent)
- Cost: ~$137 (Budget Balanced: $110-509 depending on model combination)
- Strategy: Process in batches of 3-6 conditions with quality checks

**Phase 3: Analysis and Validation**
- Purpose: Compile results, run statistical analysis
- Scope: All 18 conditions aggregated
- Duration: ~2-4 hours
- Outputs: STUDY_results.csv, ANOVA tables, diagnostic plots

#### Batch Processing Guidelines

**Recommended Batch Size:** 3-6 conditions per batch

**Per-Batch Workflow:**
1. Configure `config.ini` for each condition
2. Run `new_experiment.ps1` with 30 replications
3. Execute `audit_experiment.ps1` for quality check
4. Fix any issues with `fix_experiment.ps1`
5. Proceed to next batch

**Quality Monitoring:**
- Track parsing success rates (target: >92% for K=14, >95% for K‚â§10)
- Monitor execution time per trial (detect API throttling)
- Verify response quality with spot checks
- Check for systematic errors or biases

### Study Organization

#### Directory Structure

```
output/
‚îî‚îÄ‚îÄ studies/
    ‚îî‚îÄ‚îÄ main_study_2025/
        ‚îú‚îÄ‚îÄ exp_01_correct_k7_gemini/
        ‚îú‚îÄ‚îÄ exp_02_correct_k7_gpt4o/
        ‚îú‚îÄ‚îÄ exp_03_correct_k7_claude/
        ‚îú‚îÄ‚îÄ exp_04_correct_k10_gemini/
        ...
        ‚îú‚îÄ‚îÄ exp_18_random_k14_claude/
        ‚îú‚îÄ‚îÄ STUDY_results.csv
        ‚îî‚îÄ‚îÄ anova/
            ‚îú‚îÄ‚îÄ STUDY_analysis_log.txt
            ‚îú‚îÄ‚îÄ boxplots/
            ‚îî‚îÄ‚îÄ diagnostics/
```

#### Naming Convention

**Format:** `exp_{number}_{mapping}_{k}{model}/`

**Examples:**
- `exp_01_correct_k7_gemini/`
- `exp_10_random_k10_gpt4o/`
- `exp_18_random_k14_claude/`

**Benefits:**
- Alphabetical sorting groups by condition
- Sequential numbering aids tracking
- Clear condition identification

### Configuration Management

#### Per-Condition Configuration

For each of the 18 conditions, update `config.ini`:

```ini
[Study]
num_replications = 30
mapping_strategy = correct  # or random

[LLM]
model_name = google/gemini-2.0-flash-exp  # or others
temperature = 0.2

[Experiment]
k = 7  # or 10, 14
m = 80
```

#### Randomization Seeds

**For Reproducibility:**
Set fixed seeds in `config.ini`:
```ini
[Randomization]
personality_selection_seed = 42
list_shuffle_seed = 123
```

**Different seeds per condition** ensures independence while maintaining reproducibility.

### Performance Metrics and Bias Analysis

#### Primary Performance Metrics

The framework calculates three main performance metrics for each replication:

- **Mean Reciprocal Rank (MRR)**: Average of 1/rank across all trials, emphasizing top-ranked performance
- **Top-K Accuracy**: Proportion of trials where correct answer appears in top K positions
- **Mean Rank of Correct ID**: Average position of correct answer across all trials

All metrics are compared against theoretical chance levels using Wilcoxon signed-rank tests.

#### Positional Bias Detection Methodology

**Purpose**: Detect whether LLMs exhibit position bias in their choices - systematic preference for certain positions in the ranked list regardless of correctness, similar to how human users favor top-ranked search results even when relevance is controlled.

**Metric Choice - Rank vs MRR**:

The framework uses `mean_rank_of_correct_id` (not MRR) for positional bias detection via linear regression:

- **Linear scale**: Rank values (1, 2, 3, ..., k) are linearly spaced, satisfying linear regression assumptions
- **Uniform sensitivity**: A 1-rank shift is equally detectable at all performance levels (rank 1‚Üí2 has same weight as rank 10‚Üí11)
- **Direct interpretability**: Slope values represent "ranks per trial" (e.g., slope of 0.3 means "correct answer drifts by 0.3 rank positions per trial")
- **Matches bias mechanism**: Position bias manifests as systematic drift in where the correct answer appears in the ranked list

**Why not MRR**: MRR = 1/rank creates non-linear compression (1.0‚Üí0.5‚Üí0.33‚Üí0.25), making shifts at lower performance less detectable and complicating slope interpretation in practical terms.

**Analysis Approach**: Linear regression is performed on mean_rank_of_correct_id values across trials within each replication. The resulting slope, r-value, and p-value characterize whether the LLM's selection behavior shows systematic positional preferences. A non-zero slope suggests position-dependent choice behavior rather than purely content-based decisions.

**Note**: This differs from the framework's primary performance metric (MRR), which remains appropriate for aggregated performance reporting. Rank is used specifically for detecting positional choice bias within replications due to its superior statistical properties for linear regression analysis.

### Statistical Analysis Plan

#### Primary Analysis

**Three-Way Repeated Measures ANOVA:**
```
DV: MRR Lift (or Top-1/Top-3 Accuracy Lift)

Factors:
  - mapping_strategy (between-subjects, 2 levels)
  - k (within-subjects, 3 levels)
  - model (within-subjects, 3 levels)

Effects tested:
  - Main effects: mapping, k, model
  - Two-way interactions: mapping√ók, mapping√ómodel, k√ómodel
  - Three-way interaction: mapping√ók√ómodel
```

#### Effect Size Measures

- **Eta-squared (Œ∑¬≤)**: Proportion of variance explained by each factor
- **Cohen's d**: Standardized mean difference for pairwise comparisons
- **Lift metrics**: Performance relative to chance (comparable across k-values)

#### Post-Hoc Tests

- **Tukey HSD**: For pairwise comparisons of k-values and models
- **FDR correction**: Benjamini-Hochberg procedure for multiple comparisons
- **Planned contrasts**: K=7 vs K=14 (boundary comparison)

#### Complementary Analysis

**Bayesian Analysis:**
- Bayes Factor (BF‚ÇÅ‚ÇÄ): Evidence for effect vs null
- Credible intervals for effect sizes
- Model comparison for interaction effects

### Alternative Design Options

#### Reduced Scope Designs

**Option A: Two K-Values**
- Factors: 2 mapping √ó 2 k-values √ó 3 models
- Conditions: 12
- Experiments: 360 (with 30 reps)
- Trials: 28,800 (with 80 per rep)
- Cost: ~$73-339 (depending on model combination)
- Use when: Budget or time constrained

**Option B: Two Models**
- Factors: 2 mapping √ó 3 k-values √ó 2 models
- Conditions: 12
- Experiments: 360 (with 30 reps)
- Trials: 28,800 (with 80 per rep)
- Cost: ~$73-339 (depending on model combination)
- Use when: Model comparison less critical

#### Staged Execution

**Stage 1: Core Study (2 models)**
- Example: Llama 3.3 + Gemini 2.5 Flash Lite
- 12 conditions
- 28,800 trials
- Cost: ~$54-81
- Can publish initial results

**Stage 2: Replication (3rd model)**
- Add third model (e.g., GPT-4.1 Nano or DeepSeek V3)
- 6 additional conditions
- 14,400 trials
- Cost: ~$58-234
- Confirms generalizability

### Methods Section Template

**For Publication:**

> "We employed a 2 √ó 3 √ó 3 factorial design with mapping strategy (correct vs random) as a between-subjects factor, and group size (k ‚àà {7, 10, 14}) and model as within-subjects factors. Three models from the low-cost, high-reliability tier were selected based on cost efficiency ($0.13-1.30 per experiment via OpenRouter.ai), architectural diversity (open-source and proprietary), and demonstrated parsing reliability (95%+ success rate). Group sizes were selected to provide systematic difficulty progression with approximately 21% increases in task complexity (measured by decreases in MRR chance level) between consecutive k-values. We conducted 30 replications per condition with 80 trials per replication, providing >80% statistical power to detect small effect sizes (Cohen's d < 0.20) for main effects. This design yielded 18 experimental conditions with 540 total experiments and 43,200 trials, executed at a total cost of $110-509 depending on model selection strategy."

### Validation Checklist

Before executing the main study, verify:

- [ ] Database contains ‚â•5,000 unique subjects
- [ ] API keys configured and funded
- [ ] Pilot run (1 condition) completed successfully
- [ ] Parsing success rate >92% for K=14, >95% for K‚â§10 in pilot
- [ ] `config.ini` randomization seeds documented
- [ ] Directory structure created
- [ ] Naming convention established
- [ ] Batch processing schedule defined
- [ ] Quality monitoring procedures in place
- [ ] Backup strategy for long-running processes

### Risk Mitigation

**API Rate Limits:**
- Start with conservative parallelization (10-20 concurrent)
- Monitor response times and error rates
- Implement exponential backoff for retries

**Parsing Failures:**
- Target: <5% failure rate for K‚â§10, <8% for K=14
- Monitor: K=14 may show higher rates due to 196 comparisons per trial
- Mitigation: 80 trials provides strong buffer (expected 74 valid responses, 194% above minimum threshold of 25)

**Cost Overruns:**
- Track cumulative costs per batch
- Adjust parallelization if API costs spike
- Consider staged execution if budget concerns arise

**Data Loss:**
- Framework automatically saves all intermediate results
- Each experiment is self-contained
- Can resume interrupted experiments with `fix_experiment.ps1`

### Next Steps

After defining your study design:

1. **Configure Database**: Ensure subject pool meets requirements
2. **Run Pilot**: Execute Phase 1 (1 condition) to validate pipeline
3. **Execute Main Study**: Process remaining conditions in batches
4. **Compile Results**: Use `compile_study.ps1` for final analysis
5. **Validate**: Review diagnostic plots and statistical assumptions

For detailed execution instructions, see the **Lifecycle Guide** and **Replication Guide**.


## Error Recovery and Resilience

The framework implements comprehensive error recovery mechanisms to ensure data integrity and experimental continuity even when facing real-world failures.

### Categorized Error Handling

The pipeline categorizes errors by type and severity to apply appropriate recovery strategies:

**Statistical Issues:**
- Problems with data variance, sample sizes, or statistical test assumptions
- **Recovery**: Fallback strategies (e.g., Games-Howell when Tukey HSD fails)
- **Impact**: Analysis continues with reduced functionality but preserved scientific validity

**Data Structure Issues:**
- Missing columns, malformed files, or schema mismatches  
- **Recovery**: Validation with repair recommendations and graceful degradation
- **Impact**: Quality warnings generated while preserving completed analysis

**File I/O Issues:**
- Permission errors, missing manifests, or corrupted data files
- **Recovery**: Specific error logging with targeted recovery paths
- **Impact**: Affected components isolated while other analysis continues

**Validation Errors:**
- Manifest mismatches or experimental consistency failures
- **Recovery**: Analysis completion with quality status marking
- **Impact**: Results flagged for review but data preserved

### Intelligent Recovery Strategies

**Graceful Degradation:** When non-critical components fail, the analysis continues with reduced functionality rather than aborting entirely. For example, if Bayesian analysis fails due to data structure issues, the frequentist analysis proceeds normally.

**Intelligent Fallbacks:** The system automatically selects alternative methods when primary approaches fail. Post-hoc testing falls back from Tukey HSD to Games-Howell when equal variance assumptions are violated.

**Quality Preservation:** Results are marked with validation status (COMPLETE, PARTIAL, INVALID) while preserving all successfully completed analysis, ensuring maximum data recovery from partial failures.

**Enhanced Logging:** Error categorization enables targeted troubleshooting by distinguishing between statistical issues, data problems, and system failures, accelerating diagnosis and repair.

This multi-layered approach ensures that researchers can trust the framework to maintain scientific rigor and data integrity even when facing common real-world failures like network interruptions, storage errors, or process crashes.

### Enhanced Error Reporting and Recovery

The framework provides intelligent error detection and user-friendly guidance:

- **Model Configuration Errors**: When all LLM sessions fail (100% failure rate), the system automatically detects likely model configuration issues and provides specific guidance to check model names and API credentials.

- **Colored Error Output**: Error messages use color coding for improved visibility and categorization of different failure types.

- **Repair Cycle Limits**: The repair system implements a 3-cycle maximum to prevent infinite loops when queries consistently fail due to external issues (e.g., API problems, invalid models).

- **Progress Feedback**: All operations provide consistent timing information (Time Elapsed, Time Remaining, ETA) to keep users informed during long-running processes.

## Directory Structure

This logical hierarchy is reflected in the physical layout of the repository:

{{diagram:docs/diagrams/view_directory_structure.txt | scale=2.5 | width=100%}}

Data dictionaries provide detailed explanations for all files: [üìÅ Data Preparation Data Dictionary](../docs/DATA_PREPARATION_DATA_DICTIONARY.md) covers the `data/` directory, and [üìä Experiment Workflow Data Dictionary](../output/EXPERIMENT_WORKFLOW_DATA_DICTIONARY.md) covers the `output/` directory structure and experimental results.

## Setup and Installation

This project uses **PDM** for dependency and environment management.

1.  **Install PDM (One-Time Setup)**:
    If you don't have PDM, install it once with pip. It's best to run this from a terminal *outside* of any virtual environment.
    ```bash
    pip install --user pdm
    ```
    > **Note:** If `pdm` is not found in a new terminal, use `python -m pdm` instead.

2.  **Install Project Environment & Dependencies**:
    From the project's root directory, run the main PDM installation command. The `-G dev` flag installs all packages, including the development tools needed to run the test suite.
    ```bash
    pdm install -G dev
    ```
    This command creates a local `.venv` folder and installs all necessary packages into it.

3.  **Configure API Key**:
    *   Create a file named `.env` in the project root.
    *   Add your API key from OpenRouter. The key will start with `sk-or-`.
        `OPENROUTER_API_KEY=your-actual-api-key`

To run any project command, such as the test suite, prefix it with `pdm run`:
```bash
pdm run test
```

> **For Developers:** If you intend to contribute to the project or encounter issues with the simple setup, please see the **[Developer Setup Guide in DEVELOPERS_GUIDE.md](DEVELOPERS_GUIDE.md#getting-started-development-environment-setup)** for more detailed instructions and troubleshooting.

## Configuration (`config.ini`)

The `config.ini` file is the central hub for defining all parameters for your experiments. The pipeline automatically archives this file with the results for guaranteed reproducibility.

| Section | Parameter | Description | Example Value |
| :--- | :--- | :--- | :--- |
| **`[Experiment]`** | `num_replications` | The number of times the experiment will be repeated (`r`). | `2` |
| | `num_trials` | The number of trials for each replication (`m`). | `3` |
| | `group_size` | The number of subjects in each group (`k`). | `4` |
| | `mapping_strategy` | A key experimental variable; can be `correct` or `random`. | `correct` |
| **`[LLM]`** | `model_name` | The API identifier for the LLM to be tested for the main experiment. | `google/gemini-flash-1.5` |
| | `temperature` | Controls the randomness of the model's output (0-2). | `1` |
| | `max_parallel_sessions` | The number of concurrent API calls to make. | `10` |
| **`[Analysis]`** | `min_valid_response_threshold` | Minimum average valid responses for an experiment to be included in the final analysis. | `25` |
| **`[DataGeneration]`** | `bypass_candidate_selection` | If `true`, skips LLM-based scoring and uses all eligible candidates. | `false` |
| | `cutoff_search_start_point` | The cohort size at which to start searching for the variance curve plateau. | `3500` |
| | `smoothing_window_size` | The window size for the moving average used to smooth the variance curve. | `800` |

#### Model Selection Philosophy and Future Work
The selection of models for this study was guided by a balance of performance, cost, speed, and technical compatibility with the automated framework. Several top-tier models were not included for one of the following reasons:

-   **Prohibitive Cost**: Models like `o1 pro`, `GPT 4.5 Preview`, and `Claude 4 Opus` were excluded as a single experiment (requiring ~3,000 queries) was financially infeasible.

-   **Technical Incompatibility**: Models like `Gemini 2.5 Pro` lacked a "non-thinking" mode, making the automated parsing of a structured response table overly challenging.

-   **Excessive Runtime**: A number of large models, including `Qwen3 235B` and `Llama 3.1 Nemotron Ultra 253B`, were excluded as a full experimental run would take longer than 20 hours.

A follow-up study is planned to evaluate other powerful, medium-cost models as API costs decrease and technical features evolve. Candidates include: `Grok 3`, `Grok 4`, `Claude 4 Sonnet`, `Claude 3.7 Sonnet`, `GPT-4o`, `o3`, `GPT-4.1`, `Mistral Large 2`, `Gemini 1.5 Pro`, and various `o1`/`o3`/`o4` mini-variants.

### Analysis Settings (`[Analysis]`)

-   **`min_valid_response_threshold`**: Minimum average number of valid responses (`n_valid_responses`) for an experiment to be included in the final analysis. Set to `0` to disable.

## Known Issues and Future Work

This framework is under active development. For a detailed and up-to-date list of planned improvements, known issues, and future development tasks, please see the [Project Roadmap](ROADMAP.md).

## Choosing the Right Workflow: Separation of Concerns

The framework is designed around a clear "Create -> Check -> Fix -> Compile" model. This separation of concerns ensures that each workflow is simple, predictable, and safe.

{{grouped_figure:docs/diagrams/logic_workflow_chooser.mmd | scale=2.5 | width=50% | caption=Choosing the Right Workflow: A guide for experiment and study tasks.}}

-   **`new_experiment.ps1` (Create)**: Use this to create a new experiment from scratch for a single experimental condition.

-   **`audit_experiment.ps1` (Check - Experiment)**: Use this read-only tool to get a detailed status report on any existing experiment. It is your primary diagnostic tool.

-   **`fix_experiment.ps1` (Fix & Update)**: Use this for any experiment with a fixable error, such as an interrupted run. This is the main "fix-it" tool for common issues.

-   **`audit_study.ps1` (Check - Study)**: Use this read-only tool to get a consolidated status report on all experiments in a study directory before final analysis.

-   **`compile_study.ps1` (Compile)**: After creating and validating all experiments, use this script to aggregate the results and run the final statistical analysis.

## Core Workflows

The project is orchestrated by several PowerShell wrapper scripts that handle distinct user workflows.

### Creating a New Experiment (`new_experiment.ps1`)

This is the entry point for creating a new experiment from scratch. It reads `config.ini`, generates a timestamped directory, and runs the full batch.

```powershell
# Create and run a new experiment
.\new_experiment.ps1 -Verbose
```

### Auditing an Experiment (`audit_experiment.ps1`)

This is the primary diagnostic tool for an experiment. It performs a read-only check and provides a detailed status report.

```powershell
# Get a status report for an existing experiment
.\audit_experiment.ps1 -ExperimentDirectory "output/new_experiments/experiment_20250910_062305"
```

### Fixing or Updating an Experiment (`fix_experiment.ps1`)

This is the main "fix-it" tool for an existing experiment. It automatically diagnoses and fixes issues.

**To automatically fix a broken or incomplete experiment:**
The script will run an audit, identify the problem (e.g., missing responses, outdated analysis), and automatically apply the correct fix.
```powershell
# Automatically fix a broken experiment
.\fix_experiment.ps1 -ExperimentDirectory "output/new_experiments/experiment_20250910_062305"
```

**To interactively force an action on a valid experiment:**
If you run the script on a complete and valid experiment, it will present an interactive menu allowing you to force a full repair, an analysis update, or a re-aggregation of results.

```powershell
# Run on a valid experiment to bring up the interactive force menu
.\fix_experiment.ps1 -ExperimentDirectory "output/new_experiments/experiment_20250910_062305"
```

### Auditing a Study (`audit_study.ps1`)

This is the main diagnostic tool for a study. It performs a comprehensive, read-only audit of all experiments in a study directory and provides a consolidated summary report with a final recommendation.

```powershell
# Get a status report for an entire study
.\audit_study.ps1 -StudyDirectory "output/studies/My_First_Study"
```

### Compiling a Study (`compile_study.ps1`)

This script orchestrates the entire compilation and analysis workflow for a study. It audits, compiles, and performs the final statistical analysis on all experiments.

**Important:** This script begins with a robust pre-flight check by calling `audit_study.ps1`. If the audit reveals that any experiment is not `VALIDATED`, or that the study is already `COMPLETE`, the process will halt with a detailed report and a clear recommendation. This guarantees that analysis is only performed on a complete and ready set of data.

For organizational purposes, one would typically move all experiment folders belonging to a single study into a common directory (e.g., `output/studies/My_First_Study/`).

**To run the compilation and analysis:**
Point the script at the top-level directory containing all relevant experiment folders. It will provide a clean, high-level summary of its progress.

```powershell
# Example: Compile and analyze all experiments in the "My_First_Study" directory
.\compile_study.ps1 -StudyDirectory "output/studies/My_First_Study"
```
For detailed, real-time logs, add the `-Verbose` switch.

**Final Artifacts:**
The script generates two key outputs:

1.  A master `STUDY_results.csv` file in your study directory, containing the aggregated data from all experiments.
2.  A new `anova/` subdirectory containing the final analysis:
    *   `STUDY_analysis_log.txt`: A comprehensive text report of the statistical findings.
    *   `boxplots/`: Publication-quality plots visualizing the results.
    *   `diagnostics/`: Q-Q plots for checking statistical assumptions.

---

## Standardized Output

The pipeline generates a consistent, standardized `replication_report.txt` for every run, whether it's a new, an updated (reprocessed), or migrated experiment. This ensures that all output is easily comparable and machine-parsable.

### Replication Report Format

Each report contains a clear header, the base query used, a human-readable analysis summary, and a machine-readable JSON block with all calculated metrics.

{{diagram:docs/diagrams/format_replication_report.txt}}

**Date Handling by Mode:**

-   **Normal Mode**: The report title is `REPLICATION RUN REPORT` and the `Date` field shows the time of the original run.
-   **`--reprocess` Mode**: The report title is `REPLICATION RUN REPORT (YYYY-MM-DD HH:MM:SS)` with the reprocessing timestamp. The `Date` field continues to show the time of the **original** run for clear traceability.

### Study Analysis Log Format

The final analysis script (`analyze_study_results.py`) produces a comprehensive log file detailing the full statistical analysis of the entire study. The report is structured by metric, with each section providing descriptive statistics, the ANOVA summary, post-hoc results (if applicable), and performance groupings.

{{diagram:docs/diagrams/format_analysis_log.txt}}

---

## Key Data Formats

This section provides a reference for the structure of the most important data files used and generated by the framework.

### Primary Data Source

{{diagram:docs/diagrams/format_data_adb_raw_export.txt | caption=Format for `adb_raw_export.txt`}}

### Manual Process I/O

{{diagram:docs/diagrams/format_data_sf_chart_export.txt | caption=Format for `sf_chart_export.csv`}}

### Core Integrated & Final Databases

{{diagram:docs/diagrams/format_data_subject_db.txt | caption=Format for `subject_db.csv`}}

{{diagram:docs/diagrams/format_data_personalities_db.txt | caption=Format for `personalities_db.txt`}}

### Algorithm Configuration & Text Libraries

{{diagram:docs/diagrams/format_data_point_weights.txt | caption=Format for `point_weights.csv`}}

{{diagram:docs/diagrams/format_data_balance_thresholds.txt | caption=Format for `balance_thresholds.csv`}}

{{diagram:docs/diagrams/format_data_neutralized_text_example.txt | caption=Format for Neutralized Text Library Files (e.g., `points_in_signs.csv`, `balance_elements.csv`)}}

{{diagram:docs/diagrams/format_data_sf_delineations_library.txt | caption=Format for the Source Delineation Library (`sf_delineations_library.txt`)}}

### Intermediate Data Artifacts

{{diagram:docs/diagrams/format_data_adb_wiki_links.txt | caption=Format for `adb_wiki_links.csv`}}

{{diagram:docs/diagrams/format_data_adb_validation_report.txt | caption=Format for `adb_validation_report.csv`}}

{{diagram:docs/diagrams/format_data_adb_eligible_candidates.txt | caption=Format for `adb_eligible_candidates.txt`}}

{{diagram:docs/diagrams/format_data_eminence_scores.txt | caption=Format for `eminence_scores.csv`}}

{{diagram:docs/diagrams/format_data_ocean_scores.txt | caption=Format for `ocean_scores.csv`}}

{{diagram:docs/diagrams/format_data_adb_final_candidates.txt | caption=Format for `adb_final_candidates.txt`}}

{{diagram:docs/diagrams/format_data_sf_import.txt | caption=Format for `sf_data_import.txt`}}

---

## Testing

The project includes a comprehensive test suite managed by PDM scripts, which provides shortcuts for running tests with and without code coverage. Several integration tests offer interactive modes that provide guided tours of the framework's capabilities.

### Automated CI Checks

The project uses a GitHub Actions workflow for Continuous Integration (CI). On every push or pull request, it automatically runs a series of checks on Windows, Linux, and macOS to ensure code quality and consistency. This includes:

-   Linting all source files for correct formatting and headers.
-   Verifying that the documentation is up-to-date.

This ensures that the main branch is always stable and that all contributions adhere to the project's standards.

### Running the Test Suite

-   **To run all tests (Python and PowerShell) at once:**
    ```bash
    pdm run test
    ```
-   **To run only the PowerShell script tests:**
    ```bash
    pdm run test-ps-all
    ```
    You can also test individual PowerShell scripts (e.g., `pdm run test-ps-exp`, `pdm run test-ps-stu`).

### Code Coverage

The test suite is configured for detailed code coverage analysis using the `coverage` package.

-   **To run all tests and view a coverage report in the console:**
    ```bash
    pdm run cov
    ```
-   **To generate a detailed HTML coverage report (saved to `htmlcov/`):**
    ```bash
    pdm run cov-html
    ```
    Open `htmlcov/index.html` in your browser to explore the report.

### Statistical Validation

The framework provides external validation against GraphPad Prism 10.6.1 for academic publication.

For complete validation procedures, see the **[Statistical Analysis & Reporting Validation section in the Testing Guide](TESTING_GUIDE.md#statistical-analysis--reporting-validation)**.

**Academic Citation:** "Statistical analyses were validated against GraphPad Prism 10.6.1"