# A Framework for Testing Complex Narrative Systems

This document is the **Framework Manual** for the project. It provides a comprehensive technical guide to the framework, which is designed for the resilient and reproducible testing of large-scale LLM experiments with complex narrative systems. It offers a fully automated, end-to-end pipeline that manages the entire experimental lifecycle, from data preparation and query generation to LLM interaction, response parsing, hierarchical data aggregation, and final statistical analysis.

This manual is intended for developers, contributors, and researchers who wish to understand the system's architecture or use the framework for several types of scientific validation, including **direct, methodological, and conceptual replication, as well as for new research**.

{{toc}}

{{grouped_figure:docs/diagrams/arch_project_overview.mmd | scale=2.5 | width=100% | caption=Project Architecture: A high-level overview of the project's main functional components and their relationships.}}

## Research Question
At its core, this project investigates whether a Large Language Model (LLM) can solve a complex matching task: given a set of sanitized, narrative personality descriptions (derived from birth data) and a corresponding set of general biographical profiles, can the LLM correctly pair them at a rate significantly greater than chance?

This study introduces a novel methodological twist to probe the limits of LLM pattern recognition, as illustrated below. The link between the narrative descriptions and the biographical profiles is a faint, systematic signal generated by a deterministic, esoteric system (an astrology program). This transforms the experiment into a rigorous test of an LLM's ability to detect subtle, rule-based patterns within a noisy, high-dimensional dataset. The central question is not about the validity of the generating system, but about the capability of the AI to find a signal in its output.

{{grouped_figure:docs/diagrams/logic_matching_task.mmd | scale=2.5 | width=60% | caption=The Core Research Question: An LLM-driven matching task to detect a non-random signal.}}

## A Note on Stimulus Generation and Experimental Design
The experiment is built upon a custom database of 4,954 famous historical individuals, for whom accurate and verified birth data (date, time, place) was meticulously collected. This population was chosen for two reasons:

*   **Signal Integrity**: Accurate birth data ensures the deterministic generation of consistent personality narratives.
*   **Task Feasibility**: The public prominence of these individuals makes it plausible that LLMs have encountered their biographical information during training, making the matching task tractable.

To create a uniquely challenging test, we employed a multi-step, deterministic process to generate the textual stimuli, organized into four main stages:

1.  **Data Sourcing & Candidate Qualification:** A `Raw Subject Database` of 10,619 famous individuals was derived from the Astro-Databank (ADB) and subjected to a rigorous, deterministic filtering pass to create a high-quality cohort of 7,234 "eligible candidates."
2.  **LLM-based Candidate Selection:** This eligible cohort was then processed by two LLM-driven scripts (`generate_eminence_scores.py`, `generate_ocean_scores.py`) that scored every subject for historical eminence and personality diversity. The final selection is then performed by a third script (`select_final_candidates.py`), which applies a data-driven cutoff based on score variance to determine the final subject pool.
3.  **Manual Data Processing:** The final subject list was processed by a commercial astrology program (Solar Fire) to calculate the precise celestial positions for each person.
4.  **Profile Generation:** A custom Python script (`generate_personalities_db.py`) then processed this celestial data. Using a `Neutralized Component Library` of pre-written sentences, the script deterministically assembled a unique personality narrative for each individual based on a validated **personality assembly algorithm**.
    - The script's algorithm loads its core logicâ€”point weights and balance thresholdsâ€”from external data files (`point_weights.csv`, `balance_thresholds.csv`). It uses these to calculate weighted scores for various astrological factors (elements, modes, quadrants, etc.) and classify them as 'strong' or 'weak' based on their prominence.
    - These classifications, along with simple placements, serve as keys to look up and combine the corresponding neutralized descriptions from the component library. **The personality assembly algorithm has been rigorously validated against a ground-truth dataset generated by the source Solar Fire software, ensuring its output is bit-for-bit identical.**

{{grouped_figure:docs/diagrams/flow_stimulus_generation.mmd | scale=2.5 | width=40% | caption=The four-stage process for generating the experimental stimuli.}}

The result is a clean dataset of personality profiles where the connection to an individual's biographical profile is systematic but non-obvious.

**Crucially, this study does not seek to validate astrology.** Instead, it treats the generating program as an arbitrary, complex algorithm. The scientific objective is to determine whether an LLM, a third-party pattern-recognition system, can detect the subtle statistical regularities produced by this algorithm and use them to perform a successful matching task. The findings speak to the profound capabilities of LLMs to find signal in noisy, high-dimensional data, regardless of the source's theoretical basis.

## Data Preparation Pipeline

The data preparation pipeline is a fully automated, multi-stage workflow. It begins with data extraction from the live Astro-Databank website and concludes with the generation of the final `personalities_db.txt` file used in the experiments.

**Replication Paths:** Researchers can approach this project in several ways to validate and extend the findings, as illustrated below.

{{grouped_figure:docs/diagrams/flow_replication_paths.mmd | scale=2.5 | width=100% | caption=The Three Research Replication Paths.}}

1.  **Direct Replication (Computational Reproducibility):** To verify that the framework produces the exact findings reported in the article, clone this repository and use the static data and randomization seeds as provided. This is a bit-for-bit validation of the original results.

2.  **Methodological Replication (Testing Robustness):** To validate that the findings are robust and not an artifact of a specific dataset or randomization seed, use the framework as-is but vary the inputs. This can be done in two ways:
    *   **With a New Dataset:** Run the full `prepare_data.ps1` pipeline to generate a fresh dataset from the live Astro-Databank. This tests the statistical robustness of the method on a new sample.
    *   **With New Randomization:** Use the provided static dataset but specify a different set of randomization seeds in `config.ini`. This validates the stability of the results across different random permutations.

3.  **Conceptual Replication (Extending the Research):** To test the underlying scientific concepts with an improved or modified method, researchers can alter the framework itself. This could involve using a different LLM, modifying the analysis scripts, or changing other core parameters to conduct new research built upon this study's foundation.

#### The Automated Workflow

The automated data preparation pipeline is orchestrated by a single, intelligent PowerShell script: `prepare_data.ps1`. This is the recommended method for both initial runs and for resuming interrupted processes.

**Execution:**
```powershell
# Run the entire data preparation pipeline interactively
.\prepare_data.ps1

# Force a full re-run, deleting all existing data
.\prepare_data.ps1 -Force

# Get a read-only status report of the pipeline's progress
.\prepare_data.ps1 -ReportOnly
```
> **Warning on Using `-Force`**: The `-Force` flag triggers a full, destructive re-run of the entire pipeline. It backs up and deletes all existing data, re-downloads the full raw dataset, and re-runs all expensive LLM scoring steps. This process is very time-consuming and will incur API costs.
> **Note on Learning the Pipeline:** A step-by-step "guided tour" of this workflow is available as part of the project's testing harness. This is an excellent way for new users to learn how the pipeline works. See the **[ðŸ§ª Testing Guide (TESTING_GUIDE.md)](../TESTING_GUIDE.md)** for details on running the Layer 3 Interactive Mode.

The script is fully resumable. It automatically detects which steps have already been completed and picks up from the first missing data artifact, ensuring a smooth and efficient workflow.

#### Individual Script Details

The `prepare_data.ps1` script orchestrates a sequence of individual Python scripts followed by a manual processing step. The following is a detailed breakdown of this underlying workflow.

##### Stage 1: Data Sourcing

This stage uses `fetch_adb_data.py` to create the initial raw dataset by querying the live Astro-Databank with a specific set of pre-filters. It performs two crucial transformations at the source:
-   **Identifier Standardization:** It replaces the unstable, temporary record number (`ARN`) with a file-specific sequential `Index`, and renames the permanent Astro-Databank ID (`ADBNo`) to `idADB`.
-   **Timezone Calculation:** It immediately processes the raw timezone code from the API into the final `ZoneAbbr` and `ZoneTimeOffset` values required by downstream software.

The query only includes subjects who meet all of the following criteria:
-   **High-Quality Birth Data:** The record must have a Rodden Rating of 'A' or 'AA', indicating the birth time is from a reliable source.
-   **Deceased Individuals:** Inclusion in the **Personal > Death** category.
-   **Eminence:** Inclusion in the **Notable > Famous > Top 5% of Profession** category.

##### Stage 2: Candidate Qualification

This stage performs a rigorous, deterministic filtering pass on the raw data to create a high-quality cohort of "eligible candidates."

1.  **Link Finding (`find_wikipedia_links.py`):** This script takes the raw ADB export and finds the best-guess English Wikipedia URL for each subject. It scrapes the ADB page and uses a Wikipedia search as a fallback. The output is an intermediate file, `adb_wiki_links.csv`.
2.  **Page Validation (`validate_wikipedia_pages.py`):** This script takes the list of found links and performs an intensive content validation on each page. It handles redirects, resolves disambiguation pages, validates the subject's name, and confirms their death date. The final output is the detailed `adb_validation_report.csv`.
3.  **Final Filtering (`select_eligible_candidates.py`):** This script integrates the raw data with the Wikipedia validation report and applies the following additional criteria in order:

| Criteria | Rule | Purpose |
| :--- | :--- | :--- |
| **Wikipedia Validation** | `Status` must be `OK` | Ensures a valid English Wikipedia page was found where the name and death date were confirmed. |
| **Entry Type** | `EntryType` must be `Person` | Filters out non-person records (e.g., events, research entries). |
| **Birth Year Range** | Must be between 1900-1999 | Controls for cohort-specific confounds by ensuring a homogenous historical period. |
| **Hemisphere** | `Latitude` must contain 'N' | Controls for the potential confound of a zodiacal shift for Southern Hemisphere births. |
| **Valid Time Format** | Birth time must be present and `HH:MM` | Ensures data is complete for precise calculations. |
| **Deduplication** | Unique Name + Birth Date | Removes duplicate entries from the source database. |
    
    The final output of this stage is the `adb_eligible_candidates.txt` file.

##### Stage 3: LLM-based Candidate Selection (Optional)

This stage is a second, optional filtering pass that uses LLMs to score the "eligible candidates" to determine the final, smaller subject pool. This entire stage can be skipped by setting `bypass_candidate_selection = true` in `config.ini`.

1.  **Eminence Scoring (`generate_eminence_scores.py`):** Processes the eligible candidates list to generate a calibrated eminence score for each, producing a rank-ordered list that now includes `BirthYear`.
2.  **OCEAN Scoring (`generate_ocean_scores.py`):** A fully automated, resilient script that generates OCEAN personality scores for every subject in the eminence-ranked list.
3.  **Final Selection & Cutoff (`select_final_candidates.py`):** Performs the final filtering and selection. It takes the complete OCEAN scores list and applies a sophisticated, data-driven algorithm to determine the optimal cohort size. The script first calculates the cumulative personality variance curve for the entire cohort, smooths this curve using a moving average to eliminate local noise, and then performs a slope analysis to find the "plateau"â€”the point of diminishing returns where adding more subjects no longer meaningfully contributes to the psychological diversity of the pool. It then resolves country codes and sorts the final list by eminence.

##### Stage 4: Profile Generation

This is the final stage, which assembles the personality profiles for the selected candidates. It involves a mix of automated and manual steps.

1.  **Formatting (`prepare_sf_import.py`):** Formats the final subject list for import into the Solar Fire software. It performs a critical data integrity step by encoding each subject's unique `idADB` into a Base58 string and injecting it into the `ZoneAbbr` field. This technique allows a unique identifier to pass through the manual software step unharmed, ensuring a perfect data merge later. The `ZoneAbbr` field was specifically chosen as it is a text field that is passed through the software without modification and is not used in any astrological calculations, making it a safe channel for this data.
2.  **Manual Processing (Solar Fire):** The formatted file is imported into Solar Fire, which calculates the required celestial data and exports it as `sf_chart_export.csv`.
3.  **Neutralization (`neutralize_delineations.py`):** This script uses a powerful hybrid strategy to rewrite the esoteric source texts.
    *   **Fast Mode (`--fast`):** For initial runs, this mode bundles tasks into large, high-speed API calls (e.g., all 12 "Sun in Signs" delineations at once). This is highly efficient but may fail on some large tasks.
    *   **Robust/Resume Mode (default):** For resuming or fixing failed runs, this mode processes each of the 149 delineations as a separate, atomic task. This granular approach is slower but guarantees completion by solving potential response truncation issues from the LLM.
4.  **Integration (`create_subject_db.py`):** Bridges the manual step by reading the Solar Fire chart export, decoding the unique `idADB` from the `ZoneAbbr` field, and merging the chart data with the final subject list to produce a clean master database.
5.  **Generation (`generate_personalities_db.py`):** Assembles the final `personalities_db.txt` by combining the subject data with the neutralized delineation library according to a deterministic algorithm.

This combination of automated scripts and well-defined manual steps ensures the final dataset is both high-quality and computationally reproducible.

The pipeline can be understood through the following architecture, workflow, data flow, and logic diagrams.

### Data Preparation: Architecture

This diagram provides a map of the scripts in the data preparation pipeline, showing how they are orchestrated and which utilities they share.

{{grouped_figure:docs/diagrams/arch_prep_codebase.mmd | scale=2.5 | width=100% | caption=Data Preparation Code Architecture: The execution flow of the data processing scripts.}}

### Data Preparation: Workflow

This diagram shows the high-level, multi-stage workflow for the entire data preparation pipeline, including both automated and manual processes.

{{grouped_figure:docs/diagrams/flow_prep_pipeline.mmd | scale=2.5 | width=35% | caption=Data Preparation Workflow: The end-to-end pipeline from raw data extraction to the final generated databases, showing both manual and automated steps.}}

### Data Preparation: Data Flow

These diagrams show the sequence of data artifacts (files) created and transformed by the pipeline scripts at each major stage.

{{grouped_figure:docs/diagrams/flow_prep_1_qualification.mmd | scale=2.5 | width=75% | caption=Data Prep Flow 1: Data Sourcing and Candidate Qualification.}}

{{grouped_figure:docs/diagrams/flow_prep_2_selection.mmd | scale=2.5 | width=80% | caption=Data Prep Flow 2: LLM-based Candidate Selection.}}

{{grouped_figure:docs/diagrams/flow_prep_3_generation.mmd | scale=2.5 | width=100% | caption=Data Prep Flow 3: Profile Generation.}}

### Data Preparation: Logic

These diagrams illustrate the internal decision-making logic and control flow of each script in the data preparation pipeline.

{{grouped_figure:docs/diagrams/logic_prep_pipeline.mmd | scale=2.5 | width=50% | caption=Overall Logic for the Data Preparation Pipeline: A high-level view of the four main stages.}}

{{grouped_figure:docs/diagrams/logic_prep_find_links.mmd | scale=2.5 | width=70% | caption=Logic for Link Finding (`find_wikipedia_links.py`): The algorithm for finding Wikipedia URLs by scraping ADB and using a Wikipedia search fallback.}}

{{grouped_figure:docs/diagrams/logic_prep_validate_pages.mmd | scale=2.5 | width=80% | caption=Logic for Page Validation (`validate_wikipedia_pages.py`): The algorithm for validating Wikipedia page content, including redirect and disambiguation handling.}}

{{grouped_figure:docs/diagrams/logic_prep_eligible_candidates.mmd | scale=2.5 | width=65% | caption=Logic for Final Filtering (`select_eligible_candidates.py`): The algorithm for applying all deterministic data quality rules to create the final "eligible candidates" list.}}

{{grouped_figure:docs/diagrams/logic_prep_eminence_scoring.mmd | scale=2.5 | width=60% | caption=Logic for Eminence Scoring (`generate_eminence_scores.py`): The algorithm for batch processing, LLM interaction, and finalization of eminence scores.}}

{{grouped_figure:docs/diagrams/logic_prep_ocean_scoring.mmd | scale=2.5 | width=55% | caption=Logic for OCEAN Scoring (`generate_ocean_scores.py`): The algorithm for generating OCEAN personality scores for all eligible subjects. A robust pre-flight check ensures that interrupted runs can be safely resumed.}}

{{grouped_figure:docs/diagrams/logic_prep_final_candidates.mmd | scale=2.5 | width=35% | caption=Logic for Final Selection (`select_final_candidates.py`): The algorithm for finding the optimal cohort size by performing a slope analysis on a smoothed cumulative personality variance curve.}}

{{grouped_figure:docs/diagrams/logic_prep_neutralization.mmd | scale=2.5 | width=60% | caption=Logic for Delineation Neutralization (`neutralize_delineations.py`): The hybrid algorithm for rewriting texts. Fast mode bundles tasks for speed, while the robust default mode processes each item individually to guarantee completion.}}

{{grouped_figure:docs/diagrams/logic_prep_generation.mmd | scale=2.5 | width=70% | caption=Logic for Database Generation (`generate_personalities_db.py`): The algorithm for assembling the final description text for each subject.}}

## Experiment Lifecycle & Analysis

### Key Features

-   **Automated Batch Execution**: The `experiment_manager.py` script, driven by a simple PowerShell wrapper, manages entire experimental batches. It can run hundreds of replications, intelligently skipping completed ones to resume interrupted runs, and provides real-time progress updates, including a detailed spinner showing individual trial timers and overall replication batch ETA.
-   **Powerful Reprocessing Engine**: The manager's `--reprocess` mode allows for re-running the data processing and analysis stages on existing results without repeating expensive LLM calls. This makes it easy to apply analysis updates or bug fixes across an entire experiment.
-   **Guaranteed Reproducibility**: On every new run, the `config.ini` file is automatically archived in the run's output directory, permanently linking the results to the exact parameters that generated them.
-   **Standardized, Comprehensive Reporting**: Each replication produces a `replication_report.txt` file containing run parameters, status, a human-readable statistical summary, and a machine-parsable JSON block with all key metrics. This format is identical for new runs and reprocessed runs.
-   **Hierarchical Analysis & Aggregation**: The pipeline uses a set of dedicated compiler scripts for a fully auditable, bottom-up aggregation of results. `compile_replication_results.py` creates a summary for each run, `compile_experiment_results.py` combines those into an experiment-level summary, and finally `compile_study_results.py` creates a master `STUDY_results.csv` for the entire study.
-   **Comprehensive Self-Healing Capabilities**: The framework can automatically detect and repair multiple types of experiment corruption, including missing response files, corrupted analysis data, damaged configuration files, and malformed reports. The audit system classifies corruption severity and applies appropriate repair strategies, ensuring data integrity even after network interruptions, storage errors, or process crashes.
-   **Standardized Console Banners**: All audit results, whether for success, failure, or a required update, are now presented in a consistent, easy-to-read, 4-line colored banner, providing clear and unambiguous status reports.
-   **Streamlined ANOVA Workflow**: The final statistical analysis is a simple two-step process. `compile_study_results.py` prepares a master dataset, which `analyze_study_results.py` then automatically analyzes to generate tables and publication-quality plots using user-friendly display names defined in `config.ini`.

### Visual Architecture

The main pipeline's architecture can be understood through four different views: the code architecture, the workflows, the data flow, and the experimental logic.

#### Code Architecture Diagram
The codebase for the experiment lifecycle and analysis is organized into a clear hierarchy:

1.  **Main User Entry Points**: User-facing PowerShell scripts (`.ps1`) that orchestrate high-level workflows like creating, auditing, or fixing experiments and studies.
2.  **Experiment Lifecycle Management**: The core Python backend for managing a single experiment. This includes primary orchestrators (`experiment_manager.py`, `experiment_auditor.py`) and dedicated finalization scripts (`manage_experiment_log.py`, `compile_experiment_results.py`).
3.  **Single Replication Pipeline**: A set of scripts, managed by `replication_manager.py`, that execute the end-to-end process for a single run, from query generation to final reporting.
4.  **Study-Level Analysis**: Python scripts that operate on the outputs of multiple experiments to perform study-wide aggregation and statistical analysis.
5.  **Utility & Other Scripts**: Shared modules and standalone utility scripts that provide common functionality (e.g., `config_loader.py`) or perform auxiliary tasks.

{{grouped_figure:docs/diagrams/arch_main_codebase.mmd | scale=2.5 | width=100% | caption=Codebase Architecture: A comprehensive map of the entire Python codebase. PowerShell scripts (blue) are user-facing entry points that execute core Python logic. Solid lines indicate execution, while dotted lines show module imports.}}

#### Workflow Diagrams
The framework's functionality is organized into a clear hierarchy of workflows, initiated by dedicated PowerShell scripts.

**Experiment-Level Workflows:**
-   **Create a New Experiment (`new_experiment.ps1`):** The primary workflow for generating new experimental data for a single condition.
-   **Audit an Experiment (`audit_experiment.ps1`):** A read-only diagnostic tool that provides a detailed completeness report for an experiment.
-   **Fix or Update an Experiment (`fix_experiment.ps1`):** The main "fix-it" tool for resuming interrupted runs or reapplying analysis updates to existing data.

**Study-Level Workflows:**
-   **Audit a Study (`audit_study.ps1`):** A read-only diagnostic tool that provides a consolidated audit of all experiments in a study directory.
-   **Compile a Study (`compile_study.ps1`):** The final step in the research process. This script aggregates data from all experiments in a study, runs the statistical analysis, and generates the final reports and plots.

#### Workflow 1: Create a New Experiment

This is the primary workflow for generating new experimental data. The PowerShell entry point (`new_experiment.ps1`) calls the Python batch controller (`experiment_manager.py`). The manager creates a new, timestamped directory and runs the full set of replications from scratch.

The `replication_manager.py` script executes the full pipeline for a single run, which is broken into six distinct stages:

1.  **Build Queries**: Generates all necessary query files and trial manifests.
2.  **Run LLM Sessions**: Interacts with the LLM API in parallel to get responses.
3.  **Process LLM Responses**: Parses the raw text responses from the LLM into structured score files.
4.  **Analyze LLM Performance**: A unified two-part process that first calculates core performance metrics and then injects diagnostic bias metrics.
5.  **Generate Final Report**: Assembles the final `replication_report.txt` from the analysis results and captured logs.
6.  **Create Replication Summary**: Creates the final `REPLICATION_results.csv`, marking the run as valid.

{{grouped_figure:docs/diagrams/flow_main_1_new_experiment.mmd | scale=2.5 | width=60% | caption=Workflow 1: Create a New Experiment, showing the main control loop and the internal replication pipeline.}}

#### Workflow 2: Audit an Experiment

This workflow provides a read-only, detailed completeness report for an experiment without performing any modifications. The `audit_experiment.ps1` wrapper calls the dedicated `experiment_auditor.py` script. The full audit report, including subprocess outputs, is also saved to `experiment_audit_log.txt` within the audited directory.

{{grouped_figure:docs/diagrams/flow_main_2_audit_experiment.mmd | scale=2.5 | width=100% | caption=Workflow 2: Audit an Experiment. Provides a read-only, detailed completeness report for an experiment.}}

##### Interpreting the Audit Report
The audit script is the primary diagnostic tool for identifying issues in a failed or incomplete experiment. It uses a simple but robust rule to classify problems: the number of distinct errors found in a single replication run.

**Repairable Issues (Single Error)**
If a replication run has **exactly one** identifiable problem, it is considered safe to repair in-place. The `Status` column will show a specific, targeted error code:

| Status Code | Description | Recommended Action |
| :--- | :--- | :--- |
| **`INVALID_NAME`** | The run directory name is malformed. | Run `fix_experiment.ps1` to repair. |
| **`CONFIG_ISSUE`** | The `config.ini.archived` is missing or inconsistent. | Run `fix_experiment.ps1` to repair. |
| **`QUERY_ISSUE`** | Core query files or manifests are missing. | Run `fix_experiment.ps1` to repair. |
| **`RESPONSE_ISSUE`** | One or more LLM response files are missing. | Run `fix_experiment.ps1` to repair. |
| **`ANALYSIS_ISSUE`** | Core data is present, but analysis files are missing/outdated. | Run `fix_experiment.ps1` to repair. |

Any of these single-error states will result in an overall audit recommendation to run **`fix_experiment.ps1`**.

**Corrupted Runs (Multiple Errors)**
If a replication run has **two or more** distinct problems (e.g., a missing config file *and* missing responses), it is flagged with the status `RUN_CORRUPTED`. A corrupted run indicates a systemic issue that cannot be safely repaired automatically. The audit will recommend investigating the issue manually. Corrupted experiments should generally be deleted and re-run.

The `Details` string provides a semicolon-separated list of all detected issues (e.g., `CONFIG_MISSING; RESPONSE_FILES_INCOMPLETE`).

A key part of this validation is a strict schema check on the `replication_report.txt` file. The audit verifies that the JSON block of metrics contains *exactly* the set of required keysâ€”no more, and no less. A report with missing (`REPORT_INCOMPLETE_METRICS`) or extra, obsolete metrics (`REPORT_UNEXPECTED_METRICS`) will be flagged with an `ANALYSIS_ISSUE`. This ensures that analysis is only ever performed on data with a correct and up-to-date schema.

In addition to the per-replication table, the audit provides an `Overall Summary` that includes the `Experiment Aggregation Status`. This checks for the presence and completeness of top-level summary files (`EXPERIMENT_results.csv`, `experiment_log.csv`), confirming whether the last aggregation step for the experiment was successfully completed.

#### Workflow 3: Fixing or Updating an Experiment

This workflow is the main "fix-it" tool for any existing experiment. The `fix_experiment.ps1` script is an intelligent wrapper. It first performs a full audit by calling `experiment_auditor.py` to diagnose the experiment's state. Based on the audit result, it then calls `experiment_manager.py` to apply the correct repairs.

-   If the audit finds missing data or outdated analysis files, the script proceeds to automatically apply the correct repair.
-   If the audit finds the experiment is already complete and valid, it becomes interactive, presenting a menu that allows the user to force a full data repair, an analysis update, or a simple re-aggregation of results.

{{grouped_figure:docs/diagrams/flow_main_3_fix_experiment.mmd | scale=2.5 | width=100% | caption=Workflow 3: Fixing or Updating an Experiment, showing both automatic and interactive repair paths.}}

#### Workflow 4: Compile a Study

This workflow is used after all experiments are validated to compile, analyze, and evaluate the entire study. It performs a robust pre-flight check by calling `audit_study.ps1`. If the study is not ready for processing (or is already complete), it will halt with a clear recommendation. Otherwise, it proceeds to compile all results and run the final statistical analysis.

{{grouped_figure:docs/diagrams/flow_main_4_compile_study.mmd | scale=2.5 | width=90% | caption=Workflow 4: Compile a Study. Audits, compiles, and analyzes all experiments in a study.}}

#### Workflow 5: Audit a Study

This script is the primary diagnostic tool for assessing the overall state of a study. It performs a two-part, read-only audit:

1.  **Readiness Audit**: It iterates through every experiment folder and runs a quiet, individual audit on each to determine its status (e.g., `VALIDATED`, `NEEDS REPAIR`).
2.  **Completeness Audit**: It verifies the existence of top-level study artifacts, such as `STUDY_results.csv` and the `anova/` analysis directory.

Based on the combined results from both audits, it presents a consolidated summary table and provides a final, context-aware recommendation for the correct next step.

{{grouped_figure:docs/diagrams/flow_main_5_audit_study.mmd | scale=2.5 | width=70% | caption=Workflow 5: Audit a Study. Consolidated completeness report for all experiments in a study.}}

{{pagebreak}}
#### Data Flow Diagram

This diagram shows how data artifacts (files) are created and transformed by the experiment lifecycle and analysis scripts. It traces the flow from initial inputs like `config.ini` and the personalities database, through intermediate query and response files, to the final aggregated results and analysis plots.

{{grouped_figure:docs/diagrams/data_main_flow.mmd | scale=2.5 | width=80% | caption=Data Flow Diagram: Creation and transformation of data artifacts (files) by the experiment lifecycle and analysis scripts.}}

#### Logic Flowcharts

These diagrams illustrate the scientific and procedural methodology at each level of the experimental hierarchy.

{{grouped_figure:docs/diagrams/logic_main_replication.mmd | scale=2.5 | width=60% | caption=Replication Logic: The scientific methodology for a single replication run.}}

{{grouped_figure:docs/diagrams/logic_main_experiment.mmd | scale=2.5 | width=100% | caption=Experiment Logic: The aggregation of multiple replication results to produce final experiment-level summaries.}}

{{grouped_figure:docs/diagrams/logic_main_study.mmd | scale=2.5 | width=100% | caption=Study Logic: The complete workflow for processing a study, from auditing and aggregation to final statistical analysis.}}

## Experimental Hierarchy

The project's experiments are organized in a logical hierarchy:

-   **Study**: The highest-level grouping, representing a major research question (e.g., "Performance on Random vs. Correct Mappings").
-   **Experiment**: A complete set of runs for a single condition within a study (e.g., "Gemini 2.0 Flash with k=10 Subjects").
-   **Replication**: A single, complete run of an experiment, typically repeated 30 times for statistical power.
-   **Trial**: An individual matching task performed within a replication, typically repeated 100 times.

## Directory Structure

This logical hierarchy is reflected in the physical layout of the repository:

{{diagram:docs/diagrams/view_directory_structure.txt | scale=2.5 | width=100%}}

Data dictionaries provide detailed explanations for all files: [ðŸ“ Data Preparation Data Dictionary](../docs/DATA_PREPARATION_DATA_DICTIONARY.md) covers the `data/` directory, and [ðŸ“Š Experiment Lifecycle Data Dictionary](../output/EXPERIMENT_LIFECYCLE_DATA_DICTIONARY.md) covers the `output/` directory structure and experimental results.

## Setup and Installation

This project uses **PDM** for dependency and environment management.

1.  **Install PDM (One-Time Setup)**:
    If you don't have PDM, install it once with pip. It's best to run this from a terminal *outside* of any virtual environment.
    ```bash
    pip install --user pdm
    ```
    > **Note:** If `pdm` is not found in a new terminal, use `python -m pdm` instead.

2.  **Install Project Environment & Dependencies**:
    From the project's root directory, run the main PDM installation command. The `-G dev` flag installs all packages, including the development tools needed to run the test suite.
    ```bash
    pdm install -G dev
    ```
    This command creates a local `.venv` folder and installs all necessary packages into it.

3.  **Configure API Key**:
    *   Create a file named `.env` in the project root.
    *   Add your API key from OpenRouter. The key will start with `sk-or-`.
        `OPENROUTER_API_KEY=your-actual-api-key`

To run any project command, such as the test suite, prefix it with `pdm run`:
```bash
pdm run test
```

> **For Developers:** If you intend to contribute to the project or encounter issues with the simple setup, please see the **[Developer Setup Guide in DEVELOPERS_GUIDE.md](DEVELOPERS_GUIDE.md#getting-started-development-environment-setup)** for more detailed instructions and troubleshooting.

## Configuration (`config.ini`)

The `config.ini` file is the central hub for defining all parameters for your experiments. The pipeline automatically archives this file with the results for guaranteed reproducibility.

| Section | Parameter | Description | Example Value |
| :--- | :--- | :--- | :--- |
| **`[Study]`** | `num_replications` | The number of times the experiment will be repeated (`r`). | `2` |
| | `num_trials` | The number of trials for each replication (`m`). | `3` |
| | `group_size` | The number of subjects in each group (`k`). | `4` |
| | `mapping_strategy` | A key experimental variable; can be `correct` or `random`. | `correct` |
| **`[LLM]`** | `model_name` | The API identifier for the LLM to be tested for the main experiment. | `google/gemini-flash-1.5` |
| | `temperature` | Controls the randomness of the model's output (0-2). | `1` |
| | `max_parallel_sessions` | The number of concurrent API calls to make. | `10` |
| **`[Analysis]`** | `min_valid_response_threshold` | Minimum average valid responses for an experiment to be included in the final analysis. | `25` |
| **`[DataGeneration]`** | `bypass_candidate_selection` | If `true`, skips LLM-based scoring and uses all eligible candidates. | `false` |
| | `cutoff_search_start_point` | The cohort size at which to start searching for the variance curve plateau. | `3500` |
| | `smoothing_window_size` | The window size for the moving average used to smooth the variance curve. | `800` |

#### Model Selection Philosophy and Future Work
The selection of models for this study was guided by a balance of performance, cost, speed, and technical compatibility with the automated framework. Several top-tier models were not included for one of the following reasons:

-   **Prohibitive Cost**: Models like `o1 pro`, `GPT 4.5 Preview`, and `Claude 4 Opus` were excluded as a single experiment (requiring ~3,000 queries) was financially infeasible.

-   **Technical Incompatibility**: Models like `Gemini 2.5 Pro` lacked a "non-thinking" mode, making the automated parsing of a structured response table overly challenging.

-   **Excessive Runtime**: A number of large models, including `Qwen3 235B` and `Llama 3.1 Nemotron Ultra 253B`, were excluded as a full experimental run would take longer than 20 hours.

A follow-up study is planned to evaluate other powerful, medium-cost models as API costs decrease and technical features evolve. Candidates include: `Grok 3`, `Grok 4`, `Claude 4 Sonnet`, `Claude 3.7 Sonnet`, `GPT-4o`, `o3`, `GPT-4.1`, `Mistral Large 2`, `Gemini 1.5 Pro`, and various `o1`/`o3`/`o4` mini-variants.

### Analysis Settings (`[Analysis]`)

-   **`min_valid_response_threshold`**: Minimum average number of valid responses (`n_valid_responses`) for an experiment to be included in the final analysis. Set to `0` to disable.

## Known Issues and Future Work

This framework is under active development. For a detailed and up-to-date list of planned improvements, known issues, and future development tasks, please see the [Project Roadmap](ROADMAP.md).

## Choosing the Right Workflow: Separation of Concerns

The framework is designed around a clear "Create -> Check -> Fix -> Compile" model. This separation of concerns ensures that each workflow is simple, predictable, and safe.

{{grouped_figure:docs/diagrams/logic_workflow_chooser.mmd | scale=2.5 | width=50% | caption=Choosing the Right Workflow: A guide for experiment and study tasks.}}

-   **`new_experiment.ps1` (Create)**: Use this to create a new experiment from scratch for a single experimental condition.

-   **`audit_experiment.ps1` (Check - Experiment)**: Use this read-only tool to get a detailed status report on any existing experiment. It is your primary diagnostic tool.

-   **`fix_experiment.ps1` (Fix & Update)**: Use this for any experiment with a fixable error, such as an interrupted run. This is the main "fix-it" tool for common issues.

-   **`audit_study.ps1` (Check - Study)**: Use this read-only tool to get a consolidated status report on all experiments in a study directory before final analysis.

-   **`compile_study.ps1` (Compile)**: After creating and validating all experiments, use this script to aggregate the results and run the final statistical analysis.

## Core Workflows

The project is orchestrated by several PowerShell wrapper scripts that handle distinct user workflows.

### Creating a New Experiment (`new_experiment.ps1`)

This is the entry point for creating a new experiment from scratch. It reads `config.ini`, generates a timestamped directory, and runs the full batch.

```powershell
# Create and run a new experiment
.\new_experiment.ps1 -Verbose
```

### Auditing an Experiment (`audit_experiment.ps1`)

This is the primary diagnostic tool for an experiment. It performs a read-only check and provides a detailed status report.

```powershell
# Get a status report for an existing experiment
.\audit_experiment.ps1 -ExperimentDirectory "output/new_experiments/experiment_20250910_062305"
```

### Fixing or Updating an Experiment (`fix_experiment.ps1`)

This is the main "fix-it" tool for an existing experiment. It automatically diagnoses and fixes issues.

**To automatically fix a broken or incomplete experiment:**
The script will run an audit, identify the problem (e.g., missing responses, outdated analysis), and automatically apply the correct fix.
```powershell
# Automatically fix a broken experiment
.\fix_experiment.ps1 -ExperimentDirectory "output/new_experiments/experiment_20250910_062305"
```

**To interactively force an action on a valid experiment:**
If you run the script on a complete and valid experiment, it will present an interactive menu allowing you to force a full repair, an analysis update, or a re-aggregation of results.

```powershell
# Run on a valid experiment to bring up the interactive force menu
.\fix_experiment.ps1 -ExperimentDirectory "output/new_experiments/experiment_20250910_062305"
```

### Auditing a Study (`audit_study.ps1`)

This is the main diagnostic tool for a study. It performs a comprehensive, read-only audit of all experiments in a study directory and provides a consolidated summary report with a final recommendation.

```powershell
# Get a status report for an entire study
.\audit_study.ps1 -StudyDirectory "output/studies/My_First_Study"
```

### Compiling a Study (`compile_study.ps1`)

This script orchestrates the entire compilation and analysis workflow for a study. It audits, compiles, and performs the final statistical analysis on all experiments.

**Important:** This script begins with a robust pre-flight check by calling `audit_study.ps1`. If the audit reveals that any experiment is not `VALIDATED`, or that the study is already `COMPLETE`, the process will halt with a detailed report and a clear recommendation. This guarantees that analysis is only performed on a complete and ready set of data.

For organizational purposes, one would typically move all experiment folders belonging to a single study into a common directory (e.g., `output/studies/My_First_Study/`).

**To run the compilation and analysis:**
Point the script at the top-level directory containing all relevant experiment folders. It will provide a clean, high-level summary of its progress.

```powershell
# Example: Compile and analyze all experiments in the "My_First_Study" directory
.\compile_study.ps1 -StudyDirectory "output/studies/My_First_Study"
```
For detailed, real-time logs, add the `-Verbose` switch.

**Final Artifacts:**
The script generates two key outputs:

1.  A master `STUDY_results.csv` file in your study directory, containing the aggregated data from all experiments.
2.  A new `anova/` subdirectory containing the final analysis:
    *   `STUDY_analysis_log.txt`: A comprehensive text report of the statistical findings.
    *   `boxplots/`: Publication-quality plots visualizing the results.
    *   `diagnostics/`: Q-Q plots for checking statistical assumptions.

---

## Standardized Output

The pipeline generates a consistent, standardized `replication_report.txt` for every run, whether it's a new, an updated (reprocessed), or migrated experiment. This ensures that all output is easily comparable and machine-parsable.

### Replication Report Format

Each report contains a clear header, the base query used, a human-readable analysis summary, and a machine-readable JSON block with all calculated metrics.

{{diagram:docs/diagrams/format_replication_report.txt}}

**Date Handling by Mode:**

-   **Normal Mode**: The report title is `REPLICATION RUN REPORT` and the `Date` field shows the time of the original run.
-   **`--reprocess` Mode**: The report title is `REPLICATION RUN REPORT (YYYY-MM-DD HH:MM:SS)` with the reprocessing timestamp. The `Date` field continues to show the time of the **original** run for clear traceability.

### Study Analysis Log Format

The final analysis script (`analyze_study_results.py`) produces a comprehensive log file detailing the full statistical analysis of the entire study. The report is structured by metric, with each section providing descriptive statistics, the ANOVA summary, post-hoc results (if applicable), and performance groupings.

{{diagram:docs/diagrams/format_analysis_log.txt}}

---

## Key Data Formats

This section provides a reference for the structure of the most important data files used and generated by the framework.

### Primary Data Source

{{diagram:docs/diagrams/format_data_adb_raw_export.txt | caption=Format for `adb_raw_export.txt`}}

### Manual Process I/O

{{diagram:docs/diagrams/format_data_sf_chart_export.txt | caption=Format for `sf_chart_export.csv`}}

### Core Integrated & Final Databases

{{diagram:docs/diagrams/format_data_subject_db.txt | caption=Format for `subject_db.csv`}}

{{diagram:docs/diagrams/format_data_personalities_db.txt | caption=Format for `personalities_db.txt`}}

### Algorithm Configuration & Text Libraries

{{diagram:docs/diagrams/format_data_point_weights.txt | caption=Format for `point_weights.csv`}}

{{diagram:docs/diagrams/format_data_balance_thresholds.txt | caption=Format for `balance_thresholds.csv`}}

{{diagram:docs/diagrams/format_data_neutralized_text_example.txt | caption=Format for Neutralized Text Library Files (e.g., `points_in_signs.csv`, `balance_elements.csv`)}}

{{diagram:docs/diagrams/format_data_sf_delineations_library.txt | caption=Format for the Source Delineation Library (`sf_delineations_library.txt`)}}

### Intermediate Data Artifacts

{{diagram:docs/diagrams/format_data_adb_wiki_links.txt | caption=Format for `adb_wiki_links.csv`}}

{{diagram:docs/diagrams/format_data_adb_validation_report.txt | caption=Format for `adb_validation_report.csv`}}

{{diagram:docs/diagrams/format_data_adb_eligible_candidates.txt | caption=Format for `adb_eligible_candidates.txt`}}

{{diagram:docs/diagrams/format_data_eminence_scores.txt | caption=Format for `eminence_scores.csv`}}

{{diagram:docs/diagrams/format_data_ocean_scores.txt | caption=Format for `ocean_scores.csv`}}

{{diagram:docs/diagrams/format_data_adb_final_candidates.txt | caption=Format for `adb_final_candidates.txt`}}

{{diagram:docs/diagrams/format_data_sf_import.txt | caption=Format for `sf_data_import.txt`}}

---

## Testing

The project includes a comprehensive test suite managed by PDM scripts, which provides shortcuts for running tests with and without code coverage. Several integration tests offer interactive modes that provide guided tours of the framework's capabilities.

### Automated CI Checks

The project uses a GitHub Actions workflow for Continuous Integration (CI). On every push or pull request, it automatically runs a series of checks on Windows, Linux, and macOS to ensure code quality and consistency. This includes:

-   Linting all source files for correct formatting and headers.
-   Verifying that the documentation is up-to-date.

This ensures that the main branch is always stable and that all contributions adhere to the project's standards.

### Running the Test Suite

-   **To run all tests (Python and PowerShell) at once:**
    ```bash
    pdm run test
    ```
-   **To run only the PowerShell script tests:**
    ```bash
    pdm run test-ps-all
    ```
    You can also test individual PowerShell scripts (e.g., `pdm run test-ps-exp`, `pdm run test-ps-stu`).

### Code Coverage

The test suite is configured for detailed code coverage analysis using the `coverage` package.

-   **To run all tests and view a coverage report in the console:**
    ```bash
    pdm run cov
    ```
-   **To generate a detailed HTML coverage report (saved to `htmlcov/`):**
    ```bash
    pdm run cov-html
    ```
    Open `htmlcov/index.html` in your browser to explore the report.
