# Experiment & Study Lifecycle Guide

This document provides a high-level guide to the framework's core research workflow. Its purpose is to explain how the main user-facing scripts are used together to create, validate, and analyze a study.

## Project Architecture

The experiment and study lifecycle scripts represent the "Core Logic" of the framework. They are executed by user-facing PowerShell scripts and operate on the data generated by the Data Preparation Pipeline. The framework includes interactive testing modes that provide guided tours of these workflows.

{{grouped_figure:docs/diagrams/arch_project_overview.mmd | scale=2.5 | width=100% | caption=Project Architecture: The lifecycle scripts orchestrate the core research process.}}

## Experimental Hierarchy

The framework organizes research into a clear hierarchy. Understanding this structure is key to using the workflow scripts correctly.

```
Study
â””â”€â”€ Experiment (Condition A)
    â”œâ”€â”€ Replication 1
    â”‚   â”œâ”€â”€ Trial 1
    â”‚   â”œâ”€â”€ Trial 2
    â”‚   â””â”€â”€ ...
    â””â”€â”€ Replication 2
        â””â”€â”€ ...
â””â”€â”€ Experiment (Condition B)
    â”œâ”€â”€ Replication 1
    â”‚   â””â”€â”€ ...
    â””â”€â”€ Replication 2
        â””â”€â”€ ...
```

-   **Study:** The highest-level grouping, representing a complete research question (e.g., comparing "correct" vs. "random" mappings).
-   **Experiment:** A complete set of runs for a single experimental condition (e.g., all runs for the "correct" mapping condition).
-   **Replication:** A single, complete run of an experiment, repeated multiple times for statistical power.
-   **Trial:** An individual matching task performed within a replication.

## The Research Workflow

The end-to-end process is designed around a clear, two-stage workflow: first, you create an **Experiment** for each condition; second, you group them into a **Study** for final analysis.

{{grouped_figure:docs/diagrams/flow_research_lifecycle.mmd | scale=2.5 | width=100% | caption=The End-to-End Research Workflow.}}

### Stage 1: Create Experiments for Each Condition

This stage focuses on generating the raw data. For each condition, you will configure `config.ini` and use the following scripts:

-   **`new_experiment.ps1` (Create)**: The primary script for data generation. It runs a full set of replications for one condition.
-   **`audit_experiment.ps1` (Check)**: A read-only diagnostic tool to check the status and health of an experiment.
-   **`fix_experiment.ps1` (Fix)**: If an experiment is interrupted, this script intelligently resumes it.

**Learning the Workflow**: For users new to the framework, a comprehensive interactive guided tour is available that demonstrates the complete experiment lifecycle with detailed explanations at each step. This Layer 4 integration test walks users through:

- Creating a 2Ã—2 factorial experiment design (4 experiments)
- Auditing experiment health and completeness
- Simulating 4 distinct real-world failure scenarios:
  - Missing LLM responses (API interruption)
  - Corrupted analysis files (I/O errors)
  - Corrupted configuration data (metadata damage)
  - Corrupted report files (storage corruption)
- Automated detection and repair of each corruption type
- Final verification of experiment integrity

This can be accessed via `pdm run test-l4-interactive` (see the Testing Guide for details).

### Stage 2: Compile and Analyze the Study

Once you have a separate experiment directory for each of your conditions, you can analyze them together as a single study.

1.  **Organize Experiments**: Manually move all completed experiment directories into a single parent folder (e.g., `output/studies/My_First_Study/`).

2.  **Audit the Study (`audit_study.ps1`)**: Run this read-only script on your study directory to perform a consolidated health check on all experiments.

3.  **Compile the Study (`compile_study.ps1`)**: This is the final step. It aggregates all data, runs the statistical analysis, and generates the publication-ready reports and plots.

## Workflow Validation & Reliability

The experiment lifecycle workflow has been comprehensively validated through integration testing that demonstrates the framework's self-healing capabilities:

### Automated Corruption Detection
The audit system can detect and classify multiple types of experiment corruption:

- **REPAIR_NEEDED**: Single-category failures (missing files, simple corruption)
- **REPROCESS_NEEDED**: Analysis corruption requiring regeneration
- **AGGREGATION_NEEDED**: Experiment-level file corruption
- **MIGRATION_NEEDED**: Complex multi-category corruption (archived functionality)

### Intelligent Repair Strategies
The repair system automatically determines the appropriate recovery strategy:

- **Session Repair**: Re-runs only failed API calls, preserving existing data
- **Analysis Regeneration**: Rebuilds analysis from raw response data
- **Configuration Restoration**: Restores corrupted config files from source parameters
- **Summary Regeneration**: Rebuilds experiment-level aggregation files

### Validation Coverage
The framework's reliability is validated through comprehensive testing that covers:
- End-to-end experiment creation and compilation workflows
- Deliberate corruption scenarios with automated recovery
- State detection accuracy across all failure modes
- Data integrity verification throughout the repair process

This ensures that researchers can trust the framework to maintain data integrity even when facing common real-world failures like network interruptions, storage errors, or process crashes.

## For More Detail

This guide covers the high-level user journey. For a comprehensive technical deep-dive into the framework's architecture, methodology, and configuration options, please see the full **[ðŸ“– Framework Manual](DOCUMENTATION.md)**. For hands-on learning, the **[ðŸ§ª Testing Guide](TESTING.md)** includes comprehensive interactive tutorials, including the Layer 4 integration test that provides a complete guided tour of the experiment lifecycle with deliberate corruption and repair scenarios.