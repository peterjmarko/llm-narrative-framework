# Data Preparation Data Dictionary

This document is the **Data Preparation Data Dictionary** for the project. Its purpose is to describe the contents and structure of the `data/` directory, explaining the role of each file in the data preparation and analysis pipelines.

## ðŸ“‘ Related Documentation

- **[ðŸ”§ Framework Manual](FRAMEWORK_MANUAL.md)** - System architecture and technical specifications (see this dictionary for detailed file formats)
- **[ðŸ“ˆ Experiment and Study Workflow Data Dictionary](EXPERIMENT_WORKFLOW_DATA_DICTIONARY.md)** - Specifications for `output/` directory (the next stage after data preparation)
- **[ðŸ”¬ Replication Guide](REPLICATION_GUIDE.md)** - Procedures that use these files
- **[ðŸ“– README](../README.md)** - Project overview

---

The `data/` directory provides the foundational inputs for the project's core logic and stores the intermediate artifacts generated during the data preparation phase, as illustrated in the overall project architecture below.

{{grouped_figure:docs/diagrams/arch_project_overview.mmd | scale=2.5 | width=100% | caption=Project Architecture: The `data/` directory serves as the primary input for the core framework logic.}}

## Directory Structure

```
data/
â”‚
â”œâ”€â”€ README_DATA.md                      # This file.
â”œâ”€â”€ base_query.txt                      # LLM prompt template.
â”œâ”€â”€ personalities_db.txt                # Final database for experiments.
â”‚
â”œâ”€â”€ sources/
â”‚   â””â”€â”€ adb_raw_export.txt              # Raw data from automated fetch.
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ adb_validation_summary.txt
â”‚   â”œâ”€â”€ delineation_coverage_map.csv
â”‚   â”œâ”€â”€ eminence_scores_summary.txt
â”‚   â”œâ”€â”€ ocean_scores_summary.txt
â”‚   â”œâ”€â”€ pipeline_completion_info.json
â”‚   â””â”€â”€ ... (other audit logs)
â”‚
â”œâ”€â”€ foundational_assets/
â”‚   â”œâ”€â”€ neutralized_delineations/       # Sanitized text snippets.
â”‚   â”œâ”€â”€ sf_chart_export.csv             # Chart data from Solar Fire.
â”‚   â”œâ”€â”€ sf_delineations_library.txt     # Raw text library from Solar Fire.
â”‚   â”œâ”€â”€ country_codes.csv               # Maps country/state codes.
â”‚   â”œâ”€â”€ eminence_scores.csv             # List ranked by eminence.
â”‚   â”œâ”€â”€ ocean_scores.csv                # Definitive subject set.
â”‚   â”œâ”€â”€ point_weights.csv               # Weights for profile algorithm.
â”‚   â”œâ”€â”€ balance_thresholds.csv          # Thresholds for profile algorithm.
â”‚   â””â”€â”€ cutoff_parameter_analysis_results.csv # Optimal cutoff parameters.
â”‚                                       
â”œâ”€â”€ intermediate/
â”‚   â”œâ”€â”€ adb_eligible_candidates.txt     # Subjects passing quality checks.
â”‚   â”œâ”€â”€ adb_final_candidates.txt        # Final subject set for SF.
â”‚   â””â”€â”€ sf_data_import.txt              # Formatted for SF import.
â”‚
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ adb_wiki_links.csv              # Best-guess Wikipedia URLs.
â”‚   â”œâ”€â”€ adb_validated_subjects.csv      # Detailed output of Wikipedia page validation.
â”‚   â””â”€â”€ subject_db.csv                  # Cleaned & integrated master DB.
â”‚
â””â”€â”€ backup/
    â””â”€â”€ ... (timestamped backups)       # Automatic backups.
```

## File Descriptions by Directory

### 1. `sources/` - Raw Data

This directory contains the original, unprocessed starting point for the pipeline.

-   **`adb_raw_export.txt`**: The raw data dump from Astro-Databank, generated by `fetch_adb_data.py`. For direct replication of the study, use the version of this file included in the repository. For new research, this file is the first to be generated. It includes standardized identifiers (`Index`, `idADB`) and pre-calculated timezone information.

### 2. `reports/` - Validation & Audit Files

These files are generated during the data validation and filtering stages.

-   **`adb_validation_summary.txt`**: A human-readable summary of the Wikipedia page validation process, generated from `adb_validated_subjects.csv`.
-   **`delineation_coverage_map.csv`**: A report used by the assembly logic validation workflow to ensure test subjects provide maximum coverage of all text components.
-   **`eminence_scores_summary.txt`**: A human-readable summary of the eminence scoring run, including descriptive statistics, a score distribution histogram, and a list of the top 10 most eminent subjects. Includes a generation timestamp.
-   **`ocean_scores_summary.txt`**: The detailed summary report from `generate_ocean_scores.py`, including descriptive statistics for the entire cohort and a quintile-based analysis of variance degradation. Includes a generation timestamp.
-   **`pipeline_completion_info.json`**: A machine-readable JSON file that tracks the completion status and metrics for fallible, I/O-bound pipeline steps. It contains objects for subject qualification (`qualify_subjects`), eminence scoring (`eminence_scores`), OCEAN scoring (`ocean_scores`), and text neutralization (`neutralize_delineations`). Each object includes metrics like completion rates, success/failure counts, and paths to detailed reports. This file is used by the pipeline orchestrator to determine if a step completed successfully.
-   **`missing_eminence_scores.txt`** & **`missing_ocean_scores.txt`**: Standardized, human-readable reports detailing subjects that were not scored by an LLM process. Both files share a common structure:
    1.  **Header:** Contains the report title and a generation timestamp.
    2.  **Summary Block:** Provides high-level statistics (`Total Eligible`, `Total Scored`, `Total Missing`).
    3.  **Categorized Sections:** Breaks down missing subjects into distinct categories (e.g., "Missed During LLM Processing," "Subjects Not Processed"), each with a fixed-width table containing the subject's `idADB` and `Name`. The OCEAN report also includes the `Eminence` score for prioritization.

### 3. `foundational_assets/` - Static Assets for Generation

These files are static, pre-prepared assets that provide the rules and content for generating the final personality descriptions.

-   **`eminence_scores.csv`**: Contains the LLM-generated eminence score for every subject in the raw export. It is created by `generate_eminence_scores.py` and provides the rank-ordered input for `generate_ocean_scores.py`.
-   **`ocean_scores.csv`**: Contains the LLM-generated OCEAN personality scores for every subject. This file provides the complete dataset used for the data-driven cutoff analysis that determines the final subject pool.
-   **`cutoff_parameter_analysis_results.csv`**: Contains the grid search results for optimal cutoff parameters. These parameters are used by `config.ini` to configure the final candidate selection algorithm.
-   **`country_codes.csv`**: A mapping file to resolve country/state abbreviations.
-   **`sf_delineations_library.txt`**: The raw, complete library of interpretive text as exported from Solar Fire.
-   **`neutralized_delineations/`**: A directory of `.csv` files containing the sanitized, de-jargonized description components. This library is generated automatically by `neutralize_delineations.py`.
-   **`sf_chart_export.csv`**: The raw data exported from Solar Fire after it has processed the subjects. This is the output of the single manual step in the pipeline.
-   **`point_weights.csv` & `balance_thresholds.csv`**: Configuration files that define the core logic for the personality classification algorithm in `generate_personalities_db.py`.

### 4. `intermediate/` - Pipeline Artifacts

These files are the outputs of one pipeline script and the inputs to the next.

-   **`adb_eligible_candidates.txt`**: The output of `select_eligible_candidates.py`. Contains all subjects from the raw export that pass initial data quality checks, creating a clean pool for LLM scoring.
-   **`adb_final_candidates.txt`**: The output of `select_final_candidates.py`. This is the definitive, final set of subjects for the experiment, created by applying a data-driven cutoff to the eminence-ranked list.
-   **`sf_data_import.txt`**: The output of `prepare_sf_import.py`. This file is formatted for direct import into the Solar Fire software.

### 5. `processed/` - Cleaned & Integrated Data

This directory holds cleaned and integrated data files that are ready for downstream processing.

-   **`adb_wiki_links.csv`**: The output of `find_wikipedia_links.py`. It contains the best-guess Wikipedia URL and a sanitized `Subject_Name` for each entry from the raw data export.
-   **`adb_validated_subjects.csv`**: The output of `qualify_subjects.py`. This file contains the final validation status for each subject and is the source of truth for the sanitized `Subject_Name`.
-   **`subject_db.csv`**: The output of `create_subject_db.py`. This script integrates the Solar Fire chart data with the final subject list to produce a clean, master database ready for the final generation step.

### 6. Top-Level Files - Main Experiment Files

These are the final, high-level files used directly in the LLM experiments.

-   **`personalities_db.txt`**: This is the final, primary database used in all experiments. It is generated by `generate_personalities_db.py` and contains the columns `Index`, `idADB`, `Name`, `BirthYear`, and `DescriptionText`.
-   **`base_query.txt`**: A text file containing the prompt template for the LLM matching task.

## Transition to the Main Experiment Pipeline

Once the data preparation pipeline is complete, its primary output, `personalities_db.txt`, serves as the foundational database for the main experiments. This file, together with the `base_query.txt` prompt template, provides the necessary inputs for the first stage of the experimental workflow.

For a detailed explanation of the subsequent experiment and analysis pipeline, see the **[ðŸ“– Framework Manual](../docs/FRAMEWORK_MANUAL.md)**. For comprehensive documentation of the experimental output files and results structure, see the **[ðŸ“Š Experiment Workflow Data Dictionary](../output/EXPERIMENT_WORKFLOW_DATA_DICTIONARY.md)**.