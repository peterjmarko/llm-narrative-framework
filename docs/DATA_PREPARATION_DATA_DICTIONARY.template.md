# Data Preparation Data Dictionary

This document is the **Data Preparation Data Dictionary** for the project. Its purpose is to describe the contents and structure of the `data/` directory, explaining the role of each file in the data preparation and analysis pipelines.

The `data/` directory provides the foundational inputs for the project's core logic and stores the intermediate artifacts generated during the data preparation phase, as illustrated in the overall project architecture below.

{{grouped_figure:docs/diagrams/arch_project_overview.mmd | scale=2.5 | width=100% | caption=Project Architecture: The `data/` directory serves as the primary input for the core framework logic.}}

## Directory Structure

```
data/
â”‚
â”œâ”€â”€ README_DATA.md                # This file.
â”œâ”€â”€ base_query.txt                # LLM prompt template.
â”œâ”€â”€ personalities_db.txt          # Final database for experiments.
â”‚
â”œâ”€â”€ sources/
â”‚   â””â”€â”€ adb_raw_export.txt        # Raw data from automated fetch.
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ adb_validation_report.csv
â”‚   â”œâ”€â”€ adb_validation_summary.txt
â”‚   â”œâ”€â”€ delineation_coverage_map.csv
â”‚   â”œâ”€â”€ eminence_scores_summary.txt
â”‚   â”œâ”€â”€ ocean_scores_summary.txt
â”‚   â”œâ”€â”€ pipeline_completion_info.json
â”‚   â””â”€â”€ ... (other audit logs)
â”‚
â”œâ”€â”€ foundational_assets/
â”‚   â”œâ”€â”€ neutralized_delineations/ # Sanitized text snippets.
â”‚   â”œâ”€â”€ sf_chart_export.csv       # Chart data from Solar Fire.
â”‚   â”œâ”€â”€ sf_delineations_library.txt # Raw text library from Solar Fire.
â”‚   â”œâ”€â”€ country_codes.csv         # Maps country/state codes.
â”‚   â”œâ”€â”€ eminence_scores.csv       # List ranked by eminence.
â”‚   â”œâ”€â”€ ocean_scores.csv          # Definitive subject set.
â”‚   â”œâ”€â”€ point_weights.csv         # Weights for profile algorithm.
â”‚   â””â”€â”€ balance_thresholds.csv    # Thresholds for profile algorithm.
â”‚
â”œâ”€â”€ intermediate/
â”‚   â”œâ”€â”€ adb_eligible_candidates.txt # Subjects passing quality checks.
â”‚   â”œâ”€â”€ adb_final_candidates.txt  # Final subject set for SF.
â”‚   â””â”€â”€ sf_data_import.txt        # Formatted for SF import.
â”‚
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ adb_wiki_links.csv        # Best-guess Wikipedia URLs.
â”‚   â””â”€â”€ subject_db.csv            # Cleaned & integrated master DB.
â”‚
â””â”€â”€ backup/
    â””â”€â”€ ... (timestamped backups) # Automatic backups.
```

## File Descriptions by Directory

### 1. `sources/` - Raw Data

This directory contains the original, unprocessed starting point for the pipeline.

-   **`adb_raw_export.txt`**: The raw data dump from Astro-Databank, generated by `fetch_adb_data.py`. For direct replication of the study, use the version of this file included in the repository. For new research, this file is the first to be generated. It includes standardized identifiers (`Index`, `idADB`) and pre-calculated timezone information.

### 2. `reports/` - Validation & Audit Files

These files are generated during the data validation and filtering stages.

-   **`adb_validation_report.csv`**: The detailed, row-by-row output of the Wikipedia page validation script. It is a critical input for the final filtering step.
-   **`adb_validation_summary.txt`**: A human-readable summary of the validation report.
-   **`delineation_coverage_map.csv`**: A report used by the assembly logic validation workflow to ensure test subjects provide maximum coverage of all text components.
-   **`eminence_scores_summary.txt`**: A human-readable summary of the eminence scoring run.
-   **`ocean_scores_summary.txt`**: The detailed summary report from `generate_ocean_scores.py`, including the cutoff analysis and descriptive statistics.
-   **`pipeline_completion_info.json`**: A JSON file that tracks completion status and metrics for critical pipeline steps. It contains completion rates, missing subject counts, and paths to missing subject reports for both eminence and OCEAN scoring steps. This file is used by the pipeline orchestrator to determine if a step completed successfully or requires intervention.
-   Various other `missing_*.txt` files serve as audit logs for different pipeline stages.

### 3. `foundational_assets/` - Static Assets for Generation

These files are static, pre-prepared assets that provide the rules and content for generating the final personality descriptions.

-   **`eminence_scores.csv`**: Contains the LLM-generated eminence score for every subject in the raw export. It is created by `generate_eminence_scores.py` and provides the rank-ordered input for `generate_ocean_scores.py`.
-   **`ocean_scores.csv`**: This file is the **definitive source for the experiment's final subject pool**. It is created by `generate_ocean_scores.py`, which stops generating scores once personality diversity (variance) shows a sustained drop. The number of subjects in this file dictates the final dataset size.
-   **`country_codes.csv`**: A mapping file to resolve country/state abbreviations.
-   **`sf_delineations_library.txt`**: The raw, complete library of interpretive text as exported from Solar Fire.
-   **`neutralized_delineations/`**: A directory of `.csv` files containing the sanitized, de-jargonized description components. This library is generated automatically by `neutralize_delineations.py`, which uses a hybrid strategy: a fast, bundled initial run (`--fast`) followed by a granular, robust resume run to guarantee completion.
-   **`sf_chart_export.csv`**: The raw data exported from Solar Fire after it has processed the subjects. This is the output of the single manual step in the pipeline.
-   **`point_weights.csv` & `balance_thresholds.csv`**: Configuration files that define the core logic for the personality classification algorithm in `generate_personalities_db.py`.

### 4. `intermediate/` - Pipeline Artifacts

These files are the outputs of one pipeline script and the inputs to the next.

-   **`adb_eligible_candidates.txt`**: The output of `select_eligible_candidates.py`. Contains all subjects from the raw export that pass initial data quality checks, creating a clean pool for LLM scoring.
-   **`adb_final_candidates.txt`**: The output of `select_final_candidates.py`. This is the definitive, final set of subjects for the experiment, created by applying a data-driven cutoff to the eminence-ranked list.
-   **`sf_data_import.txt`**: The output of `prepare_sf_import.py`. This file is formatted for direct import into the Solar Fire software.

### 5. `processed/` - Cleaned & Integrated Data

This directory holds cleaned and integrated data files that are ready for downstream processing.

-   **`adb_wiki_links.csv`**: The output of `find_wikipedia_links.py`, containing the best-guess Wikipedia URL for each subject from the raw data export.
-   **`subject_db.csv`**: The output of `create_subject_db.py`. This script integrates the Solar Fire chart data with the final subject list to produce a clean, master database ready for the final generation step.

### 6. Top-Level Files - Main Experiment Files

These are the final, high-level files used directly in the LLM experiments.

-   **`personalities_db.txt`**: This is the final, primary database used in all experiments. It is generated by `generate_personalities_db.py` and contains the columns `Index`, `idADB`, `Name`, `BirthYear`, and `DescriptionText`.
-   **`base_query.txt`**: A text file containing the prompt template for the LLM matching task.

## Transition to the Main Experiment Pipeline

Once the data preparation pipeline is complete, its primary output, `personalities_db.txt`, serves as the foundational database for the main experiments. This file, together with the `base_query.txt` prompt template, provides the necessary inputs for the first stage of the experimental workflow.

For a detailed explanation of the subsequent experiment and analysis pipeline, see the **[ðŸ“– Framework Manual](../docs/FRAMEWORK_MANUAL.md)**. For comprehensive documentation of the experimental output files and results structure, see the **[ðŸ“Š Experiment Workflow Data Dictionary](../output/EXPERIMENT_WORKFLOW_DATA_DICTIONARY.md)**.