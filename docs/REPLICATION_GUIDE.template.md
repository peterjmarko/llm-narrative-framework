---
title: "Replication Guide"
subtitle: "Supplementary Material for 'A Framework for the Computationally Reproducible Testing of Complex Narrative Systems'"
author: "Peter J. Marko"
date: "[Date]"
---

## ðŸ“‘ Document Navigation

This project includes four interconnected documents:

- **ðŸ“„ Research Article** (`docs/article_main_text.md`) - Scientific findings and validation of the framework using astrology as a test case
- **ðŸ“– README** (`README.md`) - Quick start guide and feature overview
- **ðŸ”¬ Replication Guide** (this document) - Step-by-step procedures for reproducing or extending the study
- **ðŸ”§ Framework Manual** (`docs/FRAMEWORK_MANUAL.md`) - Technical specifications, data formats, and API references

---

This document is the **Replication Guide** that provides supplementary material to the main article, "A Framework for the Computationally Reproducible Testing of Complex Narrative Systems: A Case Study in Astrology." Its purpose is to serve as a detailed, step-by-step guide for researchers who wish to replicate or extend the original study's findings.

## ðŸ“š Related Documentation

- **[ðŸ”§ Framework Manual](docs/FRAMEWORK_MANUAL.md)** - Technical specifications, data formats, and API references
- **[ðŸ§ª Testing Guide](docs/TESTING_GUIDE.md)** - Interactive learning tools and validation procedures (see "Interactive Learning Tools" section for educational walkthroughs)
- **[ðŸ‘¨â€ðŸ’» Developer's Guide](DEVELOPERS_GUIDE.md)** - For those extending the framework or contributing code
- **[ðŸ“Š Data Dictionaries](docs/DATA_PREPARATION_DATA_DICTIONARY.md)** - Detailed file format specifications

For technical specifications, data formats, and API references, please refer to the **[ðŸ”§ Framework Manual](docs/FRAMEWORK_MANUAL.md)**.

## ðŸ‘¥ Who Should Read This Document

**Primary Audience:**
- **Researchers** attempting to reproduce the original study (direct replication)
- **Researchers** testing robustness with fresh data (methodological replication)
- **Researchers** extending the framework to new domains (conceptual replication)

**You should read this if you want to:**
- Understand the complete data preparation pipeline step-by-step
- Run experiments using the framework
- Compile and analyze study results
- Learn the framework's workflows through interactive tours

**You should read something else if you want to:**
- Quick overview of the project â†’ See **[ðŸ“– README](../README.md)**
- Technical specifications and data formats â†’ See **[ðŸ”§ Framework Manual](FRAMEWORK_MANUAL.md)**
- Contribute code or extend the framework â†’ See **[ðŸ‘¨â€ðŸ’» Developer's Guide](../DEVELOPERS_GUIDE.md)**
- Understand the testing strategy â†’ See **[ðŸ§ª Testing Guide](TESTING_GUIDE.md)**

---

**What Is This Framework?** The LLM Narrative Framework is an automated testing methodology that uses Large Language Models as pattern-detection engines to perform matching tasks between narrative personality descriptions and biographical profiles, determining whether systematic signals can be detected at rates significantly greater than chance.

This guide defines the three primary replication paths (direct, methodological, and conceptual) and provides a complete walkthrough of the end-to-end workflow, from initial setup and data preparation to running the main experiments and producing the final statistical analysis.

{{toc}}

> **ðŸ“˜ Note on Terminology:** This guide uses "replication" in two distinct contexts:
> 
> 1. **Research Replication** (this document's focus): The scientific practice of reproducing a study to verify findingsâ€”addressing the widely-discussed "replication crisis" in research
> 2. **Experimental Replication** (framework terminology): A single complete run of an experiment, repeated multiple times (typically 30Ã—) for statistical power
> 
> Context makes the distinction clear: "Direct Replication Procedure" refers to reproducing the study, while "30 replications per condition" refers to repeated experimental runs.

## Project Overview

### Research Question

At its core, this project investigates whether a Large Language Model (LLM) can solve a complex matching task: given a set of sanitized, narrative personality descriptions (derived from birth data) and a corresponding set of general biographical profiles, can the LLM correctly pair them at a rate significantly greater than chance?

This study introduces a novel methodological twist to probe the limits of LLM pattern recognition. The link between the narrative descriptions and the biographical profiles is a faint, systematic signal generated by a deterministic, esoteric system (an astrology program). This transforms the experiment into a rigorous test of an LLM's ability to detect subtle, rule-based patterns within a noisy, high-dimensional dataset. The central question is not about the validity of the generating system, but about the capability of the AI to find a signal in its output.

{{grouped_figure:docs/diagrams/logic_matching_task.mmd | scale=2.5 | width=60% | caption=The Core Research Question: An LLM-driven matching task to detect a non-random signal.}}

### Stimulus Generation and Experimental Design

The experiment is built upon a custom database of 4,954 famous historical individuals, for whom accurate and verified birth data (date, time, place) was meticulously collected. This population was chosen for two reasons:

*   **Signal Integrity**: Accurate birth data ensures the deterministic generation of consistent personality narratives.
*   **Task Feasibility**: The public prominence of these individuals makes it plausible that LLMs have encountered their biographical information during training, making the matching task tractable.

To create a uniquely challenging test, we employed a multi-step, deterministic process to generate the textual stimuli, organized into four main stages:

1.  **Data Sourcing & Candidate Qualification:** A `Raw Subject Database` of 10,619 famous individuals was derived from the Astro-Databank (ADB) and subjected to a rigorous, deterministic filtering pass to create a high-quality cohort of 7,234 "eligible candidates."
2.  **LLM-based Candidate Selection:** This eligible cohort was then processed by two LLM-driven scripts (`generate_eminence_scores.py`, `generate_ocean_scores.py`) that scored every subject for historical eminence and personality diversity. The final selection is then performed by a third script (`select_final_candidates.py`), which applies a data-driven cutoff based on score variance to determine the final subject pool.
3.  **Manual Data Processing:** The final subject list was processed by a commercial astrology program (Solar Fire) to calculate the precise celestial positions for each person.
4.  **Profile Generation:** A custom Python script (`generate_personalities_db.py`) then processed this celestial data. Using a `Neutralized Component Library` of pre-written sentences, the script deterministically assembled a unique personality narrative for each individual based on a validated **personality assembly algorithm**.

##### The Personality Assembly Algorithm

The algorithm translates raw celestial data into a set of classification keys (e.g., "Element Fire Strong," "Moon in Scorpio") used to assemble the final narrative. It is a five-step process driven by two key configuration files:

*   **`point_weights.csv`**: Assigns a numerical weight to each of the 12 chart points (e.g., Sun=3, Moon=3, Mercury=2, etc.) to determine its relative influence.
*   **`balance_thresholds.csv`**: Defines the ratios for classifying a factor as 'strong' or 'weak' relative to the average score in its category.

The algorithm proceeds as follows:

1.  **Calculate Weighted Sign Scores:** For each of the 12 zodiac signs, the algorithm calculates a total score by summing the weights of all chart points located within that sign.
2.  **Calculate Weighted Quadrant & Hemisphere Scores:** It calculates scores for the four quadrants and four hemispheres by summing the weights of the points located within their angular boundaries, which are defined by the chart's Ascendant and Midheaven.
3.  **Calculate Weighted Element & Mode Scores:** It calculates total scores for the four elements (Fire, Earth, Air, Water) and three modes (Cardinal, Fixed, Mutable) by summing the weighted scores of their constituent signs (e.g., the "Fire" score is the sum of the scores for Aries, Leo, and Sagittarius).
4.  **Apply Thresholds for "Balance" Classification:** For each of the five "Balance" categories (Signs, Elements, Modes, Quadrants, Hemispheres), the algorithm calculates the average score. It then uses the pre-defined ratios from `balance_thresholds.csv` to determine strong and weak thresholds. If a factor's score exceeds the strong threshold, it is classified as "Strong" (e.g., `Element Fire Strong`); if it falls below the weak threshold, it is classified as "Weak" (e.g., `Mode Cardinal Weak`).

| Category | WeakRatio | StrongRatio | Example Classification |
| :--- | :---: | :---: | :--- |
| Elements | 0.50 | 1.50 | `Element Air Strong` |
| Modes | 0.67 | 1.33 | `Mode Fixed Weak` |

5.  **Append Final "Point in Sign" Classifications:** Finally, the algorithm appends the simple placement for each of the 12 chart points (e.g., `Sun in Aries`, `Moon in Taurus`, etc.) to the list of classifications.

The complete, ordered list of these classification keys is then used to look up and concatenate the corresponding neutralized text snippets, forming the final personality description for the individual. **The personality assembly algorithm has been rigorously validated against a ground-truth dataset generated by the source Solar Fire software, ensuring its output is bit-for-bit identical.**

{{grouped_figure:docs/diagrams/flow_stimulus_generation.mmd | scale=2.5 | width=40% | caption=The four-stage process for generating the experimental stimuli.}}

The result is a clean dataset of personality profiles where the connection to an individual's biographical profile is systematic but non-obvious.

**Crucially, this study does not seek to validate astrology.** Instead, it treats the generating program as an arbitrary, complex algorithm. The scientific objective is to determine whether an LLM, a third-party pattern-recognition system, can detect the subtle statistical regularities produced by this algorithm and use them to perform a successful matching task. The findings speak to the profound capabilities of LLMs to find signal in noisy, high-dimensional data, regardless of the source's theoretical basis.

### Framework Architecture

The framework is organized into four primary components that work together to enable reproducible research on complex narrative systems. The production codebase is structured around a clear separation of concerns, with user-facing orchestration scripts executing core logic modules that operate on well-defined data artifacts.

{{grouped_figure:docs/diagrams/arch_project_overview.mmd | scale=2.0 | width=100% | caption=Figure S1: Project Architecture Overview. The framework consists of four main components: User-Facing Interface, Core Logic, Project Governance, and Data & Artifacts.}}

The **User-Facing Interface** comprises PowerShell orchestration scripts that provide a simple, consistent way to execute complex workflows. These scripts handle parameter validation, error recovery, and progress tracking, allowing researchers to focus on research questions rather than implementation details.

The **Core Logic** contains the production Python scripts in the `src/` directory that implement the actual data processing, experiment execution, and analysis algorithms. These modules are designed to be modular, testable, and reusable across different research contexts.

The **Project Governance** component includes documentation, diagrams, the validation test suite, and developer utilities that ensure the framework maintains high standards of quality, reproducibility, and transparency.

The **Data & Artifacts** component manages all inputs and outputs, including source data, generated experiments, analysis reports, and project-level documentation that provides provenance for all research artifacts.

## Research Paths

The framework supports three distinct research paths: direct, methodological, and conceptual replication.

**Path 1: Direct Replication (Computational Reproducibility)**
To ensure computational reproducibility of the original findings, researchers should use the static data files and randomization seeds included in this repository. This path validates that the framework produces the same statistical results.

**Path 2: Methodological Replication (Testing Robustness)**
To test the robustness of the findings, researchers can use the framework's automated tools to generate a fresh dataset from the live Astro-Databank (ADB). The instructions below detail this workflow, which is organized into four main stages.

**Path 3: Conceptual Replication (Extending the Research)**
To extend the research, researchers can modify the framework itself, for example, by using different LLMs for the matching task or altering the analysis scripts.

For a direct or methodological replication, it is crucial to use the exact models and versions from the original study. All models were accessed via the **OpenRouter API**. See Appendix C for the complete list of models used.

### Choosing Your Research Path

The following table summarizes key differences to help you select the appropriate replication path:

| Aspect | Direct Replication | Methodological Replication | Conceptual Replication |
|--------|-------------------|---------------------------|------------------------|
| **Data Source** | Static files (`data/`) | Fresh from live ADB | Custom/Modified |
| **Randomization Seeds** | Fixed (from config) | Fresh | Custom |
| **Framework Code** | Unchanged | Unchanged | Modified |
| **Primary Purpose** | Verify computational reproducibility | Test statistical robustness | Extend research questions |
| **Time Required** | Hours | Days | Weeks to months |
| **Estimated Cost** | ~$1,500 | ~$2,000-2,400 | Variable |
| **Prerequisites** | Basic setup (A.1) | Basic + ADB + Solar Fire (A.2) | Full dev environment (A.3) |
| **Output** | Bit-for-bit match validation | Robustness assessment | New findings |

**Quick Decision Guide:**
- **Choose Direct** if you want to verify the framework produces identical results
- **Choose Methodological** if you want to test whether findings hold with fresh data
- **Choose Conceptual** if you want to test new hypotheses or apply the framework to new domains

## Interactive Learning Tools

Before diving into the detailed procedures, researchers may benefit from interactive walkthroughs of the framework's workflows. These guided tours provide educational demonstrations of the data preparation and experiment lifecycles.

### Available Interactive Tours

**Data Preparation Pipeline Tour (Layer 3 Interactive)**

A step-by-step walkthrough of the complete data pipeline from raw data to final personality profiles.

**Command:**
```powershell
pdm run test-l3-interactive
```

**What you'll learn:**
- How the 4-stage pipeline processes data
- What each script does and how they connect
- Intermediate file formats and validation checks
- LLM interaction patterns and candidate selection logic

**Duration:** ~10-15 minutes  
**Prerequisites:** Configured `.env` file with OpenRouter API key

**Experiment Lifecycle Tour (Layer 4 Interactive)**

A comprehensive demonstration of creating, auditing, corrupting, and repairing experiments.

**Command:**
```powershell
pdm run test-l4-interactive
```

**What you'll learn:**
- How to create and run experiments
- How the audit system detects problems
- How the repair system fixes common issues
- Complete "Create â†’ Check â†’ Fix â†’ Compile" workflow

**Duration:** ~5-7 minutes  
**Prerequisites:** Basic framework setup

### Using Interactive Tours

These tours are **optional but highly recommended** for first-time users. They transform validation tests into educational experiences by:

- Pausing before each major step with explanations
- Allowing inspection of intermediate results
- Demonstrating the procedures described in this guide
- Providing hands-on understanding of framework operations

**Note:** These tours are part of the testing infrastructure and run in isolated sandbox environments. They will not modify your production data or experiments. For complete technical details about the testing architecture, see the **[ðŸ§ª Testing Guide](docs/TESTING_GUIDE.md)**.

---

## Production Codebase

The production codebase implements two principal workflows that form the backbone of the research process: the Data Preparation Pipeline and the Experiment & Study Workflow. These workflows are sequentially dependent but architecturally distinct, with the data preparation pipeline creating the foundational datasets that the experiment workflow consumes.

### Data Preparation Pipeline

The **Data Preparation Pipeline** is a fully automated, multi-stage workflow that transforms raw data from external sources (Astro-Databank, Wikipedia) into the curated `personalities_db.txt` file used in experiments. This pipeline implements sophisticated filtering, scoring, and selection algorithms to create a high-quality, diverse dataset of personality profiles.

{{grouped_figure:docs/diagrams/flow_prep_pipeline.mmd | scale=2.0 | width=35% | caption=Figure S2: Data Preparation Pipeline. The pipeline processes raw astrological data from ADB through multiple stages to create personalities_db.txt.}}

#### Replication Paths

Researchers can approach this project in several ways to validate and extend the findings, as illustrated below.

{{grouped_figure:docs/diagrams/flow_replication_paths.mmd | scale=2.5 | width=100% | caption=The Three Research Replication Paths.}}

1.  **Direct Replication (Computational Reproducibility):** To verify that the framework produces the exact findings reported in the article, clone this repository and use the static data and randomization seeds as provided. This is a bit-for-bit validation of the original results.

2.  **Methodological Replication (Testing Robustness):** To validate that the findings are robust and not an artifact of a specific dataset or randomization seed, use the framework as-is but vary the inputs. This can be done in two ways:
    *   **With a New Dataset:** Run the full `prepare_data.ps1` pipeline to generate a fresh dataset from the live Astro-Databank. This tests the statistical robustness of the method on a new sample.
    *   **With New Randomization:** Use the provided static dataset but specify a different set of randomization seeds in `config.ini`. This validates the stability of the results across different random permutations.

3.  **Conceptual Replication (Extending the Research):** To test the underlying scientific concepts with an improved or modified method, researchers can alter the framework itself. This could involve using a different LLM, modifying the analysis scripts, or changing other core parameters to conduct new research built upon this study's foundation.

#### Data Preparation: Architecture

This diagram provides a map of the scripts in the data preparation pipeline, showing how they are orchestrated and which utilities they share.

{{grouped_figure:docs/diagrams/arch_prep_codebase.mmd | scale=2.5 | width=100% | caption=Data Preparation Code Architecture: The execution flow of the data processing scripts.}}

#### Data Preparation: Workflow

This diagram shows the high-level, multi-stage workflow for the entire data preparation pipeline, including both automated and manual processes.

{{grouped_figure:docs/diagrams/flow_prep_pipeline.mmd | scale=2.5 | width=35% | caption=Data Preparation Workflow: The end-to-end pipeline from raw data extraction to the final generated databases, showing both manual and automated steps.}}

#### Data Preparation: Data Flow

These diagrams show the sequence of data artifacts (files) created and transformed by the pipeline scripts at each major stage.

{{grouped_figure:docs/diagrams/flow_prep_1_qualification.mmd | scale=2.5 | width=75% | caption=Data Prep Flow 1: Data Sourcing and Candidate Qualification.}}

{{grouped_figure:docs/diagrams/flow_prep_2_selection.mmd | scale=2.5 | width=80% | caption=Data Prep Flow 2: LLM-based Candidate Selection.}}

{{grouped_figure:docs/diagrams/flow_prep_3_generation.mmd | scale=2.5 | width=100% | caption=Data Prep Flow 3: Profile Generation.}}

Concurrent with these outputs, the individual scripts also generate summary reports as shown in the diagram below.

{{grouped_figure:docs/diagrams/flow_data_prep_reports.mmd | scale=2.5 | width=75% | caption=Data Prep Flow: Generated Reports.}}

#### Data Preparation: Logic

These diagrams illustrate the internal decision-making logic and control flow of each script in the data preparation pipeline.

{{grouped_figure:docs/diagrams/logic_prep_pipeline.mmd | scale=2.5 | width=50% | caption=Overall Logic for the Data Preparation Pipeline: A high-level view of the four main stages.}}

{{grouped_figure:docs/diagrams/logic_prep_find_links.mmd | scale=2.5 | width=70% | caption=Logic for Link Finding (`find_wikipedia_links.py`): The algorithm for finding Wikipedia URLs by scraping ADB and using a Wikipedia search fallback.}}

{{grouped_figure:docs/diagrams/logic_prep_qualify_subjects.mmd | scale=2.5 | width=80% | caption=Logic for Subject Qualification (`qualify_subjects.py`): The algorithm for validating Wikipedia/Wikidata entries, including redirect handling, disambiguation, and life status verification.}}

{{grouped_figure:docs/diagrams/logic_prep_eligible_candidates.mmd | scale=2.5 | width=65% | caption=Logic for Final Filtering (`select_eligible_candidates.py`): The algorithm for applying all deterministic data quality rules to create the final "eligible candidates" list.}}

{{grouped_figure:docs/diagrams/logic_prep_eminence_scoring.mmd | scale=2.5 | width=60% | caption=Logic for Eminence Scoring (`generate_eminence_scores.py`): The algorithm for batch processing, LLM interaction, and finalization of eminence scores.}}

{{grouped_figure:docs/diagrams/logic_prep_ocean_scoring.mmd | scale=2.5 | width=55% | caption=Logic for OCEAN Scoring (`generate_ocean_scores.py`): The algorithm for generating OCEAN personality scores for all eligible subjects. A robust pre-flight check ensures that interrupted runs can be safely resumed.}}

{{grouped_figure:docs/diagrams/logic_prep_final_candidates.mmd | scale=2.5 | width=35% | caption=Logic for Final Selection (`select_final_candidates.py`): The algorithm for finding the optimal cohort size by performing a slope analysis on a smoothed cumulative personality variance curve.}}

{{grouped_figure:docs/diagrams/logic_prep_neutralization.mmd | scale=2.5 | width=60% | caption=Logic for Delineation Neutralization (`neutralize_delineations.py`): The hybrid algorithm for rewriting texts. Fast mode bundles tasks for speed, while the robust default mode processes each item individually to guarantee completion.}}

{{grouped_figure:docs/diagrams/logic_prep_generation.mmd | scale=2.5 | width=70% | caption=Logic for Database Generation (`generate_personalities_db.py`): The algorithm for assembling the final description text for each subject.}}

#### The Automated Workflow

The automated data preparation pipeline is orchestrated by a single, intelligent PowerShell script: `prepare_data.ps1`. This is the recommended method for both initial runs and for resuming interrupted processes.

**Execution:**
```powershell
# Run the entire data preparation pipeline interactively
.\prepare_data.ps1

# Run in interactive mode with step-by-step guidance
.\prepare_data.ps1 -Interactive

# Force a full re-run, deleting all existing data
.\prepare_data.ps1 -Force

# Get a read-only status report of the pipeline's progress
.\prepare_data.ps1 -ReportOnly

# Restore the most recent backup to recover from data loss
.\prepare_data.ps1 -RestoreBackup

# Resume from a specific step if a failure occurs
.\prepare_data.ps1 -StartWithStep 6
```
> **Warning on Using `-Force`**: The `-Force` flag triggers a full, destructive re-run of the entire pipeline. It backs up and deletes all existing data, re-downloads the full raw dataset, and re-runs all expensive LLM scoring steps. This process is very time-consuming and will incur API costs.
> **Interactive Mode (`-Interactive`)**: This mode provides a step-by-step "guided tour" of the entire pipeline. Before execution begins, it displays the relevant DataGeneration parameters from `config.ini` and pauses for user confirmation. During execution, it pauses before each step to show detailed information about inputs, outputs, and script summaries, allowing users to understand exactly what the pipeline is doing. This mode is highly recommended for new users or when troubleshooting issues.
> **Data Completeness Reporting**: At the end of the pipeline, a comprehensive data completeness report is displayed that shows any missing subjects from the LLM scoring steps. This report provides clear visibility into data quality issues and includes actionable guidance on how to retry specific steps if needed.
> **Data Preparation Pipeline Summary Report**: Upon successful completion of the pipeline, a comprehensive summary report is automatically generated at `data/reports/data_preparation_pipeline_summary.txt`. This report provides a unified overview of the entire data preparation pipeline, including validation results, scoring statistics, file status, and identified issues with specific recommendations for resolution.
> **Restore Functionality (`-RestoreBackup`)**: This mode is a recovery tool. It automatically finds the most recent timestamped backup in the `data/backup/` directory and restores all files from that backup to their original locations. It is non-destructive: it copies files from the backup and will not overwrite existing files at the destination, preventing accidental data loss. This is useful for recovering from accidental file deletion or reverting to a previous stable state.

The script is fully resumable. It automatically detects which steps have already been completed and picks up from the first missing data artifact, ensuring a smooth and efficient workflow.

#### Data Preparation Pipeline Summary Report

The framework automatically generates a comprehensive summary report upon successful completion of the data preparation pipeline. This report, located at `data/reports/data_preparation_pipeline_summary.txt`, provides a unified overview of the entire pipeline status and results.

**Report Contents:**

1. **Overall Pipeline Status**
   - Overall completion rate: Percentage of eligible candidates successfully processed by LLM scoring
   - Data quality score: Percentage of raw records that passed Wikipedia validation
   - Clear definitions of what these metrics represent
   - Actual counts shown in parentheses (e.g., "97.9% (7,198/7,349)")

2. **Data Validation Summary**
   - Total records from Astro-Databank
   - Valid records (passed Wikipedia validation)
   - Failed records (did not pass validation)

3. **Candidate Qualification Section**
   - Count of eligible candidates after deterministic filtering
   - Explanation of records filtered out during qualification
   - Count of final candidates after cutoff algorithm (if available)

4. **LLM Scoring Summaries**
   - Eminence scoring statistics (subjects scored, subjects in source, mean score)
   - OCEAN scoring statistics (subjects scored, subjects in source)
   - Missing subjects counts with detailed reports
   - Clear indication that OCEAN scoring processes the eminence-filtered candidates

5. **Final Database Section**
   - Status of the final subject database
   - Total number of subjects included
   - Explanation of reduction from OCEAN database (cutoff algorithm)
   - File size and location information

6. **Delineation Library Section**
   - Status of the source delineation library
   - Count of neutralized files generated (out of 6 expected)
   - Detailed status of each neutralized file with line counts

7. **Cutoff Analysis Section**
   - Status of the cutoff parameter analysis
   - Location of the analysis file
   - Clear note that the analysis was performed on a different dataset
   - Explanation that cutoff parameters are used as a reference for selecting the final research cohort

8. **Pipeline Output Files Status**
   - Visual indicators (âœ“/âœ—) for each expected output file
   - Files grouped by pipeline stage for easy assessment

9. **Issues and Recommendations**
   - Identified bottlenecks and data quality issues
   - Specific commands to address issues (e.g., re-running from specific steps)

10. **Detailed Report References**
    - Direct paths to all detailed reports for further investigation

**Usage:**

- **Automatic Generation**: The report is automatically generated at the end of successful pipeline runs
- **Manual Regeneration**: Can be regenerated at any time using:
```bash
  python src/generate_data_preparation_summary.py
```
- **Integration with Report-Only Mode**: The report is also displayed when using `.\prepare_data.ps1 -ReportOnly`

**Benefits:**

- **Quick Assessment**: Provides immediate visibility into pipeline status without checking multiple files
- **Actionable Insights**: Includes specific recommendations for addressing identified issues
- **Comprehensive Overview**: Aggregates information from all pipeline reports into a single document
- **Problem Resolution**: Helps identify and resolve data quality issues efficiently

This summary report serves as the primary interface for understanding the overall health and completeness of your data preparation pipeline.

#### Resuming from Failed Steps

When a step fails due to network issues, API errors, or other problems, the pipeline will halt with an error message. The error message will include explicit instructions on how to resume from the failed step:
```
TO RESUME FROM THIS STEP:
Run the following command to restart from the failed step:
  .\prepare_data.ps1 -StartWithStep 6

Alternatively, you can run the pipeline using pdm:
  pdm run prep-data -StartWithStep 6
```

**Step Numbers Reference:**
- Step 1: Fetch Raw ADB Data
- Step 2: Find Wikipedia Links
- Step 3: Qualify Subjects
- Step 4: Select Eligible Candidates
- Step 5: Generate Eminence Scores
- Step 6: Generate OCEAN Scores
- Step 7: Analyze Cutoff Parameters
- Step 8: Select Final Candidates
- Step 9: Prepare Solar Fire Import File
- Step 10: Delineations Library Export (Manual)
- Step 11: Astrology Data Export (Manual)
- Step 12: Neutralize Delineations
- Step 13: Create Subject Database
- Step 14: Generate Personalities Database

**Important Notes:**
- **Automatic Downstream Execution**: When any step executes, all downstream steps are automatically forced to re-run to maintain data consistency. There are two exceptions:
  - Step 7 (Analyze Cutoff Parameters): Only forces downstream re-execution if the optimal parameters actually change
  - Step 12 (Neutralize Delineations): Has an independent state machine and only re-runs based on its own staleness/completeness checks
- **`-StopAfterStep` Parameter**: This parameter is intended for testing and debugging purposes only. Using it in production may result in data inconsistency, as dependent downstream steps will not be automatically updated.

**Common Scenarios for Resuming:**
- **Network/API Failures**: If LLM scoring steps (5 or 6) fail due to network issues, resume from the failed step to continue processing only the remaining batches.
- **Parameter Changes**: If Step 7 runs and updates cutoff parameters, Steps 8-11 will automatically re-run with the new values.
- **Manual Step Interruption**: If you need to pause during manual steps (9 or 10), the pipeline can be resumed from the next automated step.
- **Partial Completion**: When a step completes partially but doesn't finish, resuming from that step will continue from where it left off.
- **Re-running Scoring Steps**: When you re-run eminence or OCEAN scoring steps (5 or 6) with different parameters or models, the pipeline will automatically force re-execution of all downstream steps (7 and 8) to ensure data consistency, even if their output files already exist.

**Important Note on Data Dependencies**: When resuming from a scoring step (5 or 6), the pipeline automatically forces re-execution of all downstream steps to maintain data integrity. This is because downstream outputs depend on the scoring data and would be inconsistent with the new scoring results.

The resume functionality is designed to be efficient and cost-effective, as it only processes the remaining work rather than re-running the entire pipeline.

#### Individual Script Details

The `prepare_data.ps1` script orchestrates a sequence of individual Python scripts followed by a manual processing step. The following is a detailed breakdown of this underlying workflow.

##### Stage 1: Data Sourcing

This stage uses `fetch_adb_data.py` to create the initial raw dataset by querying the live Astro-Databank with a specific set of pre-filters. It performs two crucial transformations at the source:

-   **Identifier Standardization:** It replaces the unstable, temporary record number (`ARN`) with a file-specific sequential `Index`, and renames the permanent Astro-Databank ID (`ADBNo`) to `idADB`.
-   **Timezone Calculation:** It immediately processes the raw timezone code from the API into the final `ZoneAbbr` and `ZoneTimeOffset` values required by downstream software.

The query only includes subjects who meet all of the following criteria:

-   **High-Quality Birth Data:** The record must have a Rodden Rating of 'A' or 'AA', indicating the birth time is from a reliable source.
-   **Deceased Individuals:** Inclusion in the **Personal > Death** category.
-   **Eminence:** Inclusion in the **Notable > Famous > Top 5% of Profession** category.

###### Individual Script Execution

While the `prepare_data.ps1` orchestrator is the recommended approach, individual scripts can be executed for development and debugging:
```bash
# Fetch a new dataset from the live ADB
pdm run fetch-adb
```

This produces `data/sources/adb_raw_export.txt`.

##### Stage 2: Candidate Qualification

This stage performs a rigorous, deterministic filtering pass on the raw data to create a high-quality cohort of "eligible candidates."

1.  **Link Finding (`find_wikipedia_links.py`):** This script takes the raw ADB export, sanitizes the `FirstName` and `LastName` fields to remove non-essential text (like parenthetical years), and finds the best-guess English Wikipedia URL for each subject. The output is an intermediate file, `adb_wiki_links.csv`, which now contains a clean `Subject_Name` column.
```bash
    # Find Wikipedia links for all raw records
    pdm run find-links
```

2.  **Subject Qualification (`qualify_subjects.py`):** This script takes the list of found links and performs an intensive content-level validation. It resolves redirects, handles disambiguation pages, and validates the subject's name. Critically, it uses **Wikidata as the single source of truth** to confirm a subject's life status, ensuring the highest possible data integrity by relying exclusively on structured data. The final output is the detailed `adb_validated_subjects.csv`.
    
    **A Note on Reproducibility:** Because Wikipedia and Wikidata are dynamic sources, this validation is not perfectly reproducible. For direct replication, the study's pipeline relies on the static `adb_validated_subjects.csv` file included in the repository.
```bash
    # Qualify subjects by validating their Wikipedia/Wikidata entries
    pdm run qualify-subjects
```

3.  **Final Filtering (`select_eligible_candidates.py`):** This script integrates the raw data with the Wikipedia validation report and applies the following additional criteria in order:

| Criteria | Rule | Purpose |
| :--- | :--- | :--- |
| **Wikipedia Validation** | `Status` must be `OK` | Ensures a valid English Wikipedia page was found where the name and death date were confirmed. |
| **Entry Type** | `EntryType` must be `Person` | Filters out non-person records (e.g., events, research entries). |
| **Birth Year Range** | Must be between 1900-1999 | Controls for cohort-specific confounds by ensuring a homogenous historical period. |
| **Hemisphere** | `Latitude` must contain 'N' | Controls for the potential confound of a zodiacal shift for Southern Hemisphere births. |
| **Valid Time Format** | Birth time must be present and `HH:MM` | Ensures data is complete for precise calculations. |
| **Deduplication** | Unique Name + Birth Date | Removes duplicate entries from the source database. |
    
    The final output of this stage is the `adb_eligible_candidates.txt` file.
```bash
    # Create the list of eligible candidates
    pdm run select-eligible
```

##### Stage 3: LLM-based Candidate Selection (Optional)

###### Determining the Optimal Cutoff Parameters

To ensure the parameters for the final candidate selection algorithm were chosen objectively and were optimally tuned to the specific shape of the dataset's variance curve, a **systematic sensitivity analysis** was performed using the dedicated `scripts/analysis/analyze_cutoff_parameters.py` utility. The search space for the parameters was iteratively expanded to ensure a true global optimum was found.

The script performed a grid search over a wide range of values for `cutoff_search_start_point` and `smoothing_window_size`. To avoid overfitting to a single "best" result, a more robust **consensus-based recommendation algorithm** is now used. This algorithm first identifies a stable, consensus ideal cutoff by averaging the `Ideal Cutoff` values from the top-performing parameter sets (those with the lowest error). It then selects the single parameter set from the entire grid that most accurately predicts this consensus value, using a normalized sum of the parameters as a tie-breaker. This approach is superior because it prioritizes the stability of the *result* (the cutoff point) over the stability of the parameters themselves, yielding a more direct and reliable recommendation.

The final, expanded analysis revealed several key insights:

*   **The Variance Curve Shape:** The cumulative variance curve has a distinct shape: a long, steep decline followed by a wide, shallow plateau containing long-wavelength, low-amplitude noise.
*   **Justification for the Final Parameters:** A high `cutoff_search_start_point` (3500) was essential to force the analysis to begin *on the plateau*, isolating the region of interest. A large `smoothing_window_size` (800) was necessary to average out the long, shallow waves on the plateau, allowing the slope-based algorithm to detect the true global trend.

Based on this comprehensive, data-driven analysis, the following optimal parameters were chosen and set in `config.ini`:

*   `cutoff_search_start_point = 3500`
*   `smoothing_window_size = 800`

The plot of this analysis (see Figure S8) provides a clear visual justification for these choices. It shows how the algorithm, using these parameters, correctly identifies the start of the curve's final plateau, resulting in a final cutoff of 4,954 subjects.

{{grouped_figure:docs/images/variance_curve_analysis.png | caption=Figure S8: Variance Curve Analysis for Optimal Cohort Selection. The plot shows the raw cumulative variance, the smoothed curve (800-pt moving average), the search start point (3500), and the final data-driven cutoff (4954).}}

##### Stage 4: Profile Generation

This is the final stage, which assembles the personality profiles for the selected candidates. It involves a mix of automated and manual steps.

1.  **Formatting (`prepare_sf_import.py`):** Formats the final subject list for import into the Solar Fire software. It performs a critical data integrity step by encoding each subject's unique `idADB` into a Base58 string and injecting it into the `ZoneAbbr` field. This technique allows a unique identifier to pass through the manual software step unharmed, ensuring a perfect data merge later. The `ZoneAbbr` field was specifically chosen as it is a text field that is passed through the software without modification and is not used in any astrological calculations, making it a safe channel for this data.
```bash
    # Prepare the final list for the manual import step
    pdm run prep-sf-import
```

2.  **Manual Processing (Solar Fire):** The formatted file is imported into Solar Fire, which calculates the required celestial data and exports it as `sf_chart_export.csv`.

3.  **Neutralization (`neutralize_delineations.py`):** This script uses a powerful hybrid strategy to rewrite the esoteric source texts.
    
    *   **Fast Mode (`--fast`):** For initial runs, this mode bundles tasks into large, high-speed API calls (e.g., all 12 "Sun in Signs" delineations at once). This is highly efficient but may fail on some large tasks.
    *   **Robust/Resume Mode (default):** For resuming or fixing failed runs, this mode processes each of the 149 delineations as a separate, atomic task. This granular approach is slower but guarantees completion by solving potential response truncation issues from the LLM.
```bash
    # Perform the initial, high-speed neutralization
    pdm run neutralize --fast
    
    # Automatically fix any failed tasks from the fast run
    pdm run neutralize
```

###### Advanced State Machine for Step 11: Neutralize Delineations

Step 11 implements a sophisticated state machine that intelligently determines when re-processing is needed based on file timestamps, LLM model changes, and completion status. This ensures optimal efficiency while maintaining data integrity.

**State Machine Rules:**

1. **Preserve last-run information** in `data\reports\pipeline_completion_info.json`

2. **Check conditions in the order that follows**

3. **Complete Re-processing Required** - The step will back up and remove all files, then process the step fully if:
   * The last modified timestamp (LMTS) of `sf_delineations_library.txt` is newer than the earliest LMTS of the files in the `data\foundational_assets\neutralized_delineations` folder
   * The LLM is different from the LLM used the last time this step was executed

4. **Partial Processing Allowed** - If `sf_delineations_library.txt` LMTS < MIN(LTSM of files in folder) AND last_LLM = current_LLM:
   * If not all 6 files are complete in the directory, process only the missing/incomplete file(s)
   * If all 6 files are complete in the folder, skip the step entirely

**Status Types:**
- **COMPLETE**: All 6 expected files are present with correct line counts
- **PARTIAL**: Some files are present but others are missing or incomplete
- **MISSING**: No files exist in the output directory
- **STALE**: Files exist but need re-processing due to newer source or different LLM

**Completeness Validation:**
For each expected file, the system validates both existence and content completeness:

| File | Required Lines | Description |
|------|----------------|-------------|
| `balances_elements.csv` | 8 lines | Element balance calculations |
| `balances_hemispheres.csv` | 4 lines | Hemisphere balance calculations |
| `balances_modes.csv` | 6 lines | Mode balance calculations |
| `balances_quadrants.csv` | 4 lines | Quadrant balance calculations |
| `balances_signs.csv` | 12 lines | Sign balance calculations |
| `points_in_signs.csv` | 144 lines | Point-in-sign delineations |

**Line Count Validation:**
Completeness checks include validation of non-blank lines to ensure files contain the expected amount of processed content, not just empty or truncated files.

**Dependency Chain:**
When Step 11 is not COMPLETE, steps 12 (Create Subject Database) and 13 (Generate Personalities Database) will be automatically reprocessed to ensure data consistency throughout the pipeline.

#### Force Mode File Management

When using the `-Force` flag with `prepare_data.ps1`, the pipeline performs a comprehensive backup and removal of existing data artifacts before re-running the entire pipeline from scratch. This section documents exactly which files are affected and which are preserved.

**Files Backed Up and Removed:**

The following files and directories are backed up to `data/backup/` with timestamps and then removed:

**Primary Pipeline Outputs:**
- `data/sources/adb_raw_export.txt` - Raw data from Astro-Databank
- `data/processed/adb_wiki_links.csv` - Wikipedia links for candidates
- `data/processed/adb_validated_subjects.csv` - Wikipedia validation results
- `data/intermediate/adb_eligible_candidates.txt` - Filtered candidate list
- `data/foundational_assets/eminence_scores.csv` - LLM-generated eminence scores
- `data/foundational_assets/ocean_scores.csv` - LLM-generated OCEAN scores
- `data/intermediate/adb_final_candidates.txt` - Final selected candidates
- `data/intermediate/sf_data_import.txt` - Formatted data for Solar Fire
- `data/foundational_assets/sf_chart_export.csv` - Export from Solar Fire
- `data/foundational_assets/sf_delineations_library.txt` - Delineations library
- `data/processed/subject_db.csv` - Integrated subject database
- `data/personalities_db.txt` - Final personalities database

**Special Handling:**
- `data/foundational_assets/neutralized_delineations/` - Entire directory (backed up as ZIP)
- `data/foundational_assets/sf_chart_export.*` - All file extensions (wildcard handling)

**Summary and Report Files:**
- `data/reports/adb_validation_summary.txt` - Validation summary
- `data/reports/delineation_coverage_map.csv` - Coverage analysis
- `data/reports/missing_eminence_scores.txt` - Missing eminence scores report
- `data/reports/missing_ocean_scores.txt` - Missing OCEAN scores report
- `data/reports/missing_sf_subjects.csv` - Missing Solar Fire subjects report
- `data/reports/eminence_scores_summary.txt` - Eminence scoring summary
- `data/reports/ocean_scores_summary.txt` - OCEAN scoring summary

**Files Preserved (Not Removed):**

The following essential configuration and analysis files are preserved during a force re-run:

**Configuration Data:**
- `data/config/adb_research_categories.json` - Research categories configuration
- `data/foundational_assets/adb_category_map.csv` - Category mapping
- `data/foundational_assets/balance_thresholds.csv` - Balance calculation thresholds
- `data/foundational_assets/country_codes.csv` - Country code mappings
- `data/foundational_assets/point_weights.csv` - Astrological point weights

**Core Reference Files:**
- `data/base_query.txt` - Base LLM query template

**Assembly Logic Validation:**
- `data/foundational_assets/assembly_logic/personalities_db.assembly_logic.txt`
- `data/foundational_assets/assembly_logic/subject_db.assembly_logic.csv`

**Analysis Results:**
- `data/reports/cutoff_parameter_analysis_results.csv` - Cutoff analysis results
- `data/reports/variance_curve_analysis.png` - Variance curve plot

**Backup Process:**
- All removed files are backed up to `data/backup/` with timestamps
- Files are backed up individually with format: `filename.YYYYMMDD_HHMMSS.bak`
- Directories are compressed to ZIP format: `directoryname.YYYYMMDD_HHMMSS.zip`
- The backup process is atomic - if any backup fails, the entire force operation is halted

**Rationale for File Selection:**
- **Removed files** are all generated data artifacts that will be recreated by the pipeline
- **Preserved files** are either external reference data, user configurations, or expensive analysis results that don't change during pipeline re-runs

This selective approach ensures that expensive-to-generate analysis results and user configurations are preserved while still enabling a complete fresh run of the data generation pipeline.

4.  **Integration (`create_subject_db.py`):** Bridges the manual step by reading the Solar Fire chart export, decoding the unique `idADB` from the `ZoneAbbr` field, and merging the chart data with the final subject list to produce a clean master database.
```bash
    # Integrate the manual chart data export
    pdm run create-subject-db
```

5.  **Generation (`generate_personalities_db.py`):** Assembles the final `personalities_db.txt` by combining the subject data with the neutralized delineation library according to a deterministic algorithm.
```bash
    # Generate the final personalities database
    pdm run gen-db
```
    
    The output is `personalities_db.txt`, a tab-delimited file with the fields: `Index`, `Name`, `BirthYear`, and `DescriptionText`.

This combination of automated scripts and well-defined manual steps ensures the final dataset is both high-quality and computationally reproducible.

#### Solar Fire Integration and Configuration

This section provides detailed instructions for integrating Solar Fire astrology software into the data preparation pipeline. Solar Fire is used for calculating celestial positions and exporting chart data that serves as input for the personality generation algorithm.

##### Software Requirements

Solar Fire v9.0.3 is the recommended version for this framework. The software is available at https://alabe.com/solarfireV9.html.

##### File Locations

Solar Fire stores its user files in the standard Windows Documents folder. Understanding these locations is helpful for managing the import/export process:

*   **Solar Fire User Files:** `Documents\Solar Fire User Files`
*   **Subdirectories:**
    *   **Charts:** Chart ('*.sfcht') files accessible within Solar Fire
    *   **Import:** Imported birth data files (various formats)
    *   **Export:** Exported astrological data (various formats)
    *   **Points & Colors:** Settings ('*.pts') file for Displayed Points
    *   **Interpretations:** Delineations library ('*.def') files

##### One-Time Software Setup

The following configuration steps only need to be performed once. After the initial setup, you can proceed directly to the **Import/Export Workflow**.

###### 1. Configure Chart Points

You must define which astrological points are included in the calculations.

*   **Menu:** `Chart Options > Displayed Points...`
*   **Action:**
    1.  Create a new `.pts` file (e.g., `llm_narrative_dp12.pts`).
    2.  Edit this file to include exactly these 12 points: Sun, Moon, Mercury, Venus, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto, Ascendant, and Midheaven.
    3.  Save the file and ensure it is selected as the active set.

{{grouped_figure:docs/images/sf_images/sf_setup_1_displayed_points.png | width=60% | caption=Solar Fire "Displayed Points" dialog configured with the 12 required chart points.}}

###### 2. Configure Preferences

Ensure the core calculation settings match the study's methodology.

*   **Menu:** `Preferences > Edit Settings...`
*   **Action:** Verify the following default settings are active:
    *   **'Places' tab -> Atlas:** `ACS (Built-in)`
    *   **'Calculations' tab -> MC in Polar Regions:** `Always above horizon`
    *   **'Zodiac' tab -> Default Zodiac:** `Tropical`
    *   **'AutoRun' tab -> Astrologer's Assistant:** Ensure this is cleared and no tasks run on startup.

###### 3. Define Data Formats

You must define the data structure for both importing and exporting. Solar Fire maintains **separate** format lists for each, so this process must be done twice.

**a. Define Import Format**

*   **Menu:** `Utilities > Chart Import/Export...`
*   **Action:**
    1.  Go to the **'Options' tab** and click **'Edit ASCII Formats...'**.
    2.  Create a **new format definition**.
    3.  Set **'Record Format'** to `Comma Quote Delimited`.
    4.  Configure **'Fields in each record'** to contain exactly these 9 fields in this specific order: Name/Description, Date (String), Time (String), Zone Abbreviation, Zone Time (String), Place Name, Country/State Name, Latitude (String), Longitude (String).
    5.  Save the format as `CQD Import`.

{{grouped_figure:docs/images/sf_images/sf_setup_2_import_format.png | width=80% | caption=Solar Fire "Edit ASCII Formats" dialog configured for the CQD Import format.}}

**b. Define Export Format**

*   **Menu:** `Chart > Export Charts as Text...`
*   **Action:**
    1.  Click the **'Edit ASCII...'** button to open the format definitions dialog.
    2.  **This list is separate from the import list.** You must create another **new format definition**.
    3.  Repeat the exact same configuration as the import format, set it to `Comma Quote Delimited`, and add the same 9 fields in the same order.
    4.  Save the format as `CQD Export`. This ensures both workflows use an identical data structure.

{{grouped_figure:docs/images/sf_images/sf_setup_3_export_format.png | width=80% | caption=Solar Fire "Export Chart Data" format dialog configured for the CQD Export format.}}

##### Import/Export Workflow

After completing the one-time setup, follow this workflow to process the data.

###### Pre-flight Check: Clearing Existing Chart Data (For Re-runs)
If you are re-running the import process, you must first clear the existing charts from your Solar Fire charts file to avoid duplicates.

*   **Menu:** `Chart > Open...`
*   **Action:**
    1.  In the **'Chart Database'** dialog, select your charts file (e.g., `adb_candidates.sfcht`).
    2.  Click the **'All'** button to highlight every chart in the file.
    3.  Click the **'Delete...'** button, then select **'Selected Charts...'**.
    4.  A dialog will ask: "Do you wish to confirm the deletion of each chart individually?". Click **'No'** to delete all charts at once.
    5.  Click **'Cancel'** to close the 'Chart Database' dialog. The file is now empty and ready for a fresh import.

{{grouped_figure:docs/images/sf_images/sf_workflow_1_clear_charts.png | width=95% | caption=Solar Fire "Chart Database" dialog with all charts selected for deletion.}}

###### Step 1: Import Birth Data
The procedure below is for the production workflow. When validating the Personality Assembly Algorithm, choose `sf_data_import.assembly_logic.txt` in the Solar Fire import folder for #2 and save to `adb_candidates.assembly_logic` for #3.

*   **Menu:** `Utilities > Chart Import/Export...`
*   **Action:**
    1.  If a **"Confirm"** dialog appears immediately, click **'OK'**.
    2.  On the **'Import From' tab**, select `ASCII files` and choose `sf_data_import.txt` in the import folder.
    3.  On the **'Save To' tab**, ensure your `adb_candidates.sfcht` file is selected.
    4.  On the **'Options' tab**, select your `CQD Import` format.
    5.  Click the **'Convert'** button.
    6.  Once the import completes, click the **'Quit'** button to close the dialog.

{{grouped_figure:docs/images/sf_images/sf_workflow_2_import_dialog.png | width=95% | caption=Solar Fire "Chart Import/Export" dialog configured to import the prepared data.}}

###### Step 2: Calculate All Charts
The procedure below is for the production workflow. When validating the Personality Assembly Algorithm, select `adb_candidates.assembly_logic` for #1.

*   **Menu:** `Chart > Open...`
*   **Action:**
    1.  Select the charts file you just created (e.g., `adb_candidates.sfcht`). If the list of charts is empty, navigate away to a different file and then back using the **'File'** button.
    2.  Click the **'All'** button to select all charts in the file.
    3.  Click the **'Open...'** button. This will calculate all charts and add them to the "Calculated Charts" list. The processing time will vary depending on the number of subjects (typically a few minutes for each set of 1,000 charts).

> **A Note on Character Encoding:** In the "Calculated Charts" list, you may notice that some names with international characters appear corrupted (e.g., `PelÃƒÂ©` instead of `PelÃ©`). This is an expected display issue within Solar Fire. **Do not attempt to fix these names manually.** The automated scripts are designed to detect and repair these encoding errors in the next stage, ensuring the final database is clean.

###### Step 3: Export Chart Data
The procedure below is for the production workflow. When validating the Personality Assembly Algorithm, browse to the Solar Fire export folder and set the filename to `sf_chart_export.assembly_logic.csv` for #6.

*   **Menu:** `Chart > Export Charts as Text...`
*   **Action:**
    1.  In the "Calculated Charts" window, select all calculated charts.
    2.  In the menu item's **"Export Chart Data" dialog**, check the **'Chart Details'** and **'Column Types'** boxes.
    3.  Under **'Select types of points'**, ensure **'Chart Points'** is selected.
    4.  For the ASCII format, select your custom `CQD Export` format.
    5.  Set **'Field Delimiters'** to `Comma Quote (CQD)` and 'Destination' to `Export to File`.
    6.  Browse to the export directory, set the filename to `sf_chart_export.csv`, and click **Save**. Note: 'Save as type' cannot be set in this dialog.
    7.  **Warning:** Solar Fire will overwrite this file without confirmation. Click **'Export'**.
    8.  Once the export completes successfully, click the **'Quit'** button to close the dialog.

**Important Note**: The pipeline will automatically detect and copy the `sf_chart_export.csv` file from your Solar Fire Export directory (typically `Documents\Solar Fire User Files\Export\`) to the project's data directory. You do not need to manually copy this file.

The pipeline will also check for the existence of the Solar Fire export file during the "Astrology Data Export (Manual)" step. The status will be:

- **[MISSING]**: File not found in Solar Fire Export directory. Pipeline halts with instructions.
- **[PENDING]**: File found but not processed yet. Step skipped, file fetched by next step.
- **[COMPLETE]**: File already processed and copied. Step skipped.

Similarly, for the "Delineations Library Export (Manual)" step:

- **[MISSING]**: `Standard.def` file not found in Solar Fire Interpretations directory. Pipeline halts with instructions.
- **[PENDING]**: File found but not processed yet. Step skipped.
- **[COMPLETE]**: File already processed and copied. Step skipped.

{{grouped_figure:docs/images/sf_images/sf_workflow_3_export_dialog.png | width=75% | caption=Solar Fire "Export Chart Data" dialog configured for the final chart data export.}}

The exported file consists of a repeating 14-line block for each subject:

| Line(s) | Content/Fields | Description |
| :--- | :--- | :--- |
| 1 | `Name`, `Date`, `Time`, `ZoneAbbr`, `ZoneOffset`, `Place`, `State`, `Lat`, `Lon` | Subject's core birth data. `idADB` is encoded into `ZoneAbbr` field. |
| 2 | `"Body Name","Body Abbr","Longitude"` | Header line for planetary data. |
| 3-14 | `Point Name`, `Point Abbr`, `Zodiacal Longitude` | Data for each of the 12 chart points. |

The entire file consists of `N * 14` lines, where `N` is the final number of subjects.

###### Special Step: Generate Interpretation Reports (Validation Only)

This procedure only applies to Personality Assembly Algorithm validation (`pdm run test-assembly-setup`).

*   **Menu:** `Interps > Full Report...`
*   **Action:**
    1.  In the "Calculated Charts" window, select the first calculated chart.
    2.  In the "Select Text for Report" dialog, select only 'Balances' and 'Chart Points' for 'Text Categories'.
    3.  Click View. The report opens in your default word processor.
    4.  Save As 'Plain Text' in `Documents/Solar Fire User Files/Export` as `sf_raw_report.assembly_logic_[SN]` where [SN] is sequence number 1-17.
    5.  Click Cancel in the "Select Text for Report" dialog and select the next chart.
    6.  Repeat steps 2-5 for all charts (#2-#17), using sequence numbers in filenames.
    7.  Once all reports exported, click Cancel and resume validation.

##### Exporting the Delineations Library (One-Time Task)

The personality descriptions are assembled from a library of pre-written text components that must first be exported from Solar Fire.

*   **Menu:** `Interps > Interpretation Files > Natal`
*   **Action:**
    1.  Select `Standard.int` and click **'Edit'**.
    2.  In the 'Interpretations Editor', go to `File > Decompile...` and save the file. This creates `Standard.def` in `Documents\Solar Fire User Files\Interpretations`.
    3.  Copy this file to `data/foundational_assets/` and rename to `sf_delineations_library.txt`. Note: Filename extensions must be displayed for this rename.

This stage is a second, optional filtering pass that uses LLMs to score the "eligible candidates" to determine the final, smaller subject pool. This entire stage can be skipped by setting `bypass_candidate_selection = true` in `config.ini`.

1.  **Eminence Scoring (`generate_eminence_scores.py`):** Processes the eligible candidates list to generate a calibrated eminence score for each, producing a rank-ordered list that now includes `BirthYear`.
    
    **Tiered Approach for Missing Subjects:**
    The script implements a tiered approach for handling subjects that fail to receive scores due to API issues or other problems:
    
    - **â‰¥99% completion**: The pipeline continues with a simple notification, treating minor missing data as acceptable
    - **95-98% completion**: The pipeline continues but displays a prominent warning with recommended action
    - **<95% completion**: The pipeline stops with an error, indicating significant data issues that need attention
    
    This approach prevents the pipeline from stopping for minor issues (e.g., missing 99 out of 7,194 subjects) while still providing visibility and guidance when more substantial issues occur.
```bash
    # Generate eminence scores for all eligible candidates
    pdm run gen-eminence
```

2.  **OCEAN Scoring (`generate_ocean_scores.py`):** A fully automated, resilient script that generates OCEAN personality scores for every subject in the eminence-ranked list.
    
    **Tiered Approach for Missing Subjects:**
    Like the eminence scoring script, this implements the same tiered approach for handling incomplete scoring:
    
    - **â‰¥99% completion**: The pipeline continues with a simple notification
    - **95-98% completion**: The pipeline continues with a prominent warning
    - **<95% completion**: The pipeline stops with an error
    
    Both scripts write completion information to a shared JSON file (`data/reports/pipeline_completion_info.json`) that is used by the final pipeline report to provide a comprehensive data completeness overview. This file tracks completion status and metrics for critical pipeline steps, containing completion rates, missing subject counts, and paths to detailed missing subject reports. The pipeline orchestrator uses this file to determine if steps completed successfully or require intervention.
```bash
    # Generate OCEAN scores to determine the final cutoff
    pdm run gen-ocean
```

3.  **Final Selection & Cutoff (`select_final_candidates.py`):** Performs the final filtering and selection. It takes the complete OCEAN scores list and applies a sophisticated, data-driven algorithm to determine the optimal cohort size. The script first calculates the cumulative personality variance curve for the entire cohort, smooths this curve using a moving average to eliminate local noise, and then performs a slope analysis to find the "plateau"â€”the point of diminishing returns where adding more subjects no longer meaningfully contributes to the psychological diversity of the pool. It then resolves country codes and sorts the final list by eminence.
```bash
    # Create the final, transformed list of subjects
    pdm run select-final
```

###### State Machines for Steps 5 (Generate Eminence Scores) and 6 (Generate OCEAN Scores)

Steps 5 and 6 implement sophisticated state machines that track both the primary output files and summary files to determine completion status. These state machines ensure accurate detection of partial completion and provide detailed reporting for the Data Completeness Report.

**State Machine Rules:**

1. **Dual-File Validation** - Each step validates both:
   - The primary output CSV file (`eminence_scores.csv` or `ocean_scores.csv`)
   - The corresponding summary report file (`eminence_scores_summary.txt` or `ocean_scores_summary.txt`)

2. **Completion Verification** - A step is considered COMPLETE only when:
   - The summary file exists and is parseable
   - The number of scored subjects equals the total number of subjects in the source
   - The primary output file exists

3. **Status Determination Logic:**
   - **MISSING**: Neither summary file nor output file exists
   - **INCOMPLETE**: Either the summary file doesn't exist, the scored count doesn't match the total, or the output file doesn't exist
   - **COMPLETE**: All validation checks pass

**Summary File Parsing:**
The state machine parses the summary files to extract key metrics:
- `Total Scored`: Number of subjects successfully processed
- `Total in Source`: Total number of subjects in the input source

**Example Summary File Format:**
```
Eminence Scoring Summary
========================
Total in Source: 7,234
Total Scored: 7,229
Success Rate: 99.9%
Missing Subjects: 5
...
```

**Tiered Response to Missing Subjects:**
The state machine supports a tiered approach to handling incomplete scoring:

| Completion Rate | Pipeline Response | Color Coding |
|-----------------|------------------|--------------|
| â‰¥99% | Continue with notification | Green |
| 95-98% | Continue with prominent warning | Yellow |
| <95% | Halt with error | Red |

**Data Completeness Integration:**
Both steps write completion information to the shared JSON file (`data/reports/pipeline_completion_info.json`) which includes:
- Completion rate percentage
- Count of missing subjects
- Path to detailed missing subject report
- LLM model used for scoring

This information is used by the final Data Completeness Report to provide users with actionable guidance on how to address data quality issues.

**Resumption Logic:**
When a step is resumed:
- The state machine first checks the summary file to determine completion status
- If incomplete, the scoring scripts will automatically process only the missing subjects
- This enables efficient resumption without re-scoring already processed subjects

**Error Handling:**
If the summary file is corrupted or unparseable, the state machine falls back to a conservative INCOMPLETE status, ensuring data integrity is maintained.

### Experiment & Study Workflow

The **Experiment & Study Workflow** consumes the prepared data to generate experimental results across multiple conditions, then compiles these results into comprehensive statistical analyses. This workflow supports factorial experimental designs, automated result aggregation, and publication-ready statistical reporting.

The two workflows are connected through well-defined data interfaces, with the output of the data preparation pipeline serving as the input to the experiment workflow. This modular design allows researchers to update or extend either workflow independently while maintaining reproducibility.

{{grouped_figure:docs/diagrams/flow_experiment_study_workflow.mmd | scale=2.0 | width=35% | caption=Figure S3: Experiment & Study Workflow. The workflow uses personalities_db.txt to run experiments and compile study results.}}

## Replication Procedures

### Direct Replication Procedure

This procedure validates computational reproducibility by using the static data files (located in the `data/` subdirectory) and randomization seeds included in this repository to verify that the framework produces the same statistical results as the original study.

1. **Set Up Environment**: Follow the setup instructions in Appendix A.1.

2. **Verify Configuration**: Ensure `config.ini` matches the original study parameters (see Appendix A.1).

3. **Run Experiments**: Execute the experiments using the provided static data:
   ```powershell
   # For each experimental condition
   ./new_experiment.ps1
   ```

4. **Compile Results**: Once all experiments are complete:
   ```powershell
   ./compile_study.ps1 -StudyDirectory "output/studies/Original_Study"
   ```

5. **Compare Results**: Verify your results match the reported findings.

### Methodological Replication Procedure

This procedure tests the robustness of the findings by using the framework's automated tools to generate a fresh dataset from the live Astro-Databank, allowing researchers to verify that the results are not an artifact of a specific dataset.

1. **Set Up Environment**: Follow the setup instructions in Appendix A.2.

2. **Generate Fresh Dataset**: Create a new dataset from the live Astro-Databank database:
```powershell
   # Run the entire data preparation workflow
    pdm run prep-data
    # or simply:
    pdm prep-data
```
This executes `prepare_data.ps1`, which is a PowerShell wrapper that orchestrates the **14 distinct steps** of the data preparation pipeline (including automated Python scripts and manual processes). This script automatically checks the state of the pipeline and resumes from the first incomplete step, and it will pause with clear instructions when a manual user action is required.

It is highly recommended that you first run this module in read-only mode to produce a report on current data files (use the '-ReportOnly' parameter). Subsequently, it is advisable to step through execution in interactive mode to understand what can be expected on a normal run (use the '-Interactive' paramater). Once the script's operation is clear, use the '-Force' parameter to overwrite existing data.

3. **Verify Configuration**: Ensure `config.ini` matches the original study parameters (see Appendix A.1).

4. **Run Experiments**: Execute the experiments with interactive parameter selection:
```powershell
   # For each experimental condition (repeat 42 times for 2Ã—3Ã—7 design)
   ./new_experiment.ps1
```
   
   The script will display an interactive menu for selecting experimental conditions. Alternatively, manually configure `config.ini` before each run (see Appendix A.2 for details).

5. **Compile Results**: Once all experiments are complete:
```powershell
   ./compile_study.ps1 -StudyDirectory "output/studies/Methodological_Replication"
```

6. **Compare Results**: Compare your results with the original study to assess robustness.

### Conceptual Replication Procedure

This procedure extends the research by modifying the framework itself, enabling researchers to test new hypotheses, explore alternative methodologies, or apply the framework to entirely different research questions.

1. **Set Up Environment**: Follow the setup instructions in Appendix A.3.

2. **Modify Framework**: Implement your conceptual extensions (see "Suggestions for Future Research" section).

3. **Generate Dataset**: Create a dataset appropriate for your modified framework:
```powershell
   # May use prepare_data.ps1 or custom scripts
```

4. **Configure Study Design**: Update `config.ini` for your experimental design (example):
```ini
   [Study]
   mapping_strategy = your, custom, values
   group_size = your, custom, values
   model_name = your, custom, models
   temperature = your, custom, values
   ...
   
   [Experiment]
   num_replications = your custom value
   num_trials = your custom value

   [LLM]
   max_tokens = your custom value
   max_parallel_sessions = your custom value
```

5. **Run Experiments**: Execute the experiments with your modified framework:
```powershell
   # For each experimental condition
   ./new_experiment.ps1
```

6. **Compile Results**: Once all experiments are complete:
```powershell
   ./compile_study.ps1 -StudyDirectory "output/studies/Conceptual_Replication"
```

7. **Analyze Results**: Interpret your findings in the context of your conceptual extensions.

## Expected Results for Validation

This section provides baseline performance metrics from the framework's validation testing to help researchers verify their replication results.

### Performance Benchmarks

The framework has been validated using controlled test conditions with known ground truth. Researchers should expect performance metrics within these ranges when using similar experimental parameters:

**For Correct Mappings (signal present):**
- Mean MRR: 0.15-0.25 (vs chance ~0.10-0.25 depending on k)
- Top-1 Accuracy: 8-15% (vs chance ~7-14% depending on k)
- Top-3 Accuracy: 20-35% (vs chance ~20-43% depending on k)

**For Random Mappings (null condition):**
- Performance should approximate chance levels
- Statistical tests should show p > 0.05 for most metrics

### Chance Level Calculations

The framework calculates theoretical chance levels based on group size (k):

- **MRR Chance**: `(1/k) Ã— Î£(1/j)` for j=1 to k (harmonic mean formula)
- **Top-1 Accuracy Chance**: `1/k`
- **Top-3 Accuracy Chance**: `min(3, k)/k`
- **Mean Rank Chance**: `(k + 1)/2`

### Quality Control Thresholds

Experiments must meet these minimum standards for inclusion in analysis:

- **Minimum valid responses**: 25 per replication (configurable in `config.ini`)
- **Parsing success rate**: >92% for k=14, >95% for kâ‰¤10
- **Expected valid responses** (with 80 trials):
  - k=7: ~77 valid responses (96% success rate)
  - k=10: ~76 valid responses (95% success rate)
  - k=14: ~74 valid responses (93% success rate)

### Validation Checklist

âœ“ Performance metrics fall within expected ranges
âœ“ Chance level calculations match theoretical values
âœ“ Random mapping conditions approximate chance performance
âœ“ Statistical significance patterns align with experimental design
âœ“ Parsing success rates meet minimum thresholds

Significant deviations from these benchmarks may indicate configuration issues, API problems, or changes in model behavior. See the **[ðŸ“– Framework Manual](docs/FRAMEWORK_MANUAL.md)** for troubleshooting guidance.

## Suggestions for Future Research

Conceptual replication offers numerous opportunities to extend the research framework. These innovations can be categorized into three main types:

- **Methodological Parameter Innovations**: Modifications to the core algorithms, models, and analytical approaches (e.g., using different LLMs for scoring/evaluation, alternative candidate selection algorithms, different astrological parameters)
- **Operational Parameter Innovations**: Changes to the data sources, software tools, and infrastructure (e.g., different biographical databases, alternative astrological software, different LLM providers)
- **Narrative Parameter Innovations**: Applications to entirely different signal systems, cultural contexts, or temporal domains (e.g., numerology, MBTI, cross-cultural applications, longitudinal studies)

For detailed examples and guidance on each innovation type, see the **[ðŸ“– Framework Manual](docs/FRAMEWORK_MANUAL.md)** "Extending the Framework" section.

## Appendices

### Appendix A: Setup and Configuration by Replication Path

#### A.1 Direct Replication Setup

**Software Requirements:**
- Python 3.11 or higher
- PowerShell 7.0 or higher (cross-platform)
- Git
- PDM (Python Dependency Manager)

**Accounts and Services:**
- OpenRouter account with sufficient funds (approximately $1,500 for full study)
- No Astro-Databank account required (using static data)

**Installation Steps:**
1. Install PDM (one-time setup):
```powershell
   pip install --user pdm
```

2. Clone repository and install dependencies:
```powershell
   git clone [repository-url]
   cd llm-narrative-framework
   pdm install -G dev
```

3. Configure API key:
```powershell
   # Create .env file with OpenRouter API key
   "OPENROUTER_API_KEY=your-actual-api-key" | Out-File -FilePath .env -Encoding UTF8
```

4. Verify configuration:
```powershell
   # Check that config.ini matches original study parameters
   Get-Content config.ini
```

```ini
   [Study]
   mapping_strategy = correct, random
   group_size = 7, 10, 14
   model_name = anthropic/claude-sonnet-4, google/gemini-2.0-flash-001, meta-llama/llama-3.3-70b-instruct, openai/gpt-4o, deepseek/deepseek-chat-v3.1, qwen/qwen-2.5-72b-instruct, mistralai/mistral-large-2411
   
   [Experiment]
   num_replications = 30
   num_trials = 80

   [LLM]
   temperature = 0.0
   max_tokens = 8,192
   max_parallel_sessions = 10
```

#### A.2 Methodological Replication Setup

**All requirements from A.1 plus:**

**Additional Accounts and Services:**
- Astro-Databank account at astro.com
- Sufficient OpenRouter funds for data generation (additional $500-900)

**Additional Software:**
- Solar Fire software (required for manual data preparation steps)

**Additional Configuration:**
1. Configure Astro-Databank credentials in `.env`:
   ```
   ADB_USERNAME=your-astro-username
   ADB_PASSWORD=your-astro-password
   ```

2. Verify data generation settings in `config.ini`:
   ```ini
   [DataGeneration]
   bypass_candidate_selection = false
   cutoff_search_start_point = 3500
   smoothing_window_size = 800
   ```

#### A.3 Conceptual Replication Setup

**All requirements from A.1 and A.2 as needed, plus:**

**Additional Software (depending on innovation):**
- Alternative astrological software (if changing from Solar Fire)
- Custom development tools (if implementing new algorithms)
- Additional Python packages (add to pyproject.toml)

**Additional Configuration:**
- Custom configuration parameters as needed for your conceptual extensions
- Potential modifications to data structures and file formats
- Additional API keys or services as required by your innovations

### Appendix B: Troubleshooting Quick Reference

| Issue | Solution |
| :--- | :--- |
| **`pdm` command not found** | Use `python -m pdm` as an alternative |
| **API Errors during experiment** | Run `fix_experiment.ps1` to resume from failures |
| **Permission Denied with .docx** | Close files in Microsoft Word before rebuilding |
| **`git` command not found** | Install Git from git-scm.com |
| **All LLM sessions fail** | Verify model names and API credentials |
| **Repair process loops** | System limits retries to 3 cycles automatically |

For detailed troubleshooting, see the **[ðŸ“– Framework Manual](docs/FRAMEWORK_MANUAL.md)**.

### Appendix C: Models and Experimental Design

This appendix provides a comprehensive reference for both understanding the original study and designing new ones. It is organized into two parts: a specific reference for the original study's design and a general guide for researchers planning new experiments.

#### Original Study Reference

This section details the specific models, parameters, and design choices used in the original study.

##### Models Used in the Original Study

**Data Generation Models:**

| Purpose | Model Name | API Identifier |
| :--- | :--- | :--- |
| Eminence Scoring (LLM A) | OpenAI GPT-5 Chat | `openai/gpt-5-chat` |
| OCEAN Scoring (LLM B) | Anthropic Claude 4.5 Sonnet | `anthropic/claude-sonnet-4.5` |
| Neutralization (LLM C) | Google Gemini 2.5 Pro | `google/gemini-2.5-pro` |

*Note on Data Generation Models: The models listed above were accessed by the researchers in mid-2025 during a pre-release or early access phase. Their names reflect their designation at the time the study's foundational data was generated.*

**Evaluation Models (All Independent from Data Generation):**

**United States (4 models):**

| Purpose | Model Name | API Identifier | Provider | Parsing |
| :--- | :--- | :--- | :--- | :--- |
| Evaluation 1 (LLM D1) | Claude Sonnet 4 | `anthropic/claude-sonnet-4` | Anthropic | High |
| Evaluation 2 (D2) | Gemini 2.5 Flash Lite | `google/gemini-2.5-flash-lite` | Google | âš ï¸ Partial* |
| Evaluation 3 (LLM D3) | Llama 3.3 70B Instruct | `meta-llama/llama-3.3-70b-instruct` | Meta | High |
| Evaluation 4 (LLM D4) | GPT-4o | `openai/gpt-4o` | OpenAI | High |

**China (2 models):**

| Purpose | Model Name | API Identifier | Provider | Parsing |
| :--- | :--- | :--- | :--- | :--- |
| Evaluation 5 (LLM D5) | DeepSeek Chat V3.1 | `deepseek/deepseek-chat-v3.1` | DeepSeek | 100% |
| Evaluation 6 (LLM D6) | Qwen 2.5 72B Instruct | `qwen/qwen-2.5-72b-instruct` | Alibaba | 92% |

**Europe (1 model):**

| Purpose | Model Name | API Identifier | Provider | Parsing |
| :--- | :--- | :--- | :--- | :--- |
| Evaluation 7 (LLM D7) | Mistral Large 2 2411 | `mistralai/mistral-large-2411` | Mistral AI | 98% |

*Original Study Timeframe Reference:*
- *Data Generation Pipeline Execution: October 16, 2025*
- *Main Experimental Runs: October 18-22, 2025*
- *Final Statistical Analysis: October 22-25, 2025*

**Model Selection Rationale:**

All evaluation models were selected to ensure complete independence from data generation models, eliminating potential contamination from model reuse. The evaluation set represents maximum diversity across key dimensions:

- **Geographic Distribution:** 57% US, 29% Chinese, 14% European - enabling cross-cultural bias assessment
- **Open-Source Representation:** 57% open-weights (Llama, DeepSeek, Qwen2.5, Mistral) for long-term reproducibility
- **Architectural Diversity:** Standard transformers (Claude, GPT, Llama), Gemini architecture, and Mixture-of-Experts (DeepSeek, Qwen2.5)
- **Parsing Reliability:** All models achieved â‰¥92% structured output success in pilot testing (n=50 trials each)

**Pilot Testing Exclusions:**

Several relatively recent models (i.e., those published within the past 12 months) were evaluated during pilot testing but were excluded from the final study for failing to meet the required reliability and performance criteria. Exclusions fell into three main categories:

-   **Low Parsing Success Rate (<90%):** These models failed to consistently produce structured, parsable output.
    -   `Qwen: Qwen3 235B A22B Instruct` (24% success)
    -   `Google: Gemma 3` (12B and 27B variants, 23% and 80% success)
    -   `Z.AI: GLM 4.5` (83% success)
    -   `OpenAI: gpt-oss-20b` (63% success)

-   **Technical Instability:** This model caused critical failures in the automated workflow.
    -   `Qwen: Qwen3 Next 80B A3B Instruct` (Caused infinite reprocessing loops, resulting in 0% effective completion).

-   **Atypical Architecture or Output Format:** These models, while functional, were excluded to avoid confounding the analysis or because their output was fundamentally incompatible with the parsing engine.
    -   `Qwen: Qwen3 30B A3B Instruct` (Although parsing was high, it used only 3.3B active parameters, making it a significant architectural outlier that would complicate interpretation).
    -   `Google: Gemini 2.5 Pro` and `Gemini 2.5 Flash` (Generated conversational, explanatory text rather than the required structured data table, making them incompatible with the automated parser).

Note: Free or rate-limited models available via OpenRouter were not included in pilot testing due to a lack of the reliable, high-throughput access required for large-scale automated experiments.

###### Experimental Design Reference

**Original Study Design:**
- 2Ã—3Ã—7 factorial design (42 conditions)
- 30 replications per condition (experiment)
- 80 trials per replication
- Total: 100,800 trials

###### Factor Justification

-   **Mapping Strategy (Between-Subjects):** This is the core experimental manipulation.
    -   `correct`: The experimental condition, testing the LLM's ability to detect the faint, systematic signal when it is present.
    -   `random`: The null/control condition, where profiles are shuffled. This establishes a chance-level baseline and tests whether the model is merely guessing or hallucinating patterns.
    -   This was treated as a between-subjects factor to ensure a clean design where a model is not "tipped off" to the existence of random trials within a single experiment.

-   **Group Size (`k`) (Within-Subjects):** This factor systematically varies the difficulty of the matching task.
    -   `k âˆˆ {7, 10, 14}`: This range was chosen to create a clear difficulty gradient. The step from 7 to 10, and 10 to 14, each represents an approximate 20% increase in difficulty (as measured by the decrease in chance-level MRR).
    -   `k=7` serves as an easier baseline, while `k=14` pushes the model's context-processing capabilities to test for performance degradation under high cognitive load.

-   **Model (Within-Subjects):** This factor tests for the generalizability of the findings across different LLMs.
    -   The seven evaluation models were chosen for maximum diversity in architecture, provider, and training data. A detailed justification for their selection is provided in the "Model Selection Rationale" section above.

###### Sample Size Justification

The choice of **30 replications** per condition and **80 trials** per replication was made to strike an optimal balance between statistical power, metric precision, and the practical constraints of cost and time.

**Why 30 Replications? (Statistical Power)**

-   **Purpose:** The number of replications is the sample size (`n`) for the statistical tests that compare conditions (e.g., Model A vs. Model B). A larger `n` increases the power to detect real differences.
-   **Justification:** A sample size of 30 is a well-established standard in experimental research for conducting robust ANOVA and t-tests. It provides sufficient statistical power (>80%) to reliably detect small-to-medium main effects and interactions (Cohen's d > 0.20).
-   **Trade-offs:**
    -   *Fewer Replications (<20)* would significantly reduce statistical power, increasing the risk of failing to detect a true effect (Type II error).
    -   *More Replications (>50)* would offer diminishing returns in power gain while linearly increasing the total cost and runtime of the study.

**Why 80 Trials? (Metric Precision and Resilience)**

-   **Purpose:** The number of trials determines the precision of the performance metrics (like MRR) *within a single replication*. Each replication's performance score is an average across all its trials; more trials lead to a more stable and less noisy estimate.
-   **Justification:** An 80-trial count provides a strong balance:
    1.  **Reduces Noise:** It smooths out the randomness of LLM performance on any single query, yielding a more reliable data point for each replication.
    2.  **Provides Resilience:** It creates a crucial buffer against real-world API and parsing failures. With a minimum threshold of 25 valid responses required for a replication to be included in the final analysis, 80 trials offer a substantial safety margin. Even with a parsing success rate as low as 90%, we can expect 72 valid responsesâ€”nearly 3 times the required minimum.
-   **Trade-offs:**
    -   *Fewer Trials (<50)* would make each replication's metrics more volatile and highly susceptible to a few outlier results. It would also reduce the buffer against parsing failures, risking data loss.
    -   *More Trials (>100)* would offer slightly more precision but at a direct, linear increase in cost per replication, representing a point of diminishing returns.

In summary, the 30x80 design was chosen as a robust and cost-effective standard that ensures the study is both statistically powerful and resilient to the practical challenges of large-scale LLM experimentation.

###### Statistical Analysis Plan

- **Primary Analysis:** Three-Way Mixed ANOVA (1 between-subjects, 2 within-subjects factors).
- **Effect Size Measures:** Eta-squared (Î·Â²) and Cohen's d.
- **Post-Hoc Tests:** Tukey HSD with Benjamini-Hochberg FDR correction for multiple comparisons.
- **Complementary Analysis:** Bayesian analysis (Bayes Factor) to quantify evidence for or against the null hypothesis.

---

#### Guidance for Designing New Studies

This section provides general principles for designing new multi-factor experiments for methodological or conceptual replication. Note: All cost estimates are in USD and based on OpenRouter.ai rates as of October 2025.

##### 1. Define Your Factors

The framework is built for factorial designs. Start by defining the independent variables (factors) you want to investigate. Common factors include:

-   **`mapping_strategy` (Between-Subjects):** The core experimental manipulation (e.g., `correct` vs. `random`).
-   **`group_size` (`k`) (Within-Subjects):** The difficulty of the matching task. Choose values that create a systematic difficulty gradient (e.g., an easy, medium, and hard condition like 7, 10, 14).
-   **`model_name` (Within-Subjects):** The LLMs you want to compare.

##### 2. Select Your Parameters

-   **Models:** When selecting models, consider a balance of:
    -   **Cost-Effectiveness:** Choose models that fit your budget.
    -   **Architectural Diversity:** Include models from different providers (e.g., OpenAI, Google, Anthropic, Meta) and with different architectures to test generalizability.
    -   **Parsing Reliability:** Models must consistently return structured, parsable data. Test this in a pilot run.
    -   **Independence:** For conceptual replications, ensure evaluation models are different from any models used in your data generation pipeline.

-   **Group Sizes (`k`):** Select a range of `k` values that meaningfully vary the task difficulty. The original study used {7, 10, 14} because they create a roughly 20% increase in difficulty (measured by chance-level MRR) between steps. Avoid very small `k` (e.g., k < 5) where chance performance is too high.

##### 3. Determine Your Sample Size

Your sample size is a function of replications and trials, and it represents a trade-off between statistical power and resources (time and cost).

-   **Replications (`num_replications`):** This determines the statistical power for comparing conditions (e.g., model A vs. model B). The original study used **30 replications**, which provides over 80% power to detect small-to-medium sized effects. This is a robust baseline for academic research.
-   **Trials (`num_trials`):** This determines the stability and reliability of the performance metrics *within* a single replication. More trials reduce noise. The original study used **80 trials**, which provides a strong buffer against occasional API errors or parsing failures while keeping costs manageable.

A **30x80 design** is a well-justified starting point, but you may adjust these values based on your research goals and budget.

##### 4. Plan Your Execution Strategy

Never run a large-scale study in one go. Follow a phased approach:

1.  **Estimate:** Calculate the total number of trials (conditions Ã— replications Ã— trials) to estimate the total API cost and runtime.
2.  **Pilot:** Always run a small pilot study first (e.g., one condition, 5-10 replications). This is critical for validating your entire pipeline, confirming your chosen models have a high parsing success rate (>95%), and catching any configuration errors before committing to a large budget.
3.  **Execute in Batches:** Run the full study in manageable chunks (e.g., by model or by k-value). Perform quality checks after each batch using the `audit_experiment.ps1` script to ensure data integrity.

##### 5. Organize Your Study

-   **Directory Structure:** Create a dedicated study directory in `output/studies/` to hold all related experiment folders.
-   **Naming Convention:** Use a consistent, descriptive naming convention for your experiment folders (e.g., `exp_{mapping}_{k}_{model}`) to keep your work organized and easily sortable.