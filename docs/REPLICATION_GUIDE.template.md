---
title: "Replication Guide"
subtitle: "Supplementary Material for 'A Framework for the Computationally Reproducible Testing of Complex Narrative Systems'"
author: "Peter J. Marko"
date: "[Date]"
---

This document is the **Replication Guide** that provides supplementary material to the main article, "A Framework for the Computationally Reproducible Testing of Complex Narrative Systems: A Case Study in Astrology." Its purpose is to serve as a detailed, step-by-step guide for researchers who wish to replicate or extend the original study's findings. For detailed information on the components of the framework, please refer to the **[üìñ Framework Manual (docs/FRAMEWORK_MANUAL.md)](docs/FRAMEWORK_MANUAL.md)**.

This guide defines the three primary replication paths (Direct, Methodological, and Conceptual) and provides a complete walkthrough of the end-to-end workflow, from initial setup and data preparation to running the main experiments and producing the final statistical analysis.

{{toc}}

## Project Overview

### Research Question

At its core, this project investigates whether a Large Language Model (LLM) can solve a complex matching task: given a set of sanitized, narrative personality descriptions (derived from birth data) and a corresponding set of general biographical profiles, can the LLM correctly pair them at a rate significantly greater than chance?

This study introduces a novel methodological approach to probe the limits of LLM pattern recognition. The link between the narrative descriptions and the biographical profiles is a faint, systematic signal generated by a deterministic, esoteric system (an astrology program). This transforms the experiment into a rigorous test of an LLM's ability to detect subtle, rule-based patterns within a noisy, high-dimensional dataset. **Crucially, this study does not seek to validate astrology.** Instead, it treats the generating program as an arbitrary, complex algorithm, testing whether the AI can find a signal in its output.

{{grouped_figure:docs/diagrams/logic_matching_task.mmd | scale=2.0 | width=60% | caption=Figure S1: The Core Research Question: An LLM-driven matching task to detect a non-random signal.}}

### Framework Architecture

The framework is organized into four primary components that work together to enable reproducible research on complex narrative systems. The production codebase is structured around a clear separation of concerns, with user-facing orchestration scripts executing core logic modules that operate on well-defined data artifacts.

{{grouped_figure:docs/diagrams/arch_project_overview.mmd | scale=2.0 | width=100% | caption=Figure S2: Project Architecture Overview. The framework consists of four main components: User-Facing Interface, Core Logic, Project Governance, and Data & Artifacts.}}

The **User-Facing Interface** comprises PowerShell orchestration scripts that provide a simple, consistent way to execute complex workflows. These scripts handle parameter validation, error recovery, and progress tracking, allowing researchers to focus on research questions rather than implementation details.

The **Core Logic** contains the production Python scripts in the `src/` directory that implement data processing, experiment execution, and analysis algorithms. These modules are designed to be modular, testable, and reusable across different research contexts.

The **Project Governance** component includes documentation, diagrams, the validation test suite, and developer utilities that ensure the framework maintains high standards of quality, reproducibility, and transparency.

The **Data & Artifacts** component manages all inputs and outputs, including source data, generated experiments, analysis reports, and project-level documentation that provides provenance for all research artifacts.

### Stimulus Generation

The experiment is built upon a custom database of 4,954 famous historical individuals with accurate and verified birth data. The personality descriptions were generated through a multi-stage, deterministic process that:

1. Sources and qualifies candidates from the Astro-Databank
2. Applies LLM-based eminence and personality scoring
3. Processes data through commercial astrology software (Solar Fire)
4. Assembles final profiles using a validated personality assembly algorithm

The personality assembly algorithm has been rigorously validated against a ground-truth dataset generated by the source software, ensuring its output is bit-for-bit identical. The result is a dataset of personality profiles where the connection to an individual's biographical profile is systematic but non-obvious.

For detailed technical information about the stimulus generation process, please refer to the **[üìñ Framework Manual (docs/FRAMEWORK_MANUAL.md)](docs/FRAMEWORK_MANUAL.md)**.

## Getting Started

### Prerequisites

The framework was developed and validated on a specific stack of technologies. Variations are possible but not currently supported. Before proceeding, please ensure you have the following:

*   **Software:**
    *   **Operating System:** Windows (the primary development and testing platform).
    *   **PowerShell:** Version 7.0 or higher.
    *   **Git:** For cloning the repository.
    *   **Solar Fire:** A licensed copy of version 9.
    *   **GraphPad Prism:** Version 10.6.1 for validating statistical analysis and reporting functionalities.

*   **Accounts & Services:**
    *   **OpenRouter:** An account with a valid API key and sufficient funds to cover the cost of LLM queries.
    *   **Astro-Databank:** A registered account at `astro.com` (this is only required if you intend to generate a new dataset via **Path 2: Methodological Replication**).

### Setup and Installation

This project uses **PDM** for dependency and environment management. Please see the **[ü§ù Developer's Guide (DEVELOPERS_GUIDE.md)](DEVELOPERS_GUIDE.md)** for detailed information on the project environment and its maintenance.

1.  **Install PDM (One-Time Setup)**:
    If you don't have PDM, install it once with pip.
    ```bash
    pip install --user pdm
    ```

2.  **Install Project Environment & Dependencies**:
    From the project's root directory, run the main PDM installation command.
    ```bash
    pdm install -G dev
    ```

3.  **Configure API Key**:
    *   Create a file named `.env` in the project root.
    *   Add your API key from your chosen provider (e.g., OpenRouter):
        `OPENROUTER_API_KEY=your-actual-api-key`

To run any project command, prefix it with `pdm run`.

### Configuration (`config.ini`)

All experimental parameters are defined in the `config.ini` file. For a direct replication, the key settings to verify are:

*   **`[Experiment]`**:
    *   `num_replications = 30`: Selected to have 80% statistical power for small effects (Cohen's d < 0.20).
    *   `num_trials = 80`: Provides 1.80:1 signal-to-noise ratio for d > 0.20 effects.
    *   `group_size`: Should be set to `7`, `10`, or `14` depending on the experiment you wish to replicate.
    *   `mapping_strategy`: Should be set to `correct` or `random` depending on the experiment you wish to replicate.
*   **`[LLM]`**:
    *   `model_name`: The API identifier for the LLM to be tested (e.g., `google/gemini-2.0-flash-lite-001`).
    *   `temperature`: `0.0` was used in the original study for deterministic output.

The framework automatically archives this file with the results for guaranteed reproducibility.

## Choosing Your Replication Path

The framework supports three distinct research paths: direct, methodological, and conceptual replication.

### Path 1: Direct Replication (Computational Reproducibility)

To ensure computational reproducibility of the original findings, researchers should use the static data files and randomization seeds included in this repository. This path validates that the framework produces the same statistical results.

**What you need:**

- The cloned repository with all static data files
- Configured API key for the evaluation LLM
- No need for Astro-Databank account or Solar Fire (static data is provided)

**Quick start:**

1. Skip to "Running Experiments" section
2. Use the provided `config.ini` settings
3. Execute experiments using the static personalities database

### Path 2: Methodological Replication (Testing Robustness)

To test the robustness of the findings, researchers can use the framework's automated tools to generate a fresh dataset from the live Astro-Databank (ADB). This tests the statistical robustness of the method on a new sample.

**What you need:**

- All requirements from Direct Replication
- Registered account at `astro.com` with credentials in `.env` file
- Licensed copy of Solar Fire software
- Time to complete the full data preparation pipeline

**Quick start:**

1. Follow the "Data Preparation" section completely
2. Generate your own `personalities_db.txt` file
3. Proceed to "Running Experiments" with your new data

### Path 3: Conceptual Replication (Extending the Research)

To extend the research, researchers can modify the framework itself, for example, by using a different LLM for the matching task or altering the analysis scripts.

**What you need:**

- All requirements from Methodological Replication
- Programming knowledge to modify Python scripts
- Understanding of the framework architecture

**Quick start:**

1. Review the Framework Manual for architecture details
2. Identify components you wish to modify
3. Follow Path 2 if you need new data, or Path 1 if using existing data
4. Implement your modifications and run experiments

### Models Used in the Original Study

For a direct or methodological replication, it is crucial to use the exact models and versions from the original study. All models were accessed via the **OpenRouter API**.

| Purpose | Model Name | API Identifier |
| :--- | :--- | :--- |
| Eminence Scoring (LLM A) | OpenAI GPT-5 | `openai/gpt-5-chat` |
| OCEAN Scoring (LLM B) | Anthropic Claude 4 Sonnet | `anthropic/claude-4-sonnet` |
| Neutralization (LLM C) | Google Gemini 2.5 Pro | `google/gemini-2.5-pro` |
| Evaluation (LLM D) | Google Gemini 1.5 Flash | `google/gemini-1.5-flash-001` |

*Access Dates for Evaluation LLM: September 2025*

## Data Preparation

The data preparation pipeline is a fully automated, multi-stage workflow that transforms the raw data from the Astro-Databank into the final `personalities_db.txt` file used in the experiments. The pipeline is organized into four main stages, as illustrated in the diagrams below.

{{grouped_figure:docs/diagrams/flow_prep_1_qualification.mmd | scale=2.5 | width=85% | caption=Figure S5: Data Prep Flow 1 - Data Sourcing and Candidate Qualification.}}

{{grouped_figure:docs/diagrams/flow_prep_2_selection.mmd | scale=2.5 | width=100% | caption=Figure S6: Data Prep Flow 2 - LLM-based Candidate Selection.}}

{{grouped_figure:docs/diagrams/flow_prep_3_generation.mmd | scale=2.5 | width=100% | caption=Figure S7: Data Prep Flow 3 - Profile Generation.}}

The entire data preparation workflow is managed by a single, intelligent orchestrator: `prepare_data.ps1`. This script is fully resumable and is the recommended method for generating a new dataset.

**Prerequisites:** An account at `astro.com` and credentials in the `.env` file.
```powershell
# Run the entire data preparation pipeline
.\prepare_data.ps1
```
This single command will execute all the necessary steps, from fetching the raw data to generating the final `personalities_db.txt` file. The individual `pdm run` scripts (e.g., `pdm run find-links`) are also available for advanced development and debugging but are not part of the standard replication workflow.

### Stage 1: Data Sourcing

This stage uses `fetch_adb_data.py` to create the initial raw dataset by querying the live Astro-Databank with a specific set of pre-filters. It performs two crucial transformations at the source:

-   **Identifier Standardization:** It replaces the unstable, temporary record number (`ARN`) with a file-specific sequential `Index`, and renames the permanent Astro-Databank ID (`ADBNo`) to `idADB`.
-   **Timezone Calculation:** It immediately processes the raw timezone code from the API into the final `ZoneAbbr` and `ZoneTimeOffset` values required by downstream software.

The query only includes subjects who meet all of the following criteria:

-   **High-Quality Birth Data:** The record must have a Rodden Rating of 'A' or 'AA', indicating the birth time is from a reliable source.
-   **Deceased Individuals:** Inclusion in the **Personal > Death** category.
-   **Eminence:** Inclusion in the **Notable > Famous > Top 5% of Profession** category.

#### a. Fetching Raw Data (fetch_adb_data.py)

This script automates the scraping of the Astro-Databank website.

**Prerequisites:**

1. A registered account at astro.com.
2. Credentials in the .env file: ADB_USERNAME and ADB_PASSWORD.

**Execution:**
```bash
# Fetch a new dataset from the live ADB
pdm run fetch-adb
```
This produces `data/sources/adb_raw_export.txt`.

### Stage 2: Candidate Qualification

This stage performs a rigorous, deterministic filtering pass on the raw data to create a high-quality cohort of "eligible candidates."

#### a. Finding Wikipedia Links (find_wikipedia_links.py)

This script takes the raw export and finds the best-guess Wikipedia URL for each subject, creating the intermediate `data/processed/adb_wiki_links.csv` file.

```bash
# Find Wikipedia links for all raw records
pdm run find-links
```

#### b. Validating Wikipedia Pages (validate_wikipedia_pages.py)

This script takes the list of found links, validates the content of each page, and produces the final `data/reports/adb_validation_report.csv` and a human-readable summary.

**A Note on Reproducibility:** Because Wikipedia is a dynamic source, this validation is not perfectly reproducible. For direct replication, the study's pipeline relies on the static report (`adb_validation_report.csv`) included in the repository.

```bash
# Validate the content of each found Wikipedia page
pdm run validate-pages
```

#### c. Selecting Eligible Candidates (select_eligible_candidates.py)

This script integrates the raw data with the Wikipedia validation report and applies the following additional criteria in order:

| Criteria | Rule | Purpose |
| :--- | :--- | :--- |
| **Wikipedia Validation** | `Status` must be `OK` | Ensures a valid English Wikipedia page was found where the name and death date were confirmed. |
| **Entry Type** | `EntryType` must be `Person` | Filters out non-person records (e.g., events, research entries). |
| **Birth Year Range** | Must be between 1900-1999 | Controls for cohort-specific confounds by ensuring a homogenous historical period. |
| **Hemisphere** | `Latitude` must contain 'N' | Controls for the potential confound of a zodiacal shift for Southern Hemisphere births. |
| **Valid Time Format** | Birth time must be present and `HH:MM` | Ensures data is complete for precise calculations. |
| **Deduplication** | Unique Name + Birth Date | Removes duplicate entries from the source database. |

The final output of this stage is the `adb_eligible_candidates.txt` file.

### Stage 3: LLM-based Candidate Selection (Optional)

This stage is a second, optional filtering pass that uses LLMs to score the "eligible candidates" to determine the final, smaller subject pool. This entire stage can be skipped by setting `bypass_candidate_selection = true` in `config.ini`.

#### a. A Note on Determining the Optimal Cutoff Parameters
To ensure the parameters for the final candidate selection algorithm were chosen objectively and were optimally tuned to the specific shape of the dataset's variance curve, a **systematic sensitivity analysis** was performed using the dedicated `scripts/analysis/analyze_cutoff_parameters.py` utility. The search space for the parameters was iteratively expanded to ensure a true global optimum was found.

The script performed a grid search over a wide range of values for `cutoff_search_start_point` and `smoothing_window_size`. To avoid overfitting to a single "best" result, a sophisticated **stability-based recommendation algorithm** was used. This algorithm first identifies a cluster of high-performing parameter sets (those with a low error between the predicted and ideal cutoffs) and then calculates a "stability score" for each based on the average error of its neighbors in the parameter grid. The final recommendation is the parameter set with the best stability score, representing the center of the most stable, high-performing region.

The final, expanded analysis revealed several key insights:

*   **The Variance Curve Shape:** The cumulative variance curve has a distinct shape: a long, steep decline followed by a wide, shallow plateau containing long-wavelength, low-amplitude noise.
*   **Justification for the Final Parameters:** A high `cutoff_search_start_point` (3500) was essential to force the analysis to begin *on the plateau*, isolating the region of interest. A large `smoothing_window_size` (800) was necessary to average out the long, shallow waves on the plateau, allowing the slope-based algorithm to detect the true global trend.

Based on this comprehensive, data-driven analysis, the following optimal parameters were chosen and set in `config.ini`:

*   `cutoff_search_start_point = 3500`
*   `smoothing_window_size = 800`

The plot of this analysis (see Figure S8) provides a clear visual justification for these choices. It shows how the algorithm, using these parameters, correctly identifies the start of the curve's final plateau, resulting in a final cutoff of 4,954 subjects.

{{grouped_figure:data/reports/variance_curve_analysis.png | caption=Figure S8: Variance Curve Analysis for Optimal Cohort Selection. The plot shows the raw cumulative variance, the smoothed curve (800-pt moving average), the search start point (3500), and the final data-driven cutoff (4954).}}

#### b. Eminence Scoring (generate_eminence_scores.py)
Processes the eligible candidates list to generate a calibrated eminence score for each, producing a rank-ordered list that now includes `BirthYear`.

```bash
# Generate eminence scores for all eligible candidates
pdm run gen-eminence
```

#### c. OCEAN Scoring (generate_ocean_scores.py)
A fully automated, resilient script that generates OCEAN personality scores for every subject in the eminence-ranked list.

```bash
# Generate OCEAN scores to determine the final cutoff
pdm run gen-ocean
```

#### d. Final Selection & Cutoff (select_final_candidates.py)
Performs the final filtering and selection. It takes the complete OCEAN scores list and applies a sophisticated, data-driven algorithm to determine the optimal cohort size. The script first calculates the cumulative personality variance curve for the entire cohort, smooths this curve using a moving average to eliminate local noise, and then performs a slope analysis to find the "plateau"‚Äîthe point of diminishing returns where adding more subjects no longer meaningfully contributes to the psychological diversity of the pool. It then resolves country codes and sorts the final list by eminence.

```bash
# Create the final, transformed list of subjects
pdm run select-final
```

### Stage 4: Profile Generation

This is the final stage, which assembles the personality profiles for the selected candidates. It involves a mix of automated and manual steps.

#### a. Formatting for Import (prepare_sf_import.py)
Formats the final subject list for import into the Solar Fire software. It performs a critical data integrity step by encoding each subject's unique `idADB` into a Base58 string and injecting it into the `ZoneAbbr` field. This technique allows a unique identifier to pass through the manual software step unharmed, ensuring a perfect data merge later. The `ZoneAbbr` field was specifically chosen as it is a text field that is passed through the software without modification and is not used in any astrological calculations, making it a safe channel for this data.

```bash
# Prepare the final list for the manual import step
pdm run prep-sf-import
```

#### b. Manual Processing (Solar Fire)
The formatted file is imported into Solar Fire, which calculates the required celestial data and exports it as `sf_chart_export.csv`.

**For detailed setup instructions and step-by-step guidance, please refer to the [üìñ Framework Manual (docs/FRAMEWORK_MANUAL.md)](docs/FRAMEWORK_MANUAL.md#solar-fire-integration-and-configuration).**

The process consists of:

1. **Software Requirements**: Solar Fire v9.0.3 (available at https://alabe.com/solarfireV9.html)
2. **One-Time Setup**: Configure chart points, preferences, and data formats
3. **Import/Export Workflow**: Import the formatted birth data, calculate all charts, and export chart data
4. **Output**: The exported file contains a repeating 14-line block for each subject with celestial position data

#### c. Neutralization (neutralize_delineations.py)
This script uses a powerful hybrid strategy to rewrite the esoteric source texts.

*   **Fast Mode (`--fast`):** For initial runs, this mode bundles tasks into large, high-speed API calls (e.g., all 12 "Sun in Signs" delineations at once). This is highly efficient but may fail on some large tasks.
*   **Robust/Resume Mode (default):** For resuming or fixing failed runs, this mode processes each of the 149 delineations as a separate, atomic task. This granular approach is slower but guarantees completion by solving potential response truncation issues from the LLM.

```bash
# Perform the initial, high-speed neutralization
pdm run neutralize --fast

# Automatically fix any failed tasks from the fast run
pdm run neutralize
```

#### d. Integration (create_subject_db.py)
Bridges the manual step by reading the Solar Fire chart export, decoding the unique `idADB` from the `ZoneAbbr` field, and merging the chart data with the final subject list to produce a clean master database.

```bash
# Integrate the manual chart data export
pdm run create-subject-db
```

#### e. Generation (generate_personalities_db.py)
Assembles the final `personalities_db.txt` by combining the subject data with the neutralized delineation library according to a deterministic algorithm.

```bash
# Generate the final personalities database
pdm run gen-db
```

**Note**: Before proceeding with the neutralization step, ensure you have completed the Solar Fire setup and exported the delineations library as described in the [üìñ Framework Manual (docs/FRAMEWORK_MANUAL.md)](docs/FRAMEWORK_MANUAL.md#exporting-the-delineations-library-one-time-task).

The output is `personalities_db.txt`, a tab-delimited file with the fields: `Index`, `Name`, `BirthYear`, and `DescriptionText`.

With the `personalities_db.txt` file generated, the data preparation phase is complete. The following sections describe how to run the experiment workflow and analysis.

## Running Experiments

The research process is divided into two main stages: first, generating the data for each experimental condition, and second, compiling those conditions into a final study for analysis.

### Experiment Design Considerations

Before running experiments, consider the following design factors:

1. **Factorial Design**: The study employs a 2√ó3√ó3 factorial design with multiple factors:
   - `mapping_strategy` (correct vs random) - 2 levels
   - `group_size` (k = 7, 10, 14) - 3 levels
   - `model` (different LLMs) - 3 levels

2. **Statistical Power**: The original study used 30 replications per condition with 80 trials per replication, providing >80% power to detect small effect sizes (Cohen's d < 0.20).

3. **Cost Considerations**: LLM API costs vary by model. Consider using cost-effective models for large-scale experiments.

### Stage 1: Generate Data for Each Experimental Condition

The first stage is to generate a complete set of results for each of the conditions in your study's factorial design.

For a factorial study, you can use the `generate_factorial_commands.ps1` script to automatically generate all the necessary commands:

```powershell
# Generate commands for all experimental conditions
./generate_factorial_commands.ps1 -OutputScript "run_factorial_study.ps1"
```

This will create a script with all the necessary `new_experiment.ps1` commands for each condition. You can then execute this script to run all experiments.

For each condition, you will:

1.  **Configure `config.ini`**: Set the `mapping_strategy` and `group_size` parameters for the specific condition you are running.
2.  **Run the Experiment**: Execute the `new_experiment.ps1` script. This will create a new, self-contained experiment directory in `output/new_experiments/`.

**Execution:**
```powershell
# Example: After setting parameters in config.ini
./new_experiment.ps1
```

> **Tip for Long Runs:** Generating multiple experiments can take a significant amount of time and may be interrupted. The framework is designed for this.
>
> -   Use `audit_experiment.ps1` to get a detailed, read-only status report on any experiment.
> -   Use `fix_experiment.ps1` to intelligently resume any interrupted run. The script will automatically pick up where it left off, ensuring no work is lost.

### Stage 2: Compile and Analyze the Study

Once you have generated and validated all experiment directories, you can proceed to the final analysis.

**Step 1: Organize Your Experiments**
Manually create a new study directory (e.g., `output/studies/My_Replication_Study/`) and move all of your completed experiment folders into it.

**Step 2: Perform a Pre-Flight Check (`audit_study.ps1`)**
Before running the final compilation, it is best practice to run a consolidated audit on the entire study directory. This script checks every experiment and confirms that the study is complete and ready for analysis.

**Execution:**
```powershell
./audit_study.ps1 -StudyDirectory "output/studies/My_Replication_Study"
```

**Step 3: Run the Final Analysis (`compile_study.ps1`)**
This is the final step. The `compile_study.ps1` script orchestrates the entire analysis pipeline. It aggregates the data from all experiments, runs the statistical analysis, and generates the final, publication-ready reports and plots.

**Execution:**
```powershell
./compile_study.ps1 -StudyDirectory "output/studies/My_Replication_Study"
```

**Final Artifacts:**
The script generates two key outputs in your study directory:

1.  A master `STUDY_results.csv` file containing the aggregated data from all experiments.
2.  A new `anova/` subdirectory containing:
    *   `STUDY_analysis_log.txt`: A comprehensive text report of the statistical findings.
    *   `boxplots/`: Publication-quality plots visualizing the results.
    *   `diagnostics/`: Q-Q plots used for checking statistical assumptions.

## Reference Materials

### Models Used in the Original Study

For a direct or methodological replication, it is crucial to use the exact models and versions from the original study. All models were accessed via the **OpenRouter API**.

| Purpose | Model Name | API Identifier |
| :--- | :--- | :--- |
| Eminence Scoring (LLM A) | OpenAI GPT-5 | `openai/gpt-5-chat` |
| OCEAN Scoring (LLM B) | Anthropic Claude 4 Sonnet | `anthropic/claude-4-sonnet` |
| Neutralization (LLM C) | Google Gemini 2.5 Pro | `google/gemini-2.5-pro` |
| Evaluation (LLM D) | Google Gemini 1.5 Flash | `google/gemini-1.5-flash-001` |

*Access Dates for Evaluation LLM: September 2025*

### Troubleshooting Common Issues

This section provides solutions to the most common issues researchers may encounter when setting up the framework or running experiments.

| Issue | Solution |
| :--- | :--- |
| **`pdm` command not found** | This usually means the Python scripts directory is not in your system's PATH. You can either add it, or use `python -m pdm` as a reliable alternative (e.g., `python -m pdm install -G dev`). |
| **API Errors during an experiment run** | Network issues or API rate limits can cause individual LLM calls to fail. The framework is designed for this. Simply run the `fix_experiment.ps1` script on the experiment directory. It will automatically find and re-run only the failed API calls. |
| **"Permission Denied" error when building DOCX files** | This error occurs if a `.docx` file is open in Microsoft Word while the `pdm run build-docs` script is running. Close the file in Word, and the script will automatically retry and continue. |
| **`git` command not found** | The framework requires Git for versioning and reproducibility checks. Please install it from [git-scm.com](https://git-scm.com/downloads) and ensure it is available in your system's PATH. |
| **All LLM sessions fail (100% failure rate)** | This indicates a model configuration problem. Verify the model name in `config.ini` matches available models and check your API credentials and permissions. |
| **Repair process loops indefinitely** | The repair system automatically limits retry attempts to 3 cycles maximum. After 3 cycles, it proceeds with available data to prevent endless loops when external factors cause persistent failures. |
| **Enhanced status messages** | The framework now provides colored error output and detailed progress tracking (elapsed time, remaining time, ETA) for better visibility during long-running operations. |

### Related Files

*   `base_query.txt`
    This file contains the final prompt template used for the LLM matching task. It is the product of a systematic, multi-stage piloting process. Various prompt structures and phrasing were tested to find the version that yielded the most reliable and consistently parsable structured output from the target LLM.

*   `country_codes.csv`
    This file provides a mapping from the country/state abbreviations used in the Astro-Databank to their full, standardized names. A sample is shown below. The complete file can be found at `data/foundational_assets/country_codes.csv`.

    | Abbreviation | Country |
    | :--- | :--- |
    | `AB (CAN)` | Canada |
    | `AK (US)` | United States |
    | `ENG (UK)` | United Kingdom |
    | `FR` | France |
    | `GER` | Germany |